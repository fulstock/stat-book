\documentclass[oneside,final,14pt]{extreport}

\usepackage[nottoc,notlot,notlof]{tocbibind}
\usepackage{cmap}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}

\usepackage{microtype}

\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{relsize}
\usepackage{hyperref}
\usepackage{tikz-cd}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
\usepgfplotslibrary{fillbetween}
\usepackage{adjustbox}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\usepackage{hhline}
\usepackage{multirow}
\raggedbottom
\allowdisplaybreaks

\usepackage{etoolbox}
    \makeatletter
    \patchcmd{\@makechapterhead}{\vspace*{50\p@}}{}{}{}% Removes space above \chapter head
    \patchcmd{\@makeschapterhead}{\vspace*{50\p@}}{}{}{}% Removes space above \chapter* head
    \makeatother
    
\newcommand{\frc}[2]{\raisebox{2pt}{$#1$}\big/\raisebox{-3pt}{$#2$}}
\newcommand\mydef{{\bf Опр.}}
\newcommand\mynote{{\bf Замеч.}}
\newcommand\myst{{\bf Утв.}}
\newcommand\mycon{{\bf Следствие.}}
\newcommand\myth{{\bf Теорема.}}
\newcommand\myqed{{\bf Док-во.}}
\newcommand\myex{{\bf Пример.}}
\newcommand\myprob[1]{{\mathbb{P}(#1)}}
\newcommand\mydes{{\bf Обозн.}}

\renewcommand{\qedsymbol}{$\blacksquare$}
\renewenvironment{proof}{{\bfseries Доказательство.}}{\qed}

\theoremstyle{plain}
\newtheorem*{thm*}{Утверждение}
\newtheorem{thm}{Утверждение}

\newtheorem*{lem}{Лемма}
\newtheorem*{crlr}{Следствие}

\theoremstyle{definition}
\newtheorem*{defn}{Определение}
\newtheorem*{exmp}{Пример}
\newtheorem*{symb}{Обозначение}
\newtheorem*{rmrk}{Замечание}
\newtheorem*{task}{Задача}

\newtheoremstyle{named}{}{}{\itshape}{}{\bfseries}{.}{.5em}{\thmnote{#3}}
\theoremstyle{named}
\newtheorem*{namedthm}{Теорема}


\newcommand\sbullet[1][.5]{\mathbin{\vcenter{\hbox{\scalebox{#1}{$\bullet$}}}}}
\newenvironment{compactlist}{
\begin{list}{{$\sbullet[.75]$}}{
\setlength\partopsep{0pt}
\setlength\parskip{0pt}
\setlength\parsep{0pt}
\setlength\topsep{0pt}
\setlength\itemsep{0pt}
}
}{
\end{list}
}

\setpapersize{A4}
\setmarginsrb{2cm}{1.5cm}{2cm}{1.5cm}{0pt}{0mm}{0pt}{13mm}
\linespread{1.05}

\usepackage{indentfirst}
\sloppy

\usepackage{graphicx}
%\usepackage{fancyhdr}

\begin{document}
\begin{titlepage}
    \centering
    \vfill
    {\scshape\large
        Московский государственный университет\\
        им. М.В. Ломоносова\\
        Факультет вычислительной математики и кибернетики\\
   }
    \vskip1cm
    {\scshape\huge
        Теория вероятностей\\
        Математическая статистика\\
   }

    \vfill
    \includegraphics[width=8cm]{pic.png}
    \vfill
    {\upshape\large
        Москва\\
        ~2020
   }
\end{titlepage}

\newpage
Данное учебное пособие составлено на основе экзаменационных билетов 2020 года по курсу <<Теория вероятностей и математическая статистика>> факультета ВМК МГУ и \textit{не является} конспектом лекций, а также \textit{не проверялось} преподавателями курса. При составлении преимущественно использовались материалы, указанные в разделе \textbf{Литература}.

\vspace{5mm}

Авторский коллектив:
\begin{compactlist}
    \item Главный редактор (теория вероятностей)~---~Рожков~И.\,С.
    \item Главный редактор (математическая статистика)~---~Рыгин~А.\,С.
    \item Рецензент~---~Селезнёв~М.\,В.
    \item Иллюстратор~---~Васильев~Р.\,Л.
\end{compactlist}

\vspace{1cm}

\textbf{Авторы \textit{запрещают} Российскому авторскому обществу и любым другим организациям производить любого рода лицензирование данного произведения и осуществлять в интересах авторов какую бы то ни было иную связанную с авторскими правами деятельность без их письменного разрешения.}

\microtypesetup{protrusion=false} % disables protrusion locally in the document
\tableofcontents % prints Table of Contents
\microtypesetup{protrusion=true} % enables protrusion

\chapter{Теория вероятностей}

\section{Вероятностное пространство. Операции над событиями. Свойства вероятности}
\begin{defn}
    {\it Пространство элементарных исходов} $\Omega$~--- любое непустое множество, содержащее все возможные результаты случайного эксперименты. Элементы $\omega \in \Omega$~--- {\it элементарные исходы}.
\end{defn}

\begin{defn}
{\it Алгебра} $\mathcal{A}$~--- множество подмножеств $\Omega$, обладающее следующими свойствами:

\begin{enumerate}
    \item $\Omega \in \mathcal{A}$;
    \item $A \in \mathcal{A} \Rightarrow \overline{A} \in \mathcal{A}$; (здесь $\overline{A} = \Omega \setminus A$~--- {\it дополнение} к $A$)
    \item $A, B \in \mathcal{A} \Rightarrow A \cup B \in \mathcal{A}$ (по индукции: $A_1, A_2, \ldots, A_n \in \mathcal{A} \Rightarrow \bigcup\limits_{i=1}^n A_i \in \mathcal{A}$).
\end{enumerate}
\end{defn}

\begin{rmrk}
    Если $A, B \in \mathcal{A}$, то $A \cap B \equiv \overline{\overline{A} \cup \overline{B}} \in \mathcal{A}$
\end{rmrk}

\begin{defn}
$\sigma \text{\it{-алгебра~}} \mathcal{F}$~--- множество подмножеств $\Omega$, обладающее следующими свойствами:

\begin{enumerate}
    \item $\Omega \in \mathcal{F}$;
    \item $A \in \mathcal{F} \Rightarrow \overline{A} \in \mathcal{F}$;
    \item $A_1, A_2,\ldots, A_n,\ldots \in \mathcal{F} \Rightarrow \bigcup\limits_{i=1}^\infty A_i \in \mathcal{F}$.
\end{enumerate}
\end{defn}

\begin{rmrk}
    Любая $\sigma \text{-алгебра}$ является алгеброй. Первые два пункта определений идентичны, рассмотрим третий. Для любой конечной последовательности $A_1, A_2,\ldots, A_n \in \mathcal{A}$ составим соответствующую счётную последовательность $A_1, A_2, \ldots, A_n, A_{n+1}=\varnothing, A_{n+2}=\varnothing,\ldots \in \mathcal{A}$. По определению $\sigma \text{-алгебры}$: $\bigcup\limits_{i=1}^\infty A_i \in \mathcal{F} \Rightarrow \bigcup\limits_{i=1}^n A_i \in \mathcal{F}$, следовательно, выполнен третий пункт определения алгебры.
\end{rmrk}

\begin{defn}
    {\it Случайное событие} $A$~--- элемент $\sigma \text{-алгебры~} \mathcal{F}$. $A=\varnothing$~---{\it невозможное событие}, $A=\Omega$~--- {\it достоверное событие}. Событие $\overline{A}$~--- {\it противоположное} $A$, т.е. происходит тогда и только тогда, когда не происходит $A$.

Операции над событиями:

\begin{compactlist}
    \item {\it Объединение} $A \cup B$~--- происходит тогда и только тогда, когда происходят или $A$, или $B$, или оба вместе.
    \item {\it Пересечение} $A \cap B$ (или $AB$)~--- происходит тогда и только тогда, когда происходят и $A$ и $B$ вместе. Если $AB = \varnothing$, то события $A$ и $B$ называются {\it несовместными}.
    \item {\it Разность} $A \setminus B$~--- происходит тогда и только тогда, когда происходит $A$ и не происходит $B$.
    \item {\it Симметрическая разность} $A \triangle B$ ~--- происходит тогда и только тогда, когда либо происходит $A$ и не происходит $B$, либо происходит $B$ и не происходит $A$.
\end{compactlist}
\end{defn}

\begin{defn}
    $\sigma \text{-алгебра}$ {\it порождена классом $K$}, если она является пересечением всех $\sigma \text{-алгебр}$, содержащих $K$, т.е. является {\it минимальной $\sigma \text{-алгеброй}$}, содержащей $K$.
\end{defn}

\begin{exmp}
    Пусть $K = \{A\}$, тогда $\sigma (K) = \{\varnothing, \Omega, A, \overline{A}\}$.
\end{exmp}

\begin{defn}
    {\it Вероятностная мера}~--- функция $\mathbb{P}: \mathcal{F} \rightarrow \mathbb{R}$, обладающая следующими свойствами:

\begin{enumerate}
    \item $\myprob{A} \geqslant 0~\forall A \in \mathcal{F}$ ({\it неотрицательность})
    \item $\myprob{\Omega} = 1$ ({\it нормировка})
    \item $\forall A_1, A_2, \ldots, A_n\ldots \in \mathcal{F},~ A_{i}A_{j} = \varnothing~ \; \forall i, j \in \mathbb{N}, i \ne j \Rightarrow $
    
    $ \Rightarrow \myprob{\bigcup\limits_{i=1}^\infty A_i} = \sum\limits_{i=1}^\infty \myprob{A_i}$
    
    ({\it счётная аддитивность~--- вероятность счётного объединения несовместных событий равна сумме их вероятностей})
\end{enumerate}
\end{defn}

\begin{namedthm}[Свойства вероятности]\leavevmode
    \begin{enumerate}
       \item $\myprob{\varnothing}=0$;
        \item $A, B \in \mathcal{F}, B \subset A \Rightarrow \myprob{A} \geqslant \myprob{B}$ (монотонность);
       \item $\myprob{A \setminus B} = \myprob{A} - \myprob{AB}$;
       \item $\myprob{A \cup B} = \myprob{A} + \myprob{B} - \myprob{AB}$.
       \item $ \forall \; A_1 \supseteq A_2 \supseteq \ldots \supseteq A_n \supseteq \ldots \colon \bigcap\limits_{n = 1}^{\infty} A_n = A \quad \Rightarrow$
       
       $\displaystyle \lim_{n \to \infty}\myprob{A_n} = \myprob{A}$ (непрерывность).
    \end{enumerate}
\end{namedthm}


\begin{proof}\leavevmode
    \begin{enumerate}
    \item Рассмотрим последовательность событий $A_{1} = \Omega, A_{2} = \varnothing, \ldots, A_{n} = \varnothing, \ldots$:
    \begin{equation*}
        \bigcup\limits_{i=1}^\infty A_i = \Omega \Rightarrow \myprob{\bigcup\limits_{i=1}^\infty A_i} = \myprob{\Omega} = 1.
    \end{equation*}
    При этом $A_{i}A_j = \varnothing~(i \ne j)$, следовательно, по пункту 3 определения вероятности: $\sum\limits_{i=2}^\infty \myprob{\varnothing} = 0 \Rightarrow \myprob{\varnothing} = 0$.
    \item $B \subset A \Rightarrow A = (A \setminus B) \cup B$. Из неотрицательности вероятности и того, что $(A \setminus B) \cap B = \varnothing$, следует, что $\myprob{A} = \myprob{A \setminus B} + \myprob{B} \geqslant \myprob{B}$. Кроме того, $\myprob{A \setminus B} = \myprob{A} - \myprob{B}$.
    \item Представим $A$ в виде $A = (A \setminus B) \: \cup \: AB$. Очевидно, что \\ $(A \setminus B) \: \cap \: AB = \varnothing$ (т.е. эти события несовместны). Используем свойство аддитивности (см. замечание сразу после доказательства): 
    \begin{equation*}
        \myprob{A} = \myprob{(A \setminus B) \cup AB} = \myprob{A \setminus B} + \myprob{AB}
    \end{equation*}
    Отсюда и следует, что $\myprob{A \setminus B} = \myprob{A} - \myprob{AB}.$
    \item Представим $A$ в виде $A \: \cup \: B = (A \setminus AB)\: \cup \:B$. Очевидно, что \\ $(A \setminus AB) \cap B = \varnothing$ (т.е. эти события тоже несовместны).
    \begin{gather*}
        \myprob{A \cup B} = \myprob{(A \setminus AB) \cup B} = \myprob{A \setminus AB} + \myprob{B} \\
        \myprob{A \setminus AB} = \myprob{A} - \myprob{A \cap AB} = \myprob{A} - \myprob{AB}
    \end{gather*}
    Следовательно, $\myprob{A \cup B} = \myprob{A} + \myprob{B} - \myprob{AB}$.
    \item Рассмотрим множества $C_n = A_n \setminus A_{n+1}, \; n \in \mathbb{N}$. Они несовместны (пусть $l < m$, тогда $C_m = (A_m \setminus A_{m+1}) \subset A_m \subset A_{m-1} \subset \ldots \subset A_{l+1},$ но $C_l = A_l \setminus A_{l+1} \Rightarrow C_l \cap C_m = \varnothing$). Тогда
    $$A_1 = \bigcup\limits_{n = 1}^{\infty} C_n \cup A, \quad \myprob{A_1} = \mathbb{P}\left({\bigcup\limits_{n = 1}^{\infty} C_n \cup A}\right) = \sum\limits_{n=1}^{\infty} \myprob{C_n} + \myprob{A} $$
    $$ \sum\limits_{n=1}^{\infty} \myprob{C_n} = \myprob{A_1} - \myprob{A}$$
    Таким образом, ряд из вероятностей событий $C_n$ сходится. Это равносильно тому, что его остаток $\sum\limits_{n=k}^{\infty} \myprob{C_n}$ стремится к нулю при $k \rightarrow \infty$. Но при этом
    $$ \bigcup\limits_{n = k}^{\infty} C_n \cup A = A_k, \quad \myprob{A_k} = \mathbb{P}\left({\bigcup\limits_{n = k}^{\infty} C_n \cup A}\right) = \sum\limits_{n=k}^{\infty} \myprob{C_n} + \myprob{A} $$
    Перейдя в последнем равенстве к пределу при $k \rightarrow \infty,$ получим
    $$ \lim_{k \to \infty} \myprob{A_k} = \myprob{A}.$$

\end{enumerate}
\end{proof}

\begin{rmrk}
    Интуитивно понятно, что из счётной аддитивности следует и конечная аддитивность. Покажем это строго. Для любых несовместных $A_1, A_2, \ldots, A_n$ мы можем рассмотреть бесконечную последовательность $A_1, A_2, \ldots, A_n, \varnothing, \varnothing, \ldots$ . $\forall \; i \ne j \; A_i A_j = \varnothing$, так как наши $n$ событий несовместны, а пересечение любого множества с пустым множеством тоже является пустым множеством. Тогда 
    \begin{equation*}
        \mathbb{P}\left({\bigcup\limits_{i=1}^\infty A_i}\right) 
        = \sum\limits_{i=1}^\infty \myprob{A_i} 
        = \{\myprob{\varnothing} = 0\}
        = \sum\limits_{i=1}^n \myprob{A_i}.
    \end{equation*}
    Кроме того, мы знаем, что $\forall B$ верно $B \cup \varnothing = B$. А значит, $\bigcup\limits_{i = 1}^\infty A_i = \bigcup\limits_{i=1}^n A_i$.\\
    Из этого и следует то, что $\myprob{\bigcup\limits_{i=1}^n A_i} = \sum\limits_{i=1}^n \myprob{A_i}.$
\end{rmrk}

\begin{rmrk}
    Можно показать, что счётная аддитивность равносильна одновременному наличию конечной аддитивности и непрерывности. Иными словами, третью аксиому в определении вероятности можно заменить на пару утверждений:
    \begin{itemize}
        \item $\forall \; A, B \colon A \cap B = \varnothing \; \Rightarrow \; \myprob{A \cup B} = \myprob{A} + \myprob{B}$ 
        
        (По индукции можно показать аддитивность для любого конечного $n$)
        \item $ \forall \; A_1 \supseteq A_2 \supseteq \ldots \supseteq A_n \supseteq \ldots \colon \bigcap\limits_{n = 1}^{\infty} A_n
       = A \quad \Rightarrow$ 
       
       $\displaystyle \lim_{n \to \infty}\myprob{A_n} = \myprob{A}.$
    \end{itemize}
\end{rmrk}

\pagebreak

\begin{defn}
    Тройка $(\Omega, \mathcal{F}, \mathbb{P})$~--- {\it вероятностное пространство}.
\end{defn}
\begin{rmrk}
    Вероятностное пространство не является пространством в функциональном смысле.
\end{rmrk}

\begin{exmp}
    Тройка $\left( [0, 1], \mathcal{B}_{[0, 1]}, \lambda \right)$, где $\lambda$ ~--- мера Лебега, является вероятностным пространством. В самом деле:
    \begin{itemize}
    \item $\Omega = [0, 1] \neq \varnothing$
    \item $\mathcal{B}_{[0,1]} = \mathcal{B} \cap [0, 1] $ ~--- $\sigma$-алгебра над $\Omega = [0, 1]$ 
    \item $\lambda(\Omega) = \lambda([0, 1]) = 1, \; \forall \: A \in \mathcal{B}_{[0, 1]} \; \lambda(A) \geqslant 0$ и мера (счётного) объединения непересекающихся множеств есть (счётная) сумма их мер, т.е. выполняются три аксиомы вероятности.
    \end{itemize}
\end{exmp}

\begin{defn}
    Пусть $\Omega = \{\omega_1, \omega_2, \ldots, \omega_n\}$~--- конечное непустое множество, $\mathcal{F}$~--- множество всех подмножеств $\Omega$. Положим $\myprob{\{\omega_i\}} = p_i$. Вероятностное пространство, определённое таким образом,~--- {\it дискретное вероятностное пространство}. Тогда для любого события $A = \{\omega_{i_1}, \ldots, \omega_{i_k}\}$ его вероятность $\myprob{A} = \sum\limits_{j=1}^k p_{i_j}$.
\end{defn}

\begin{defn}
    {\it Классическое определение вероятности}:
    \begin{equation*}
        p_1 = p_2 = \ldots =p_n=\cfrac{1}{n}, \quad \myprob{A} = \cfrac{k}{n},~ k=|A|,
    \end{equation*}
    т.е. все элементарные исходы считаются {\it равновозможными}.
\end{defn}

\section {Условная вероятность. Независимость событий. Критерий независимости. Формула полной вероятности. Формула Байеса}

\begin{defn}
    Пусть задано вероятностное пространство $(\Omega, \mathcal{F}, \mathbb{P})$, ${A, B \in \mathcal{F}, \, \myprob{B} > 0}$. {\it Условная вероятность события $A$ при событии~$B$}:
    \begin{equation*}
        \myprob{A|B}=\cfrac{\myprob{AB}}{\myprob{B}}
    \end{equation*}
\end{defn}

\begin{thm*}
    Условная вероятность $\myprob{A|B}$~--- вероятность, заданная на $\mathcal{F}$.
\end{thm*}

\begin{proof}
    Проверим три аксиомы из определения вероятности.

\begin{enumerate}
    \item $\forall A \in \mathcal{F} \quad \myprob{A|B} \geqslant 0, \text{т.к.}~ \myprob{AB} \geqslant 0,~ \myprob{B} > 0$
    \item  $\myprob{\Omega|B} = \cfrac{\myprob{B \cap \Omega}}{\myprob{B}} = \cfrac{\myprob{B}}{\myprob{B}} = 1$
    \item Пусть дана некоторая последовательность событий $\it A_1, A_2, \ldots A_n, \ldots$; $A_i \cap A_j = \varnothing \: ({\it i \ne j})$. Тогда: 
    \begin{multline*}
        \mathbb{P}\left(\,\left. \bigcup\limits_{i=1}^\infty A_i \right| B\right)  = \cfrac{\mathbb{P}\left(\left(\bigcup\limits_{i=1}^\infty A_i\right) \cap B\right)}{\myprob{B}} = \cfrac{\mathbb{P}\left(\bigcup\limits_{i=1}^\infty\left(A_i \cap B\right)\right)}{\myprob{B}} = \\
        = \cfrac{\sum\limits_{i=1}^\infty \myprob{A_i \cap B}}{\myprob{B}}
        = \sum\limits_{i=1}^\infty \myprob{A_i | B}.
    \end{multline*}
\end{enumerate}
\end{proof}
\begin{rmrk}
    Некоторые свойства условной вероятности:
    \begin{enumerate}
        \item Если $A \cap B = \varnothing,$ то $\myprob {A | B} = 0.$ 
        \item Если $B \subset A$, то $\myprob{A|B} = 1.$ Например, $\myprob{B|B} = 1.$
    \end{enumerate}
\end{rmrk}

\subsubsection{Независимость событий}

\begin{defn}
    Пусть есть вероятностное пространство $(\Omega, \mathcal{F}, \mathbb{P})$. События $A_1, \ldots, A_n \in \mathcal{F}$ называются {\it независимыми в совокупности}, если $\forall k \in \overline{2, n}$ и $\forall i_{1}, \ldots, i_{k} \colon 1 \leqslant i_1 < i_2 < \ldots < i_k \leqslant n$ выполняется 
    \begin{equation*}
        \mathbb{P}\left(\bigcap\limits_{j=1}^k A_{i_j}\right) = \prod\limits_{j=1}^k \myprob{A_{i_j}}
    \end{equation*}

Иными словами, события независимы в совокупности, если вероятность одновременного наступления любого набора из этих событий равна произведению вероятностей событий, входящих в этот набор. В частности, при $n = 2$: события $A$ и $B$ независимы, если $\myprob{A \cap B} = \myprob{A}\myprob{B}$.
\end{defn}

\begin{namedthm}[Свойства независимых событий]\leavevmode
    \begin{enumerate}
        \item Если $A = \varnothing$ или $\myprob{A} = 0$, то $\forall B \colon \myprob{B} > 0$ события $A$ и $B$ независимы.
        \item Пусть $A$ и $B$ независимы. Тогда события $\overline{A}$ и $B$, $A$ и $\overline{B}$, $\overline{A}$ и $\overline{B}$ также независимы. 
        \item Пусть $A \subset B$ и $\myprob{A} > 0, \, \myprob{B} < 1$. Тогда $A$ и $B$ зависимы. 
        \item Если события $A$ и $B$ независимы и $\myprob{B} > 0$, то $\myprob{A|B} = \myprob{A}$.
    \end{enumerate}
\end{namedthm}

\begin{proof}
\begin{enumerate} 
    \item Если $A = \varnothing$, то $AB = \varnothing \Rightarrow \myprob{AB} = 0.$ Но $ \myprob{A}\myprob{B} = 0 \cdot \myprob{B} = 0 \Rightarrow \myprob{AB} = \myprob{A} \myprob{B}$.
    
    Если же ${\myprob{A} = 0}$, то ${AB \subset A \Rightarrow \mathbb{P}(AB) \leqslant \myprob{A} = 0}$. В то же время ${0 = \myprob{AB} = 0 \cdot \myprob{B} = \myprob{A} \myprob{B}}$.

    \item Докажем независимость $\overline{A}$ и $B$. Для события $B$ справедливо представление $B = AB \cup \overline{A}B$. Тогда
    \begin{multline*}
        \myprob{B} = \myprob{AB} + \myprob{\overline{A}B}, \text{где}~ \myprob{AB} = \myprob{A}\myprob{B} \Rightarrow \\
        \Rightarrow \myprob{\overline{A}B} = \myprob{B} - \myprob{A}\myprob{B} = \myprob{B} (1 - \myprob{A}) = \myprob{\overline{A}}\myprob{B}
    \end{multline*}
    Независимость $\overline{A}$ и $B$ доказана. Аналогично доказываются остальные утверждения.
    \item Предположим, что события независимы. Тогда $\myprob{AB} = \myprob{A}\myprob{B},$ но в силу вложенности $A \subset B$: $\myprob{AB} = \myprob{A}$, следовательно, $\myprob{B} = 1$, что противоречит условию.
    \item $\myprob{A | B} = \cfrac{\myprob{AB}}{\myprob{B}} = \cfrac{\myprob{A}\myprob{B}}{\myprob{B}} = \myprob{A}$
\end{enumerate}
\end{proof}

\begin{rmrk}
    В общем случае из попарной независимости событий $A_1, \ldots, A_n$ не следует их независимость в совокупности.
    \begin{exmp}
        Рассмотрим вероятностное пространство, в котором всего 4 различных элементарных исхода: $\Omega = \{\omega_1, \omega_2, \omega_3, \omega_4 \}$. Пусть $\mathcal{F}$~--- множество всех подмножеств $\Omega,~\myprob{\{\omega_i\}} = \cfrac{1}{4},~i = \overline{1,4}$.
        
        Рассмотрим три события 
        \begin{equation*}
            A_1 = \{\omega_1, \omega_4 \},~ 
            A_2 = \{\omega_2, \omega_4 \},~
            A_3 = \{\omega_3, \omega_4 \}
        \end{equation*}
        
        Их пересечения имеют вид:
        \begin{equation*}
            A_1A_2 = A_2A_3 = A_3A_1 = \{\omega_4 \},~
            A_1A_2A_3 = \{\omega_4 \}
        \end{equation*}
        
        Докажем, что события $A_1, A_2, A_3$ не являются независимыми в совокупности:
        \begin{gather*}
            \myprob{A_1} = \myprob{A_2} = \myprob{A_3} = \cfrac{1}{2}, \; \myprob{A_1A_2} = \myprob{A_2A_3} = \myprob{A_3A_1} = \cfrac{1}{4}, \\ \myprob{A_1A_2A_3} = \cfrac{1}{4} \neq \cfrac{1}{8} = \myprob{A_1}\myprob{A_2}\myprob{A_3}
        \end{gather*}
    \end{exmp}
\end{rmrk}

\begin{symb}
    \begin{equation*}
        A_{i}^{(\delta)} =
        \begin{cases}
            A_{i}, & \delta = 1; \\
            \overline{A_{i}}, & \delta = 0.
        \end{cases}
    \end{equation*}
\end{symb}

\begin{namedthm}[Критерий независимости]
    События $A_1, \ldots, A_n$ независимы в совокупности $\Leftrightarrow \forall ~ \delta_1, \delta_2, \ldots \delta_n \in \{0, 1\}$ выполнено равенство
    \begin{equation*}
        \mathbb{P}\left( \bigcap_{i=1}^{n} A_{i}^{\left( \delta_{i} \right)} \right)
        = \prod_{i=1}^{n}\mathbb{P}\left( A_{i}^{\left(\delta_{i}\right)} \right)
    \end{equation*}
\end{namedthm}

\begin{namedthm}[Формула полной вероятности]
    Пусть даны события $A, B_1, \ldots, B_n, \ldots$; $\myprob{B_i} > 0, $ причём $B_i B_j = \varnothing~(i \neq j)$ и $\bigcup\limits_{i=1}^{\infty}B_i \supset A~$ (например, $\bigcup\limits_{i=1}^{\infty}B_i = \Omega$). Тогда справедлива формула:
\begin{equation*}
    \mathbb{P}(A)=\sum\limits_{i=1}^{\infty} \mathbb{P}\left(B_{i}\right) \cdot \mathbb{P}\left(A | B_{i}\right)
\end{equation*}
\end{namedthm}

\begin{proof}
    Достаточно заметить, что при вышеперечисленных условиях $A = \bigcup\limits_{i=1}^{\infty}(AB_i),$ и $AB_i \cap AB_j = \varnothing ~(i \neq j).$ Тогда, учитывая $\myprob{B_i} > 0$, получаем
    \begin{equation*}
        \mathbb{P}(A)=\sum\limits_{i=1}^{\infty} \mathbb{P}\left(A B_{i}\right)=\sum\limits_{i=1}^{\infty} \mathbb{P}\left(B_{i}\right) \frac{\mathbb{P}\left(A B_{i}\right)}{\mathbb{P}\left(B_{i}\right)}=\sum\limits_{i=1}^{\infty} \mathbb{P}\left(B_{i}\right) \cdot \mathbb{P}\left(A | B_{i}\right)
    \end{equation*}
\end{proof}

\begin{namedthm}[Формулы Байеса]
    Пусть даны события $A, H_1, \ldots, H_n, \ldots$; ${\myprob{A} > 0}$, ${\myprob{H_i} > 0}$, причём $H_i H_j = \varnothing ~(i \neq j)$ и $\bigcup\limits_{i=1}^\infty H_i \supset A$ (например, $\bigcup\limits_{i=1}^{\infty}H_i = \Omega$). Тогда справедливы {\it формулы Байеса}:
    \begin{equation*}
        \mathbb{P}\left(H_{i} | A\right)= \frac{\mathbb{P}\left(H_{i}\right) \cdot \mathbb{P}\left(A | H_{i}\right)}{\sum\limits_{j=1}^{\infty} \mathbb{P}\left(H_{j}\right) \cdot \mathbb{P}\left(A | H_{j}\right)}, \quad i = \overline{1,n}
    \end{equation*}
\end{namedthm}
\pagebreak
\begin{proof}
    Согласно формуле полной вероятности, в знаменателе дроби стоит вероятность $A$. Тогда
    \begin{equation*}
        \frac{\mathbb{P}\left(H_{i}\right) \cdot \mathbb{P}\left(A | H_{i}\right)}{\mathbb{P}(A)}=\frac{\mathbb{P}\left(H_{i}\right) \cdot \mathbb{P}\left(A H_{i}\right)}{\mathbb{P}(A) \cdot \mathbb{P}\left(H_{i}\right)}=\frac{\mathbb{P}\left(A H_{i}\right)}{\mathbb{P}(A)}=\mathbb{P}\left(H_{i} | A\right) 
    \end{equation*}
\end{proof}

Вероятности $P(H_i)$, вычисленные заранее, до проведения эксперимента, называют {\it априорными вероятностями} (a’priori~--- «до опыта»). Условные вероятности $\myprob{H_i | A}$ называют {\it апостериорными вероятностями} (a’posteriori~--- «после опыта»). Формула Байеса позволяет переоценить заранее известные вероятности после того, как получено знание о результате эксперимента.

\begin{exmp}
    Тест на рак имеет надёжность $99\%$ (т.е. вероятность как положительной, так и отрицательной ошибки равна $1\%$), рак появляется у $1\%$ населения. Какова вероятность того, что человек болен раком, если у него позитивный результат теста?
    
    Составим таблицу для вероятностей всех возможных событий:
    \begin{center}
    \begin{tabular}{|c|c|c|}
    \hline \multirow{2}{*} {Результат теста} & \multicolumn{2}{|c|} {Пациент реально болен} \\
    \cline {2-3} & Да & Нет \\
    \hline Положительный & $0,99 \cdot 0,01$ & $0,01 \cdot 0,99$ \\
    \hline Отрицательный & $0,01 \cdot 0,01$ & $0,99 \cdot 0,99$ \\
    \hline
    \end{tabular}
    \end{center}
    
    Введём следующие обозначения для событий: $H_{+} = \{\text{пациент болен}\}$, $H_{-} = \{\text{пациент здоров}\}$, $R_{+} = \{\text{положительный результат теста}\}$, $R_{-} = \{\text{отрицательный результат теста}\}$. Найдём вероятность события $H_{+}$ при условии $R_{+}$ по формуле Байеса:
    \begin{multline*}
        \mathbb{P}(H_{+}|R_{+}) = \cfrac{\mathbb{P}(H_{+})\mathbb{P}(R_{+}|H_{+})}{\mathbb{P}(H_{+})\mathbb{P}(R_{+}|H_{+}) + \mathbb{P}(H_{-})\mathbb{P}(R_{+}|H_{-})} = \\
        = \cfrac{0,99 \cdot 0,01}{(0,99 \cdot 0,01) + (0,01 \cdot 0,99)} = 0,5
    \end{multline*}
    
    Иными словами, вероятность того, что пациент болен, равна отношению вероятности истинного положительного результата теста к вероятности любого положительного результата.
    
    Рассмотрим более общий случай. Пусть $q$~--- вероятность неправильного результата теста, $p$~--- вероятность заболеть раком, тогда
    \begin{equation*}
        \mathbb{P}(H_{+}|R_{+}) 
        = \cfrac{(1-q) p}{(1-q) p+q(1-p)} 
        = \cfrac{p-q p}{p+q-2 q p}
    \end{equation*}
    Эта функция принимает значение $0,5$ на диагонали $p = q$; ниже диагонали~--- вероятность выше $0,5$, т.е. чтобы верить результатам теста, вероятность болезни должна превышать вероятность его ошибки.
\end{exmp}

\section{Случайная величина. Порождённое и индуцированное вероятностные пространства. Функция распределения, ее свойства}

\begin{defn}
    {\it $\text{Борелевская~} \sigma \text{-алгебра~} \mathfrak{B}$}~--- $\sigma \text{-алгебра}$, порождённая множеством всех открытых интервалов на $\mathbb{R}$ (иными словами, минимальная $\sigma$-алгебра, содержащая все открытые интервалы). Элемент $B \in \mathfrak{B}$~--- {\it борелевское множество}.
\end{defn}

\begin{defn}
    {\it Борелевская функция}~--- функция $f: \mathbb{R} \rightarrow \mathbb{R}$:
    \begin{equation*}
        \forall B \in \mathfrak{B} \quad f^{-1}(B) \in \mathfrak{B}
    \end{equation*}
    
    Т.е. борелевская функция - это функция, для которой прообраз (множество $f^{-1}(B) = \{x \colon f(x) \in B\}$) любого борелевского множества также является борелевским множеством.
\end{defn}

\begin{exmp}
    Функция Дирихле $D: \mathbb{R} \rightarrow \{0,1\}$
    \begin{equation*}
        D(x) =
        \begin{cases}
            1, & x \in \mathbb{Q}; \\
            0, & x \in \mathbb{R} \setminus \mathbb{Q}
        \end{cases}
    \end{equation*}
является борелевской. 

В самом деле, прообразом любого борелевского множества ${A \colon 1 \in A, \, 0 \notin A}$ является множество рациональных чисел; прообразом борелевского множества ${B \colon 0 \in B, \, 1 \notin B}$ является множество иррациональных чисел; прооборазом борелевского множества ${C \colon 0 \in C, \, 1 \in C}$ является вся вещественная прямая, а прообразом борелевского множества ${D \colon 0 \notin D, \, 1 \notin D}$ является пустое множество. Но $\mathbb{R}, \, \mathbb{Q}, \, \mathbb{I}, \, \varnothing $~--- борелевские множества, а значит, выполняется определение борелевской функции.
\end{exmp}

\begin{defn}
    Функция $\xi$: $\Omega \mapsto \mathbb{R}^{(n)}$ называется {\it измеримой относительно $\sigma\text{-алгебры} \: \mathcal{F}$}, если полный прообраз борелевского множества $B$ лежит в $\sigma\text{-алгебре} \: \mathcal{F}$, т.е. 
    \begin{equation*}
        \xi^{-1}(B) = \{\omega \colon \xi(\omega) \in B \} \in \mathcal{F} \quad \forall B \in \mathfrak{B}
    \end{equation*}
\end{defn}

\begin{rmrk}
    Борелевская функция ~--- это функция, измеримая относительно борелевской ${\sigma \text{-алгебры}}$.
\end{rmrk}

\subsubsection{Случайные величины}
\begin{defn}
    Пара $(X, \mathcal{U})$, где $X$ ~--- произвольное множество, а $\mathcal{U}$ ~--- $\sigma$-алгебра над ним ~--- {\it измеримое пространство}.
    Например, $(\Omega, \mathcal{F})$ и $(\mathbb{R}, \mathfrak{B})$ - измеримые пространства.
    
    Элементы $\sigma$-алгебры $\mathcal{U}$ называются {\it измеримыми множествами}.
\end{defn}

\begin{defn}
    Пусть даны измеримые пространства $(\Omega, \mathcal{F})$ и $(\mathbb{R}, \mathfrak{B})$. Тогда измеримая относительно $\mathcal{F}$ функция $\xi: \Omega \to \mathbb{R}$ называется {\it случайной величиной}.
\end{defn}
\begin{rmrk}
    Если мы вспомним, что элементы $\sigma$-алгебры $\mathcal{F}$ называются событиями, то определение можно переформулировать следующим образом: 
    
    Пусть даны измеримые пространства $(\Omega, \mathcal{F})$ и $(\mathbb{R}, \mathfrak{B})$. Функция $\xi \colon \Omega \mapsto \mathbb{R}$ называется случайной величиной, если прообраз любого борелевского множества $B \in \mathfrak{B}$ является событием.
\end{rmrk}
\begin{exmp} Пусть дана функция $\xi$:
\begin{equation*}
    \xi(\omega) = 
    \begin{cases}
        1, & \omega \in \left[0; \frac{1}{2} \right]; \\
        0, & \omega \in (\frac{1}{2}; 1],
    \end{cases}
\end{equation*}

$\Omega = [0; 1], \mathcal{F} = \{\varnothing, \Omega\}$~--- минимальная ${\sigma \text{-алгебра}}$.  

Докажем неизмеримость функции $\xi$; для этого достаточно найти такое борелевское множество, прообраз которого не будет принадлежать ${\sigma \text{-алгебре}}$. В данном случае $\mathcal{F}$ состоит всего лишь из двух множеств~--- $\{[0; 1], \varnothing\}$.

Как и в примере с функцией Дирихле, попробуем перебрать борелевские множества, содержащие значения $\xi(\omega)$. Тогда мы увидим, что для любого борелевского множества $A \colon 0 \in A, \, 1 \notin A$~--- например, множества ${A_1 = (-\infty, \frac{1}{3})}$~--- его прообразом является множество $(\frac{1}{2}, 1]$. Но это множество не входит в $\mathcal{F}$, а значит, $\xi(\omega)$ неизмерима.

Отсюда можно сделать несколько выводов. Во-первых, измеримость функции зависит от выбора ${\sigma \text{-алгебры}}$. Например, если мы рассмотрим ту же функцию $\xi(\omega)$ на том же $\Omega = [0; 1]$, но с другой ${\sigma \text{-алгеброй}}$ ${\widehat{\mathcal{F}} = \{[0; 1], [0; \frac{1}{2}], (\frac{1}{2}, 1], \varnothing\}}$, то наша функция будет измеримой, а следовательно ~--- случайной величиной.

Во-вторых (забегая немного вперёд), именно из-за неизмеримости $\xi(\omega)$ относительно $\mathcal{F}$ мы не можем посчитать вероятность попадания значений этой функции в некоторые интервалы, к примеру, $\myprob{\xi < \frac{1}{3}}$. Ведь $\myprob{\xi < \frac{1}{3}} = \myprob{\xi \in A_1} = \myprob{\omega \in (\frac{1}{2}, 1]}$, но множество $(\frac{1}{2}, 1] \notin \mathcal{F}$, а вероятность~--- это отображение $\mathbb{P}: \mathcal{F} \mapsto \mathbb{R}$, и она не определена для этого множества.
\end{exmp} 

\begin{thm*}
    Пусть $\xi$~--- случайная величина, $g: \mathbb{R} \rightarrow \mathbb{R}$~--- борелевская функция. Тогда $g(\xi)$~--- случайная величина.
\end{thm*}

\begin{proof}
    Проверим, что прообраз любого борелевского множества при отображении $g(\xi) \colon \Omega \mapsto \mathbb{R}$ является событием. Возьмём произвольное $B \in \mathfrak{B}(\mathbb{R})$ и положим $B_1 = g^{-1}(B)$. Множество $B_1$~--- борелевское, так как функция $g$ борелевская. Но тогда $g^{-1}(B) = \xi^{-1}(B_1)$. Это множество принадлежит $\mathcal{F}$, поскольку $B_1$~--- борелевское множество и $\xi$~--- случайная величина. 
\end{proof}

\begin{thm*}
    Пусть $\mathcal{E}$~--- класс подмножеств $\mathbb{R}$, $\sigma(\mathcal{E}) = \mathfrak{B}$ (например, множество интервалов).

    Тогда $\xi$~--- случайная величина $\Leftrightarrow$ $\forall E \in \mathcal{E}: ~\xi^{-1}(E) \in \mathcal{F}$.
\end{thm*}

\begin{proof}
    \begin{itemize}
        \item[$\Leftarrow$] Пусть $\mathcal{D} = \{D \colon D \in \mathfrak{B}, \, \xi^{-1}(D) \in \mathcal{F} \}$. Тогда $\mathcal{E} \subseteq \mathcal{D}$. Далее, в силу свойств прообразов и случайной величины $\xi$:
    \begin{gather*}
        \xi^{-1}\left(\bigcup\limits_\alpha A_\alpha\right) 
        = \bigcup\limits_\alpha \xi^{-1}(A_\alpha), \quad
        \xi^{-1}(\overline{A}) 
        = \overline{\xi^{-1}(A)}, \\
        \xi^{-1}\left(\bigcap\limits_\alpha A_\alpha\right) = \bigcap\limits_\alpha \xi^{-1}(A_\alpha),
    \end{gather*}
    следовательно, $\mathcal{D}$~--- ${\sigma \text{-алгебра}}$. $\mathfrak{B} = \sigma(\mathcal{E}) \subseteq \sigma(\mathcal{D}) = \mathcal{D} \subseteq \mathfrak{B} \Rightarrow \mathfrak{B} = \mathcal{D}.$
    
    \item[$\Rightarrow$] Следует непосредственно из определения случайной величины.
    \end{itemize}
\end{proof}

\begin{crlr}
    $\xi$~--- случайная величина $\Leftrightarrow$ $\forall x \in \mathbb{R}:~ \{\omega \colon \xi(\omega) < x \} \in \mathcal{F}$. Причем вместо знака $<$ может стоять любой другой знак неравенства, как строгого, так и нестрогого.
\end{crlr}

\subsubsection{Порождённое и индуцированное вероятностные пространства}
\begin{symb}
    $\sigma\text{-алгебра}$, порожденная случайной величиной $\xi$:
    \begin{equation*}
        \mathcal{F}_\xi = \{\xi^{-1}(B), \, B \in \mathfrak{B} \}
    \end{equation*}
\end{symb}

Отметим следующие факты:
\begin{enumerate}
    \item $\mathcal{F}_\xi \subset \mathcal{F}.$
    \item $\mathcal{F}_\xi$~--- ${\sigma \text{-алгебра}}$. Действительно
    \begin{equation*}
        \xi^{-1}(\overline{B}) = \overline{\xi^{-1}(B)}, \quad
        \xi^{-1}\left(\bigcup\limits_{i=1}^{\infty}B_i\right) = \bigcup\limits_{i=1}^\infty \xi^{-1}(B_i),
    \end{equation*}
   если $B_i$ попарно не пересекаются.
\end{enumerate}

\begin{defn}
    Вероятностное пространство $(\Omega,\mathcal{F}_\xi,\mathbb{P})$ называется {\it порожденным случайной величиной $\xi$}.
\end{defn}

\begin{defn}
    {\it Распределение случайной величины} $\xi$~--- функция ${P_\xi: \mathfrak{B} \mapsto \mathbb{R}}$:
    \begin{equation*}
        P_\xi(B) = \myprob{\xi^{-1}(B)} = \myprob{\xi \in B}
    \end{equation*}
\end{defn}

\begin{rmrk}
    Можно рассматривать распределение как композицию отображений. Если $\xi: \Omega \mapsto \mathbb{R}$, то полный прообраз~--- это отображение $\xi^{-1}: \mathfrak{B} \mapsto \mathcal{F}$. В свою очередь, $\mathbb{P}: \mathcal{F} \mapsto \mathbb{R}$. Тогда
    \begin{equation*}
        P_{\xi} = \mathbb{P} \circ \xi^{-1}, \quad P_{\xi}: \mathfrak{B} \mapsto \mathbb{R}.
    \end{equation*}
\end{rmrk}

\begin{defn}
    Вероятностное пространство $(\mathbb{R}, \mathfrak{B}, P_\xi)$ называется {\it индуцированным случайной величиной $\xi$}.
\end{defn}

\subsubsection{Функция распределения, её свойства}
\begin{defn}
    {\it Функция распределения} $F_\xi (x)$ случайной величины $\xi$~--- функция $F_\xi: \mathbb{R} \rightarrow \mathbb{R}$:
    \begin{equation*}
        F_{\xi}(x)=P_{\xi}((-\infty, x))=\mathbb{P}(\xi<x)
    \end{equation*}
\end{defn}

\begin{thm*}
    $F_\xi(x)$ однозначно определяет $P_\xi(B)$.
\end{thm*}
\begin{proof}
    Действительно, любое борелевское множество может быть представлено в виде разности числовой оси, одной или двух полупрямых и не более чем счётного объединения отрезков. В силу однозначности определения $P_\xi([a;b]) = F_\xi(b + 0) - F_\xi(a)$ утверждение теоремы справедливо.
\end{proof}

\begin{namedthm}[Свойства функции распределения]\leavevmode
\begin{enumerate}
    \item $\forall x~ 0 \leqslant F_\xi(x) \leqslant 1$;
    \item $F_\xi(x)$ монотонно неубывает (т.е. $x_1 < x_2 \Rightarrow F_\xi(x_1) \leqslant F_\xi(x_2)~ \forall x_1, x_2$);
    \item $\lim\limits_{x \rightarrow +\infty} F_\xi(x) = 1, \lim\limits_{x \rightarrow -\infty} F_\xi(x) = 0$;
    \item $F_\xi(x)$ непрерывна слева (т.е. $F_\xi(x_0 - 0) = \lim\limits_{x \rightarrow x_0 - 0}F_\xi(x) = F_\xi(x_0)$).
\end{enumerate}
\end{namedthm}

\begin{proof}
\begin{enumerate}
    \item Следует из свойств вероятности.
    \item 
        $x_1 < x_2 \Rightarrow \{\xi < x_1 \} \subseteq \{\xi < x_2\}$. Из монотонности вероятности следует:
        \begin{equation*}
            \myprob{\xi < x_1} = F_\xi(x_1) \; \leqslant \; \myprob{\xi < x_2} = F_\xi(x_2).
        \end{equation*}
    \item 
        Пределы существуют в силу монотонности и ограниченности $F_\xi(x)$. Докажем, что $F_\xi(-n) \xrightarrow[n \to +\infty]{} 0$.
        
        Рассмотрим последовательность вложенных событий $B_n = \{\xi < -n \}$, $B_{n+1} = \{\xi < -(n+1) \} \subseteq B_n = \{\xi < -n \} ~ \forall n ~ \geqslant 1$:
        \begin{equation*}
            \bigcap\limits_{j = 1}^{\infty}B_j = \{\omega ~|~ \xi(\omega) < x, \forall x \in \mathbb{R} \} \Rightarrow \bigcap\limits_{j = 1}^{\infty}B_j = \varnothing.
        \end{equation*}
    
        $F_\xi(-n) = \myprob{B_n} \xrightarrow[n \to +\infty]{} \myprob{B} = 0$ (в силу непрерывности вероятностной меры)
    
        Отсюда следует: $F_\xi(n) \xrightarrow[n \to +\infty]{} 1 \Leftrightarrow 1 - F_\xi(n) = \myprob{\xi \geqslant n} \xrightarrow[n \to +\infty]{} 0$.
    
        Аналогично, пусть $B_n = \{\xi \geqslant n\}, B_{n+1} = \{\xi \geqslant (n+1) \} \subseteq B_n, \ldots$:
        \begin{equation*}
            \bigcap\limits_{j = 1}^{\infty}B_j = \varnothing \Rightarrow \xi(w) > x \quad \forall x \in \mathbb{R}
        \end{equation*}
        Следовательно, 
        \begin{equation*}
            1 - F_\xi(n) = \myprob{B_n} \xrightarrow[n \to +\infty]{} \myprob{B} = 0 \Rightarrow F_\xi(n) \xrightarrow[n \to +\infty]{}1
        \end{equation*}
    \item
        Докажем, что $F_\xi(x_0 - \frac{1}{n}) \xrightarrow[n \to +\infty]{} F_\xi(x_0)$, что равносильно 
        \begin{multline*}
            F_\xi(x_0) - F_\xi \left(x_0 - \frac{1}{n} \right) 
            = \myprob{\xi < x_0} - \mathbb{P}\left( \xi < x_0 - \frac{1}{n} \right) = \\ 
            = \mathbb{P}\left( x_0 - \frac{1}{n} \leqslant \xi < x_0 \right) \xrightarrow[n \to +\infty]{} 0
                \end{multline*}
        Сходимость выполняется в силу непрерывности вероятностной меры.
\end{enumerate}
\end{proof}

\begin{task}
Пусть есть не более чем счётное множество элементарных исходов $\Omega$. Рассмотрим функции ${f(\omega) \colon \Omega \mapsto \mathbb{R}}$.

Какая функция всегда измерима (т.е. измерима относительно любой ${\sigma \text{-алгебры}}$)? 
% Константа
Относительно какой ${\sigma \text{-алгебры}}$ измерима любая функция ${f(\omega) \colon \Omega \mapsto \mathbb{R}}$?
% Полной сигма-алгебры (2 ^ Omega)
\end{task}
 
\section{Дискретные, сингулярные и абсолютно непрерывные функции распределения и случайные величины. Плотность распределения. Теорема Лебега о разложении функции распределения}

\begin{defn}
    Распределение $\xi$ называется {\it дискретным}, если существует не более чем счётное множество $B$, т.ч. $P_\xi(B) = 1$. \textit{Дискретная функция распределения} имеет вид:
    \begin{equation*}
        F_\xi(x) = \mathbb{P}(\xi \leqslant x) = \sum\limits_{x_i \leqslant x}{}p_{i} = \sum\limits_{x_i \leqslant x}{}\mathbb{P}(\xi = x_{i})
    \end{equation*}
\end{defn}

\begin{rmrk}
    Для любой дискретной функции распределения $F_\xi(x)$ число скачков~--- не более чем счётное.
    
    Действительно, можно перенумеровать все скачки следующим образом:
    \begin{equation*}
        \Delta_{n}=\left\{t \colon F_{x}(t+0)-F_{x}(t)>\frac{1}{n}\right\},~ |\Delta_{n} | \leqslant n
    \end{equation*}
    
    Т.е. на каждом шаге мы считаем все скачки величины более $\frac{1}{n}$, а таких скачков не более чем $n$, так как функция распределения ограничена снизу нулём, сверху единицей, и, кроме того, монотонна.
    Множество точек разрыва представимо в виде $\bigcup\limits_{n = 1}^{\infty} \Delta_{n}$, т.е. не более чем счётно.
\end{rmrk}

\begin{defn}
    Распределение $\xi$ называется {\it абсолютно непрерывным}, если существует $f(x) \overset{\text{п.н.}}{\geqslant} 0$ такая, что для любого борелевского множества $B$ справедливо
    \begin{equation*}
        P_\xi(B) = \int\limits_B f(x) \lambda(dx),
    \end{equation*}
    
    где $f(x)$~--- {\it плотность распределения}, $\lambda$~--- мера Лебега. Абсолютно непрерывная функция распределения имеет вид:
    \begin{equation*}
        F_\xi(x) = \mathbb{P}(\xi \leqslant x) = \int\limits_{-\infty}^x f(t)dt
    \end{equation*}
\end{defn}
\begin{rmrk}
    $f(x) \overset{\text{п.н.}}{\geqslant} 0$ (почти наверное), если множество точек, где это неравенство не выполняется, имеет меру нуль по Лебегу, т.е. $\mathbb{P}(f(x) < 0) = 0$.
\end{rmrk}
\begin{rmrk}
    В некоторых вариантах определения от плотности требуется неотрицательность не почти наверное, а всюду на $\mathbb{R}$. Это вопрос соглашения, так как интеграл по множеству меры нуль в любом случае равен нулю.
\end{rmrk}

\begin{rmrk}
    В случае абсолютно непрерывного распределения вероятность попасть в конкретную точку равна нулю. Действительно,
    \begin{equation*}
        \mathbb{P}(\xi = x) = \int\limits_{x}^{x} f(t)dt = 0
    \end{equation*}
\end{rmrk}
\begin{namedthm}[Свойства плотности]\leavevmode
\begin{enumerate}
    \item $f_{\xi}(x) = \dfrac{\partial{F}}{\partial{x}}(x)$ почти всюду 
    
    (кроме, может быть, множества меры нуль по Лебегу~--- например, функция равномерного распределения $\mathbf{U}[0;1]$ не дифференцируема в точках $0$ и $1$);
    \item $f_\xi(x) \overset{\text{п.н.}}{\geqslant} 0~ \forall x$ (неотрицательность);
    \item $\int\limits_{-\infty}^{+\infty} f_\xi(t) dt = 1$ (нормировка).
\end{enumerate}
\end{namedthm}
\begin{proof}
    Первое свойство очевидно из свойств интегралов с переменным верхним пределом, второе выполнено в силу определения плотности распределения. Рассмотрим третье. Если в определении абсолютно непрерывного распределения в качестве борелевского множества взять всю числовую прямую, получим: 
    \begin{equation*}
        \mathbb{P}(\xi \in \mathbb{R})=1=\int\limits_{\mathbb{R}} f_{\xi}(x) dx
    \end{equation*}
\end{proof}

\begin{defn}
    {\it Точка роста} функции распределения $F_\xi(x)$~--- точка $x_0$:
\begin{equation*}
    \forall \varepsilon > 0~ F_\xi(x_0 + \varepsilon) - F_\xi(x_0 - \varepsilon) > 0
\end{equation*}
\end{defn}

\begin{rmrk}
    Возможен случай, когда точка роста является точкой разрыва:
    \begin{multline*}
    \lim _{\varepsilon \to 0} \quad F_{\xi}\left(x_{0}+\varepsilon\right)-F_{\xi}\left(x_{0}-\varepsilon\right)=F_{\xi}\left(x_{0}+0\right)-F_{\xi}\left(x_{0}\right)>0 \Leftrightarrow \\
    \Leftrightarrow \lim_{\varepsilon \to 0}\mathbb{P}\left(x_{0}-\varepsilon \leqslant \xi<x_{0}+\varepsilon\right)=\mathbb{P}\left(\xi=x_{0}\right)>0
    \end{multline*}
\end{rmrk}

\begin{defn}
    Функция распределения $F_\xi(x)$ называется {\it сингулярной}, если она непрерывна и множество точек её роста имеет нулевую меру Лебега.
\end{defn}
\begin{exmp}
    Сингулярной функцией является \textit{Канторова лестница} $c [0;1] \mapsto [0;1]$, которая строится следующим образом:
    
    $c(0) = 0, c(1) = 1$. Далее интервал $(0, 1)$ разбивается на три равные части $(0, \frac{1}{3})$, $(\frac{1}{3}, \frac{2}{3})$, $(\frac{2}{3}, 1)$. На среднем сегменте полагаем $c(x) = \frac{1}{2}$, оставшиеся два сегмента снова разбиваются на три равные части каждый, и на соответствующих средних сегментах полагаем $c(x) = \frac{1}{4}$ и $c(x) = \frac{3}{4}$. Каждый из оставшихся сегментов снова делится на три части, и на внутренних сегментах $c(x)$ определяется как постоянная, равная среднему арифметическому между соседними, уже определенными значениями $c(x)$. На остальных точках единичного отрезка определяется по непрерывности. 
\end{exmp}

\begin{namedthm}[Теорема Лебега о разложении функции распределения]
    Пусть $\xi$~--- случайная величина с функцией распределения $F_\xi(x).$ Тогда существуют и определены единственным образом три функции распределения $F_{ac}(x), F_s(x), F_d(x)$, абсолютно непрерывная, сингулярная и дискретная соответственно, а также три числа $p_1, p_2, p_3 \geqslant 0, p_1 + p_2 + p_3 = 1$ такие, что 
    \begin{equation*}
        F_{\xi}(x)=p_{1} F_{ac}(x)+p_{2} F_{s}(x)+p_{3} F_{d}(x).
    \end{equation*}
\end{namedthm}

\section{Числовые характеристики случайных величин: моменты, математическое ожидание, дисперсия. Их свойства}

\subsubsection{Математическое ожидание случайной величины}

\begin{defn}
    Пусть задано вероятностное пространство $(\Omega, \mathcal{F}, \mathbb{P})$ и случайная величина $\xi: \Omega \mapsto \mathbb{R}$. Если существует интеграл Лебега от $\xi$ по мере $\mathbb{P}$ по множеству $\Omega$, то он называется {\it математическим ожиданием} случайной величины $\xi$ и обозначается как $\mathbb{E}\xi$ или $\operatorname{M}\xi$.
    $$ \mathbb{E}\xi = \int\limits_{\Omega} \xi(\omega) \mathbb{P}(d\omega).$$
\end{defn}

\begin{defn}
    {\it Математическое ожидание (среднее значение, первый момент)} случайной величины $\xi$, имеющей дискретное распределение со значениями $a_1, a_2, \ldots$~--- сумма (абсолютно) сходящегося ряда
    \begin{equation*}
        \mathbb{E} \xi=\sum\limits_{i} a_{i} p_{i}=\sum\limits_{i} a_{i} \mathbb{P}\left(\xi=a_{i}\right).
    \end{equation*}
\end{defn}

\begin{defn}
    {\it Математическое ожидание} случайной величины $\xi$, имеющей абсолютно непрерывное распределение с плотностью распределения $f(x)$~--- значение (абсолютно) сходящегося интеграла
    \begin{equation*}
        \mathbb{E} \xi=\int\limits_{\mathbb{R}} x f(x) dx.
    \end{equation*}
\end{defn}

Математическое ожидание имеет простой физический смысл: если на прямой разместить единичную массу, поместив в точки $a_i$ массу $p_i$ (для дискретного распределения) или «размазав» её с плотностью $f_\xi(x)$ (для абсолютно непрерывного распределения), то точка $\mathbb{E}\xi$ будет координатой <<центра тяжести>> прямой.

\begin{namedthm}[Свойства математического ожидания]
    Везде далее предполагается, что рассматриваемые математические ожидания существуют.
\begin{enumerate}
    \item Для произвольной борелевской функции $g(x)$ со значениями в $\mathbb{R}$
    $$\mathbb{E} g(\xi)=\left\{\begin{array}{l}\sum\limits_{k} g\left(a_{k}\right) \mathbb{P}\left(\xi=a_{k}\right), \text {если распределение} \: \xi \: \text {дискретно;} \\ \int\limits_{-\infty}^{+\infty} g(x) f_{\xi}(x) d x, \text {если распределение} \: \xi \: \text {абсолютно непрерывно.}\end{array}\right.$$
    
    Такое же свойство верно и для числовых функций нескольких аргументов $g(x_1, \ldots,x_n)$, если $\xi$ - вектор из $n$ случайных величин, а в сумме и в интеграле участвует их совместное распределение. Например, для $g(x,y) = x + y$ и для случайных величин $\xi$ и $\eta$ с плотностью совместного распределения $f(x,y)$ верно: 
    \begin{equation}
        \mathbb{E}(\xi+\eta)=\int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty}(x+y) f(x, y) d x d y
    \end{equation}
    
    \item Математическое ожидание постоянной равно ей самой: $\mathbb{E} c=c.$
    \item Постоянный множитель можно вынести за знак математического ожидания: $\mathbb{E}(c\xi) = c\mathbb{E}\xi.$
    
    Это следует из свойства 1. при $g(x) = cx$.
    \item Математическое ожидание суммы {\it любых} случайных величин равно сумме их математических ожиданий: $\mathbb{E}(\xi + \eta) = \mathbb{E}\xi + \mathbb{E}\eta.$
    
    \item Если $\xi \overset{\text{п.н.}}{\geqslant} 0$, то $\mathbb{E}\xi \geqslant 0.$
    
    \mycon{} Если $\xi \overset{\text{п.н.}}{\leqslant} \eta$, то $\mathbb{E}\xi \leqslant \mathbb{E}\eta.$
    
    \mycon{} Если $a \overset{\text{п.н.}}{\leqslant} \xi \overset{\text{п.н.}}{\leqslant} b$, то $a \leqslant \mathbb{E}\xi \leqslant b$.
    
    \item Математическое ожидание произведения {\it независимых} случайных величин равно произведению их математических ожиданий.
    
    {\bf Замечание.}
        Обратное неверно: из равенства $\mathbb{E}(\xi \eta) = \mathbb{E}\xi \mathbb{E} \eta$ {\it не следует} независимость величин $\xi$ и $\eta$.
    
    \item $|\mathbb{E}\xi| \leqslant \mathbb{E}|\xi|.$
    
    \item $\xi \overset{\text{п.н.}}{\geqslant} 0, \: \mathbb{E}\xi = 0 \quad \Rightarrow \quad \xi \overset{\text{п.н.}}{=} 0.$
    
    \item $\myprob{A} = \mathbb{E}\left[\mathbb{I}_A(\omega)\right].$
    
    \item Если функция $g(x)$ выпукла, то $\mathbb{E}g(\xi) \geqslant g(\mathbb{E}\xi)$ (неравенство Йенсена).
    
\end{enumerate}
\end{namedthm}

\begin{proof}
\begin{enumerate}
    \item Достаточно рассмотреть случайную величину $\eta = g(\xi)$ на том же вероятностном пространстве и заметить, что $\forall \: \omega \in \Omega \colon \eta(\omega) = g(\xi(\omega))$. Тогда 
    $$ \mathbb{E}\eta = \int\limits_{\Omega} \eta(\omega) \mathbb{P}(d\omega) = \int\limits_{\Omega} g(\xi(\omega)) \mathbb{P}(d\omega). $$
    Отсюда и вытекают формулы для дискретного и абсолютного случая.
    
    \item Рассмотрим функцию $g(x) \equiv c$ и произвольную случайную величину $\xi$. Тогда 
    $$ \mathbb{E}g(\xi) = \int\limits_{\Omega}g(\xi(\omega))\mathbb{P}(d\omega) = 
    c \int\limits_{\Omega}\mathbb{P}(d\omega) = c \, \myprob{\Omega} = c.$$
    
    \item Рассмотрим функцию $g(x) = cx$. Тогда
    $$ \mathbb{E}(c\xi) = \mathbb{E}g(\xi) = \int\limits_{\Omega}g(\xi(\omega))\mathbb{P}(d\omega) = 
    c \int\limits_{\Omega}\xi(\omega)\mathbb{P}(d\omega) = 
    c \, \mathbb{E}\xi.$$
    
    \item Воспользуемся равенством (1) и теоремой о совместном распределении:
        $$\begin{aligned}
        \mathbb{E}(\xi+\eta) &=\int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty}(x+y) f(x, y) d x d y=\\
        &=\int\limits_{-\infty}^{\infty} x d x \int\limits_{-\infty}^{\infty} f(x, y) d y+\int\limits_{-\infty}^{\infty} y d y \int\limits_{-\infty}^{\infty} f(x, y) d x=\\
        &=\int\limits_{-\infty}^{\infty} x f_{\xi}(x) d x+\int\limits_{-\infty}^{\infty} y f_{\eta}(y) d y=\mathbb{E} \xi+\mathbb{E} \eta.
        \end{aligned}$$
    \item Неотрицательность $\xi$ означает, что $a_i \geqslant 0$ при всех $i \colon p_i > 0$ в случае дискретного распределения, либо $f_\xi(x) = 0$ при $x < 0$ (кроме, может быть, множества меры нуль) - для абсолютно непрерывного распределения. И в том, и в другом случае имеем:
        $$\mathbb{E} \xi=\sum a_{i} p_{i} \geqslant 0 \quad \text {или} \quad \mathbb{E} \xi=\int\limits_{0}^{\infty} x f(x) d x \geqslant 0.$$
        
    \item В равенстве (1) заменим сложение умножением и плотность совместного распределения произведением плотностей (это возможно в силу независимости случайных величин):
        $$\begin{aligned}
        \mathbb{E}(\xi \eta) &=\int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} x y f_{\xi}(x) f_{\eta}(y) d x d y=\\
        &=\int\limits_{-\infty}^{\infty} x f_{\xi}(x) d x \int\limits_{-\infty}^{\infty} y f_{\eta}(y) d y=\mathbb{E} \xi \mathbb{E} \eta ~~~ \square.
        \end{aligned}$$
        
    \item Это верно в силу неравенства треугольника (для дискретного случая) и аналогичного неравенства для интегралов (для непрерывного случая).
    
    \item Это свойство мы докажем, заранее предполагая, что $\xi$ имеет дискретное распределение с неотрицательными значениями $a_k \geqslant 0$. Равенство $\mathbb{E}\xi = \sum a_k p_k = 0$ означает, что все слагаемые в этой сумме равны нулю, т. е. все вероятности $p_k$ нулевые, кроме вероятности, соответствующей значению $a_k = 0$.
    
    \item Это верно в силу определений функции-индикатора и матожидания.
    
    \item Начнём с утверждения: если функция $g$ выпукла, то для любого ${y \in \mathbb{R}} \; {\exists \: c = c(y) \colon} \forall \: x \in \mathbb{R} \; g(x) \geqslant g(y) + c(y)(x - y)$. Это вытекает из того, что график выпуклой функции лежит не ниже любой из касательной к нему*. Положим в этом неравенстве $y = \mathbb{E}\xi$. Тогда
    $$ g(\xi) \geqslant g(\mathbb{E}\xi) + c(\mathbb{E}\xi)(\xi - \mathbb{E}\xi) $$
    $$ \mathbb{E}g(\xi) \geqslant \mathbb{E}g(\mathbb{E}\xi) +  \mathbb{E}c(\mathbb{E}\xi)(\xi - \mathbb{E}\xi)$$
    Здесь $g(\mathbb{E}\xi), c(\mathbb{E}\xi)$ - константы, а $\mathbb{E}(\xi - \mathbb{E}\xi) = 0$, а значит,
    $$ \mathbb{E}g(\xi) \geqslant g(\mathbb{E}\xi).$$
    
    *Вообще говоря, выпуклая функция может не иметь первой производной и, следовательно, касательной на не более чем счётном множестве точек, но тогда можно заменить касательную на опорную гиперплоскость.
\end{enumerate}
\end{proof}

\subsubsection{Дисперсия и моменты старших порядков}

\begin{defn}
    Пусть ${\mathbb{E}|\xi|^k < \infty}$. Число ${\mathbb{E}\xi^k}$ называется {\it моментом порядка $k$ или $k$-м моментом} случайной величины $\xi$, ${\mathbb{E}|\xi|^k}$~--- {\it абсолютным $k$-м моментом}, ${\mathbb{E}(\xi - \mathbb{E}\xi)^k}$~--- {\it центральным $k$-м моментом}, наконец, ${\mathbb{E}|\xi - \mathbb{E}\xi|^k}$~--- {\it абсолютным центральным $k$-м моментом} случайной величины $\xi$.
\end{defn}

\begin{defn}
    Число $\mathbb{D}\xi = \mathbb{E}(\xi - \mathbb{E}\xi)^2$ (центральный момент второго порядка) называется {\it дисперсией} случайной величины $\xi$, $\sigma = \sqrt{\mathbb{D}\xi}$~--- её {\it среднеквадратичным отклонением}.
\end{defn} 

\begin{thm*}
    Если существует момент порядка $t > 0$ случайной величины $\xi$, то существует и ее момент порядка $s$, где $0 < s < t$.
\end{thm*}

\begin{proof} 
Заметим, что $|\xi|^s \leqslant |\xi|^t + 1.$ В силу следствия из свойства 5 для математического ожидания можно получить из неравенства для случайных величин такое же неравенство для их математических ожиданий: $\mathbb{E}|\xi|^s \leqslant \mathbb{E}|\xi|^t + 1 < \infty.$
\end{proof}

\subsubsection{Свойства дисперсии}

\begin{rmrk}
        Во всех свойствах предполагается существование вторых моментов случайных величин. Тогда (в силу вышеописанной теоремы) существуют и сами матожидания.
\end{rmrk} 

\begin{enumerate}

    \item Дисперсия может быть вычислена по формуле: $\mathbb{D}\xi = \mathbb{E}\xi^2 - (\mathbb{E}\xi)^2$.
    
    \begin{proof}
        Обозначим для удобства $a = \mathbb{E}\xi.$ Тогда
    $$\mathbb{D} \xi=\mathbb{E}(\xi-a)^{2}=\mathbb{E}\left(\xi^{2}-2 a \xi+a^{2}\right)=\mathbb{E} \xi^{2}-2 a \mathbb{E} \xi+a^{2}=\mathbb{E} \xi^{2}-a^{2}.$$
    \end{proof} 
    
    \item При умножении случайной величины на постоянную $c$ дисперсия увеличивается в $c^2$ раз: $\mathbb{D}(c\xi) = c^2\mathbb{D}\xi.$
    \item Дисперсия всегда неотрицательна: $\mathbb{D}\xi \geqslant 0.$
    
    \begin{proof}
        Пусть $a = \mathbb{E}\xi.$ Дисперсия есть математичекое ожидание неотрицательной случайной величины $(\xi - a)^2$, откуда (и из свойства 5 матожидания) следует неотрицательность дисперсии.
    \end{proof}
    
    \item Дисперсия обращается в нуль лишь для вырожденного распределения: если $\mathbb{D}\xi = 0$, то $\xi \overset{\text{п.н.}}{=} \text{const}$, и наоборот.
    
    \begin{proof}
        $\mathbb{D}\xi = 0 \Rightarrow (\xi - a)^2 \overset{\text{п.н.}}{=} 0, \xi \overset{\text{п.н.}}{=} a =\text{const}$. И наоборот: если $\xi \overset{\text{п.н.}}{=} c$, то $\mathbb{D} \xi=\mathbb{E}(c-\mathbb{E} c)^{2}=\mathbb{E} 0=0.$
    \end{proof} 
    
    \item Дисперсия не зависит от сдвига случайной величины на постоянную: $\mathbb{D}(\xi + c) = \mathbb{D}\xi.$
    
    \item Если $\xi$ и $\eta$ независимы, то $\mathbb{D}(\xi + \eta) = \mathbb{D}\xi + \mathbb{D}\eta.$
    
    \begin{proof}
    Действительно, применяя свойство (6) матожидания, получим:
    $$\begin{aligned}
    \mathbb{D}(\xi+\eta) &=\mathbb{E}(\xi+\eta)^{2}-(\mathbb{E}(\xi+\eta))^{2}=\\
    &=\mathbb{E} \xi^{2}+\mathbb{E} \eta^{2}+2 \mathbb{E}(\xi \eta)-(\mathbb{E} \xi)^{2}-(\mathbb{E} \eta)^{2}-2 \mathbb{E} \xi \mathbb{E} \eta=\mathbb{D} \xi+\mathbb{D} \eta
    \end{aligned}.$$
    \end{proof}
    
    \begin{rmrk}
        Обратное, аналогично замечанию к свойству (6) матожидания, неверно.
    \end{rmrk} 
    
    \begin{crlr}
        Если $\xi$ и $\eta$ независимы, то $\mathbb{D}(\xi-\eta)=\mathbb{D} \xi+\mathbb{D} \eta$. 
    \end{crlr} 
    
    \begin{proof}
    Из свойств (6) и (2) получим: 
    $$\mathbb{D}(\xi-\eta)=\mathbb{D}(\xi+(-\eta))=\mathbb{D} \xi+\mathbb{D}(-\eta)=\mathbb{D} \xi+(-1)^{2} \mathbb{D} \eta=\mathbb{D} \xi+\mathbb{D} \eta.$$
    \end{proof} 
    
    \begin{crlr}
        Для произвольных случайных величин $\xi$ и $\eta$ имеет место равенство:
    $$\mathbb{D}(\xi \pm \eta)=\mathbb{D} \xi+\mathbb{D} \eta \pm 2(\mathbb{E}(\xi \eta)-\mathbb{E} \xi \mathbb{E} \eta).$$
    \end{crlr} 
    
    \begin{rmrk}
    В последнем равенстве величина $\mathbb{E}(\xi \eta)-\mathbb{E} \xi \mathbb{E} \eta$ есть {\it ковариация} случайных величин $\xi$ и $\eta$ ~--- $\text{cov}(\xi, \eta)$.
    \end{rmrk}
    
\end{enumerate}

\subsubsection{Другие числовые характеристики}
    \definecolor{color1}{HTML}{ffba00}
    \definecolor{color2}{HTML}{00bfff}
    \definecolor{color3}{HTML}{ff00ff}
    \pgfplotsset{skewkurt/.style={
        height=10cm, width=10cm,
        xmin=-1, xmax=7,
        ymin=-0.05,
        ymax=0.7,
        ticks=none,
        axis line style = thick,
        axis lines = middle,
        enlargelimits=false,
   }}
    \tikzset{
        declare function={normal(\x,\sstd,\mu)=1/sqrt(2*pi*\sstd)*exp(-(\x-\mu)^2/(2*\sstd));},
        declare function={
            cdfapp(\x)=1/(1 + exp(-0.07056*(\x)^3 - 1.5976*(\x);
       },
        declare function={
            sknorm(\x,\mu,\std,\alpha)=
            2 * normal(\x,\std*\std,\mu) * 
            cdfapp(\alpha*(\x-\mu)/\std);
       },
   }
\begin{defn}
    {\it Коэффициент асимметрии} случайной величины $\xi$:
    \begin{equation*}
        \gamma_1=\mathbb{E}\left(\frac{\xi - \mathbb{E}\xi}{\sqrt{\mathbb{D}\xi}}\right)^3
    \end{equation*}
    Характеризует <<скошенность>> графика плотности распределения: \medskip\hfill\break
    \begin{center}
    \begin{tikzpicture}
    \begin{axis}[skewkurt, ymax=0.5]
        \draw [dashed,black!50] (3,0) -- (3,0.4);
        \addplot[very thick, color1, samples=100,domain=-1:7] {sknorm(x,3,1.9,-8)};
        \addlegendentry{$\gamma_1<0$}
        \addplot[very thick, color2, samples=100,domain=-1:7] {sknorm(x,3,1.9,8)};
        \addlegendentry{$\gamma_1>0$}
        \addplot[very thick, color3, samples=100,domain=-1:7] {sknorm(x,3,1,0)};
        \addlegendentry{$\gamma_1=0$}
    \end{axis}
    \end{tikzpicture}
    \end{center}
\end{defn}

\begin{defn}
    {\it Коэффициент эксцесса} случайной величины $\xi$:
    \begin{equation*}
        \gamma_2=\mathbb{E}\left(\frac{\xi - \mathbb{E}\xi}{\sqrt{\mathbb{D}\xi}}\right)^4 - 3
    \end{equation*}
    Характеризует <<островершинность>> графика плотности распределения:
    \medskip\hfill\break
    \begin{center}
    \begin{tikzpicture}
    \begin{axis}[skewkurt]
        \draw [dashed,black!50] (3,0) -- (3,0.625);
        \addplot[very thick, color1, samples=100,domain=-1:7] {normal(x,2.2,3)};
        \addlegendentry{$\gamma_2<0$}
        \addplot[very thick, color2, samples=100,domain=-1:7] {normal(x,0.4,3)};
        \addlegendentry{$\gamma_2>0$}
        \addplot[very thick, color3, samples=100,domain=-1:7] {normal(x,1,3)};
        \addlegendentry{$\gamma_2=0$}
    \end{axis}
    \end{tikzpicture}
    \end{center}
\end{defn}

\begin{rmrk}
    Слагаемое $-3$ добавлено, чтобы коэффициент эксцесса стандартного нормального распределения был равен нулю. Иногда его не учитывают и считают, что коэффициент эксцесса $\mathcal{N}(0,1)$ равен 3.
\end{rmrk}

\iffalse
\medskip\hfill\break
    \begin{center}
    \begin{tikzpicture}[declare function={sigma(\x)=1/(1+exp(-\x));}]
\begin{axis}
[
    grid=major,     
    xmin=-6,
    xmax=6,
    axis x line=bottom,
    ytick={0,.5,1},
    ymax=1,
    axis y line=middle,
    samples=100,
    domain=-6:6,
    legend style={at={(1,0.9)}}     
]
    \draw [dashed,black!50] (1,0.365) -- (1,0.865);
    \addplot[very thick,black,mark=none, samples=100,domain=-6:1]   (x, {.5 * sigma(x)});
    \addplot[very thick,black,mark=none, samples=100,domain=1:6]   (x, {.5 + .5 * sigma(x)});
\end{axis}
\end{tikzpicture}
    \end{center}
\fi

\section {Числовые характеристики случайных величин: квантили. Медиана и ее свойства. Интерквартильный размах}

\begin{defn}
    {\it Медианой} $\operatorname{Med} \xi$ распределения случайной величины $\xi$ называется любое из {\it чисел} $\mu$ таких, что
    \begin{equation*}
        \myprob{\xi \leqslant \mu} \geqslant \frac{1}{2}, ~~~ \myprob{\xi \geqslant \mu} \geqslant \frac{1}{2}.
    \end{equation*}
\end{defn} 

\begin{rmrk}
    Медиана распределения всегда существует, но может быть не единственна.
\end{rmrk} 

\begin{defn}
{ \itКвантиль порядка $\gamma$} ~--- это такое число $\kappa_\gamma$, для которого выполняется 
$$ \begin{cases} \myprob{\xi \leqslant \kappa_\gamma} = F(\kappa_\gamma) \geqslant \gamma, \\
\myprob{\xi \geqslant \kappa_\gamma} = 1 - F(\kappa_\gamma) \geqslant 1 - \gamma \end{cases}
$$
\end{defn}

\begin{rmrk}
    Если функция распределения $F$ непрерывна и строго монотонна, то {\it квантилем} уровня (порядка) $\gamma$, где $\gamma \in (0; 1), $ является решение $x_\gamma$ уравнения $F(x_\gamma) = \gamma.$ 
    Тогда квантиль порядка $\gamma$ отрезает от области под графиком плотности область с площадью с площадью $\gamma$ слева от себя. Справа от $\kappa_\gamma$ площадь области равна $1 - \gamma$.
    
    Если же случайная величина не является абсолютно непрерывной, то уравнение $F(x_\gamma) = \gamma$ может не иметь решений. Например, для приведенного ниже графика не существует $x_{\frac{1}{2}}: F(x_{\frac{1}{2}}) = \frac{1}{2}$.
        
    \medskip\hfill\break
    \begin{center}
    \begin{tikzpicture}[declare function={sigma(\x)=1/(1+exp(-\x));}]
    \begin{axis}
    [
        grid=major,     
        xmin=-6,
        xmax=6,
        axis x line=bottom,
        ytick={0,.5,1},
        ymax=1,
        axis y line=middle,
        samples=100,
        domain=-6:6,
        legend style={at={(1,0.9)}}     
    ]
        \draw [dashed,black!50] (1,0.365) -- (1,0.865);
        \addplot[very thick,black,mark=none, samples=100,domain=-6:1]   (x, {.5 * sigma(x)});
        \addplot[very thick,black,mark=none, samples=100,domain=1:6]   (x, {.5 + .5 * sigma(x)});
    \end{axis}
    \end{tikzpicture}
    \end{center}
\end{rmrk} 

\begin{defn}
    Квантили уровней, кратных $0.01$, называют {\it процентилями}, квантили уровней, кратных $0.1$, — {\it децилями}, уровней, кратных $0.25$, — {\it квартилями}.
\end{defn} 

\begin{rmrk}
    Медиана является квантилем уровня $\frac{1}{2}.$
\end{rmrk} 

\begin{namedthm}[Свойства медианы]\leavevmode
\begin{enumerate}
    \item Медиана случайной величины $\xi$ минимизирует средний модуль её отклонения:
    \begin{equation*}
        \mathbb{E}|\xi - \operatorname{Med} \xi| 
    = \min _{a} \mathbb{E}|\xi-a|;
    \end{equation*}
    \item Отклонение медианы случайной величины $\xi$ от её математического ожидания $\mathbb{E}\xi$ не превышает по модулю среднеквадратичного отклонения $\sigma = \sqrt{\mathbb{D}\xi}$:
    \begin{equation*}
        |\mathbb{E}\xi - \operatorname{Med}\xi| \leqslant \sigma.
    \end{equation*}
\end{enumerate}
\end{namedthm}

\begin{proof}
    \begin{enumerate}
        \item Рассмотрим случайную величину $\eta = \xi - \operatorname{Med}\xi$. Очевидно, что $\operatorname{Med} \eta = 0$. Тогда нам надо показать, что $\forall c \in \mathbb{R} $ справедливо 
        \begin{equation*}
            \mathbb{E} |\eta - c| - \mathbb{E}|\eta| \geqslant 0.
        \end{equation*}
        
        Рассмотрим случай $c > 0$. Заметим, что 
        \begin{gather*}
            |\eta - c| - |\eta| = c, \quad \eta < 0 \\
            |\eta - c| - |\eta|  \geqslant -c, \quad \eta \geqslant 0.
        \end{gather*}
        Тогда
        \begin{gather*}
            \mathbb{E} \left[~|\eta - c| - |\eta|~\right] = \mathbb{E} \left[~(|\eta - c| - |\eta|)~\mathbb{I}(\eta < 0) + (|\eta - c| - |\eta|)~\mathbb{I}(\eta \geqslant 0)~\right] \\
            \mathbb{E} \left[~|\eta - c| - |\eta|~\right] \geqslant c~\mathbb{P}(\eta < 0) - c~\mathbb{P}(\eta \geqslant 0).
        \end{gather*}
        Так как $\operatorname{Med}\eta = 0$, то $\mathbb{P}(\eta \leqslant 0) = \mathbb{P}(\eta \geqslant 0) = \frac{1}{2}$.
        Отсюда вытекает
        \begin{equation*}
            \mathbb{E}\left[~|\eta - c| - |\eta|~\right] \geqslant 0.
        \end{equation*}
        Случай $c < 0$ сводится к предыдущему умножением случайной величины и $c$ на $-1$. Отсюда следует, что медиана действительно минимизирует средний модуль отклонения.
        \item Рассмотрим цепочку неравенств:
        \begin{multline*}
            |\mathbb{E}\xi - \operatorname{Med}\xi| =
            |\mathbb{E}\left[ \operatorname{Med}\xi - \xi \right]| \leqslant 
            {\text{\{Седьмое свойство мат. ожидания\}}} \\ \leqslant \mathbb{E} |\operatorname{Med}\xi - \xi|
            \leqslant {\text{\{Первое свойство медианы\}}} \\ \leqslant
            \mathbb{E}| \mathbb{E}\xi - \xi| \leqslant
            {\text{\{неравенство Йенсена\}}} \\ 
            \leqslant
            \sqrt{\mathbb{E}| \mathbb{E}\xi - \xi|^2} = 
            \sqrt{\mathbb{D}\xi} = \sigma.
        \end{multline*}
    \end{enumerate}
\end{proof}

\subsubsection{Интерквантильный размах}

\begin{defn}
    {\it Интерквартильным размахом} называется разность между третьим и первым квартилями, то есть ${\displaystyle x_{0{,}75}-x_{0{,}25}}.$
\end{defn} 

В каком-то смысле эту величину можно считать аналогом дисперсии случайной величины, устойчивой к выбросам.

\section {Испытания Бернулли. Биномиальное распределение. Теорема Пуассона. Распределение Пуассона}
\begin{defn}
    {\it Схема Бернулли}~--- последовательность независимых испытаний, в каждом из которых возможны лишь два исхода~--- <<успех>> и <<неудача>>, при этом успех в каждом испытании происходит с одной и той же вероятностью $p \in (0;1)$, а неудача~--- с вероятностью $q = 1 - p.$
\end{defn}

\begin{namedthm}[Формула Бернулли]
Пусть $\xi$~--- случайная величина, равная числу успехов в $n$ испытаниях. Тогда $\forall k = \overline{1,n}$ вероятность получить в $n$ испытаниях ровно $k$ успехов равна
\begin{equation*}
    \mathbb{P}\left(\xi=k\right)=C_{n}^{k} p^{k} q^{n-k}
\end{equation*}
\end{namedthm}

\begin{proof}
Рассмотрим один элементарный исход события $A = \{\xi = k \}$:
\begin{equation*}
    (\underbrace{y, y, \ldots, y}_{k}, \underbrace{\text{\it н}, \text{\it н}, \ldots,\text{\it н}}_{n-k})
\end{equation*}
когда первые $k$ испытаний завершились успехом (у), остальные неудачей (н). Поскольку испытания независимы, вероятность такого элементарного исхода равна $p^k(1 - p)^{n-k}.$ Другие элементарные исходы из события $A$ отличаются лишь расположением $k$ успехов на $n$ местах. Поэтому событие $A$ состоит из $C_n^k$ элементарых исходов, вероятность каждого из которых равна $p^kq^{n-k}$.
\end{proof}

\begin{defn}
    Пусть $\xi_1, \ldots, \xi_n$~--- последовательность независимых случайных величин, имеющих одинаковое распределение Бернулли с параметром $p$ ($\mathbf{Bi}(p)$), то есть принимает значение $1$ (<<успех>>) с вероятностью $p$ и $0$ (<<неудача>>) с вероятностью $1 - p = q$. Тогда говорят, что случайная величина $\xi = \xi_1 + \ldots + \xi_n$ имеет {\it биномиальное распределение} с параметрами $n$ и $p$ ($\mathbf{B}(n, p)$).
\end{defn}

\subsubsection{Числовые характеристики $\mathbf{Bi}(p)$}
\begin{enumerate}
    \item Математическое ожидание:
    \begin{equation*}
        \mathbb{E}\xi =  1 \cdot p + 0 \cdot q = p
    \end{equation*}
    \item Дисперсия:
        $$\mathbb{E}\xi^2 = 1^2 \cdot p + 0^2 \cdot q = p; \quad \mathbb{D}\xi = \mathbb{E}\xi^2 - (\mathbb{E}\xi)^2 = p - p^2 = p \cdot (1 - p) = pq$$
\end{enumerate}

\subsubsection{Числовые характеристики $\mathbf{B}(n, p)$}
\begin{enumerate}
    \item Математическое ожидание:
    \begin{equation*}
        \mathbb{E}\xi = \mathbb{E}(\xi_1 + \ldots + \xi_n) = \mathbb{E}\xi_1 + \ldots + \mathbb{E}\xi_n = \underbrace{p + \ldots + p}_{n} = np
    \end{equation*}
    \item Дисперсия:
    \begin{equation*}
        \mathbb{D}\xi = \mathbb{D}(\xi_1 + \ldots + \xi_n) = \mathbb{D}\xi_1 + \ldots + \mathbb{D}\xi_n = \underbrace{p \cdot q + \ldots + p \cdot q}_{n} = npq
    \end{equation*}
\end{enumerate}

\begin{namedthm}[Теорема Пуассона]
    Пусть проводится $n$ обобщённых испытаний Бернулли (т.е. вероятность успеха испытания зависит от $n$) с вероятностью успеха $p_n$, $\xi$~--- количество успехов в этих испытаниях и $n p_{n} \underset{n \to +\infty}{\longrightarrow} \lambda$. Тогда
    \begin{equation*}
        \forall k \in \mathbb{Z},~ 0 \leqslant k \leqslant n: \quad \mathbb{P}\left(\xi=k\right) \underset{n \to +\infty}{\longrightarrow} \frac{\lambda^{k}}{k !} e^{-\lambda}
    \end{equation*}
\end{namedthm}

\begin{proof}
    По условию теоремы, $n p_n \xrightarrow[n \to +\infty]{} \lambda$. Тогда $p_n \xrightarrow[n \to +\infty]{} 0$. Рассмотрим формулу Бернулли:
    \begin{multline*}
        \mathbb{P}(\xi = k) = C_n^k p^k (1 - p)^{n-k} = \frac{n!}{k!(n - k)!} \cdot \frac{\lambda^k}{n^k} \cdot \left(1 - \frac{\lambda}{n}\right)^{-k} \cdot \left(1 - \frac{\lambda}{n}\right)^n = \\
        = \frac{\lambda^k}{k!} \cdot \frac{(n - k + 1) \cdot \ldots \cdot (n - 1) \cdot n}{n^k} \cdot \left(1 - \frac{\lambda}{n}\right)^n \cdot \left(1 - \frac{\lambda}{n}\right)^{-k}
    \end{multline*}
    Перейдём к пределу при $n \to +\infty$:
    \begin{equation*}
        \frac{(n - k + 1) \cdot \ldots \cdot (n - 1) \cdot n}{n^k} \to 1,~ \left(1 - \frac{\lambda}{n}\right)^n \to e^{-\lambda},~ \left(1 - \frac{\lambda}{n}\right)^{-k} \to 1
    \end{equation*}
    
    Таким образом, получим
    \begin{equation*}
        \mathbb{P}(\xi = k) \xrightarrow[n \to +\infty]{} \frac{\lambda^k}{k!} e^{-\lambda}
    \end{equation*}
\end{proof}

\begin{defn}
    Набор вероятностей $\{\frac{\lambda^k}{k!} e^{-\lambda} \}$, где $k$ принимает значения $0, 1, 2, \ldots$, называется {\it распределением Пуассона} с параметром $\lambda > 0$ ($\mathbf{Pois}(\lambda)$).
\end{defn}
\begin{rmrk}
    Распределение Пуассона представляет собой число событий, произошедших за фиксированное время, при условии, что данные события происходят с некоторой фиксированной средней интенсивностью (за которую отвечает параметр $\lambda$) и независимо друг от друга.
\end{rmrk}

\subsubsection{Числовые характеристики $\mathbf{Pois}(\lambda)$}
\begin{enumerate}
    \item Математическое ожидание:
    \begin{align*}
        \sum\limits_{k=0}^{\infty} e^{-\lambda} \frac{\lambda^k}{k!} = e^{-\lambda} \sum\limits_{k=0}^{\infty} \frac{\lambda^k}{k!} = e^{-\lambda} e^\lambda = 1 \\
        \mathbb{E}\xi = \sum\limits_{k=1}^{\infty} k e^{-\lambda} \frac{\lambda^k}{k!} = e^{-\lambda} \lambda \underbrace{\sum\limits_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k - 1)!}}_{= e^\lambda} = \lambda
    \end{align*}
    \item Дисперсия:
    \begin{align*}
        \mathbb{E}\xi(\xi - 1) = \sum\limits_{k=0}^{\infty} k (k - 1) \frac{\lambda^k}{k!} e^{-\lambda}  = \lambda^2 e^{-\lambda} \sum\limits_{k=0}^{\infty} \frac{\lambda^{k-2}}{(k-2)!} = \lambda^2 e^{-\lambda} e^\lambda = \lambda^2 \\
        \mathbb{E}\xi^2 = \mathbb{E}\xi(\xi - 1) + \mathbb{E}\xi = \lambda^2 + \lambda \quad \Rightarrow \quad \mathbb{D}\xi = \mathbb{E}\xi^2 - (\mathbb{E}\xi)^2 = \lambda
    \end{align*}
\end{enumerate}

\section{Испытания Бернулли. Геометрическое распределение. Теорема Реньи. Показательное распределение}
Рассмотрим схему экспериментов Бернулли с вероятностью успеха $p$, неудачи~--- $q = 1 - p$. Вероятность того, что первый успех произойдёт в испытании с номером $k \in \mathbb{N}$, очевидно, равна $\myprob{\tau = k} = pq^{k-1}$.

\begin{defn}
    Набор вероятностей $\{p q^{k-1}\}$, где $k$ принимает любые значения из множества натуральных чисел, называется {\it геометрическим распределением} вероятностей ($\mathbf{Geom}(p)$).
\end{defn}

Аналогично можно ввести геометрическое распределение как <<число неудач до первого успеха>>. Тогда $k$ будем принимать значения из множества $\{0, 1, 2, \ldots\}$.

\subsubsection{Числовые характеристики $\mathbf{Geom}(p)$}
\begin{enumerate}
    \item Математическое ожидание:
    \begin{multline*}
        \mathbb{E} \xi=\sum\limits_{k=1}^{\infty} k p q^{k-1}=p \sum\limits_{k=1}^{\infty} k q^{k-1}=p \sum\limits_{k=1}^{\infty} \frac{d q^{k}}{d q} = \\
        = p \frac{d}{d q}\left(\sum\limits_{k=1}^{\infty} q^{k}\right)=p \frac{d}{d q}\left(\frac{q}{1-q}\right)=p \frac{1}{(1-q)^{2}}=\frac{1}{p}
    \end{multline*}
    \item Дисперсия:
    \begin{multline*}
        \mathbb{E} \xi(\xi-1)=\sum\limits_{k=1}^{\infty} k(k-1) p q^{k-1}=p q \sum\limits_{k=0}^{\infty} \frac{d^{2} q^{k}}{d q^{2}} =p q \frac{d^{2}}{d q^{2}}\left(\sum\limits_{k=0}^{\infty} q^{k}\right) = \\
        =p q \frac{d^{2}}{d q^{2}}\left(\frac{1}{1-q}\right)=p q \frac{2}{(1-q)^{3}}=\frac{2 q}{p^{2}} \\
        \mathbb{D} \xi=\mathbb{E} \xi(\xi-1)+\mathbb{E} \xi-(\mathbb{E} \xi)^{2}=\frac{2 q}{p^{2}}+\frac{1}{p}-\frac{1}{p^{2}}=\frac{2 q-1+p}{p^{2}}=\frac{q}{p^{2}}
    \end{multline*}
\end{enumerate}

\begin{rmrk}
    Если определять геометрическое распределение как количество неудач до первого успеха, его математическое ожидание изменится:
    \begin{multline*}
        \mathbb{E} \xi=\sum\limits^{\infty}_{\color{red}k=0} k p q^{\color{red}k}= {\color{red}q}p \sum\limits_{k=1}^{\infty} k q^{k-1}= {\color{red}q}p \sum\limits_{k=1}^{\infty} \frac{d q^{k}}{d q} = \\
        = {\color{red}q}p \frac{d}{d q}\left(\sum\limits_{k=1}^{\infty} q^{k}\right)={\color{red}q} p \frac{d}{d q}\left(\frac{q}{1-q}\right)= {\color{red}q}p \frac{1}{(1-q)^{2}}=\frac{{\color{red}q}}{p}
    \end{multline*}
    Так как $q \in (0,1)$, математическое ожидание станет меньше, и это логично ~--- ведь количество неудач до первого успеха всегда на единицу меньше номера первого успеха. (Используя это наблюдение, можно посчитать мат. ожидание ещё проще ~--- $\mathbb{E}(\xi - 1) = \frac{1}{p} - 1 = \frac{1-p}{p} = \frac{q}{p}$). Дисперсия же не зависит от сдвига и останется прежней.
\end{rmrk}

\begin{defn}
    Случайная величина $\xi$ имеет {\it показательное (экспоненциальное) распределение} с параметром $\lambda > 0$ ($\mathbf{Exp}(\lambda)$), если $\xi$ имеет следующие плотность и функцию распределения:
    \begin{equation*}
        f(x) = 
        \begin{cases}
            0, & \text{если $x < 0$;} \\
            \lambda e^{-\lambda x}, & \text{если $x \geqslant 0$.}
        \end{cases}
        \quad 
        F(x) = 
        \begin{cases}
            0, & \text{если $x < 0$;} \\
            1 - e^{-\lambda x}, & \text{если $x \geqslant 0$.}
        \end{cases}
    \end{equation*}
\end{defn}

\begin{rmrk}
    Показательное распределение моделирует время между двумя последовательными свершениями одного и того же события. К примеру, пусть есть магазин, в который время от времени заходят покупатели. При определённых допущениях время между появлениями двух последовательных покупателей будет случайной величиной с экспоненциальным распределением. Среднее время ожидания нового покупателя равно $\frac{1}{\lambda}$. Сам параметр $\lambda$ тогда может быть интерпретирован как среднее число новых покупателей за единицу времени. 
\end{rmrk}

\subsubsection{Числовые характеристики $\mathbf{Exp}(\lambda)$}

Найдём для произвольного $k \in \mathbb{N}$ момент порядка $k$:
\begin{equation*}
    \mathbb{E} \xi^{k}=\int\limits_{-\infty}^{\infty} x^{k} f_{\xi}(x) d x=\int\limits_{0}^{\infty} x^{k} \lambda e^{-\lambda x} d x=\frac{1}{\lambda^{k}} \int\limits_{0}^{\infty}(\lambda x)^{k} e^{-\lambda x} d(\lambda x)=\frac{k !}{\lambda^{k}}
\end{equation*}

В последнем равенстве была использована формула для гамма-функции:
\begin{equation*}
    \Gamma(k+1)=\int\limits_{0}^{\infty} u^{k} e^{-u} d u=k !
\end{equation*}
\begin{enumerate}
    \item Математическое ожидание:
    \begin{equation*}
        \mathbb{E} \xi=\frac{1}{\lambda}
    \end{equation*}
    \item Дисперсия:
    \begin{equation*}
        \mathbb{E} \xi^{2}=\frac{2}{\lambda^{2}}, \quad \mathbb{D} \xi=\mathbb{E} \xi^{2}-(\mathbb{E} \xi)^{2}=\frac{1}{\lambda^{2}}
    \end{equation*}
\end{enumerate}

\begin{thm*}
    Пусть проводится $n$ обобщённых испытаний Бернулли (т.е. вероятность успеха испытания зависит от $n$) с вероятностью успеха $p_n$, $\xi \sim \mathbf{Geom}(p_n)$, $n p_{n} \underset{n \to +\infty}{\longrightarrow} \lambda > 0$. Тогда распределение случайной величины $\frac{\xi}{n}$ сходится к показательному с параметром $\lambda$ при $n \to +\infty$.
\end{thm*}

\begin{proof}
    Пусть $F_n$~--- функция распределения случайной величины $\frac{\xi}{n}$. Тогда для $x \geqslant 0$:
    \begin{equation*}
        F_{n}(x)=\mathbb{P}\left(\frac{\xi_{n}}{n} \leqslant x\right)=\mathbb{P}\left(\xi_{n} \leqslant n x\right)=\mathbb{P}\left(\xi_{n} \leq\lfloor n x\rfloor\right)=1-\left(1-p_{n}\right)^{\lfloor n x\rfloor}
    \end{equation*}
    
    Далее, т.к. $\left(1-p_{n}\right)^{n}=\left(1-\frac{n p_{n}}{n}\right)^{n} \xrightarrow[n \to +\infty]{} e^{-\lambda}$, то $(1 - p_n)^{nx} \xrightarrow[n \to +\infty]{} e^{-\lambda x}.$ По определению, $\lfloor n x\rfloor \leqslant n x<\lfloor n x\rfloor+1$ или, что эквивалентно, $n x-1<\lfloor n x\rfloor \leqslant n x.$ и таким образом верно $(1 - p_n)^{\lfloor nx \rfloor} \xrightarrow[n \to +\infty]{} e^{-\lambda x}$. Следовательно, $F_n(x) \xrightarrow[n \to +\infty]{} 1 - e^{-\lambda x}$, что есть функция показательного распределения. 
\end{proof}

\begin{namedthm}[Теорема Реньи]
Пусть даны случайная величина $N \sim \mathbf{Geom}(p)$, $\xi_1, \xi_2, \ldots$~--- независимые одинаково распределённые случайные величины, $\xi_i \geqslant 0$ и $0 < a = \mathbb{E}\xi < \infty$, $S_n = \sum\limits_{i=1}^N \xi_i$. Тогда
\begin{equation*}
    \sup\limits_{x}\left|\mathbb{P}\left(\frac{p}{a} S_{N}<x\right)-G(x)\right| \underset{p \rightarrow 0}{\longrightarrow} 0,
\end{equation*}

где $G(x)=\left(1-e^{-x}\right) I_{x \geqslant 0}$~--- функция стандартного показательного распределения $\mathbf{Exp}(1)$.

Если $b^2 = \mathbb{E}X_i^2$, тогда
\begin{equation*}
    \sup\limits_{x}\left|\mathbb{P}\left(\frac{p}{a} S_{N}<x\right)-G(x)\right| \leqslant \frac{p b^{2}}{(1-p) a^{2}}
\end{equation*}
\end{namedthm}

\section{Испытания Бернулли. Теорема Муавра—Лапласа. Нормальное распределение}

\begin{namedthm} [Теорема. (Локальная предельная теорема Муавра-Лапласа.)]
    Пусть $S_n$ - число успехов в $n$ испытаниях Бернулли с вероятностью успеха $p$. Если $n p(1-p) \underset{n \to +\infty}{\longrightarrow} \infty$, то
$$\forall m \in \mathbb{Z}: 0 \leqslant m \leqslant n \quad \mathbb{P}\left(S_{n}=m\right)=\frac{1}{\sigma \sqrt{2 \pi} } e^{-\frac{x^{2}}{2}}\left(1+\underline{O}\left(\frac{1}{\sigma}\right)\right),$$
где $x = \frac{m - np}{\sigma},$ а $\sigma=\sqrt{\mathbb{D} S_{n}}=\sqrt{n p(1-p)}$.
\end{namedthm}  

\begin{namedthm}[Интегральная теорема Муавра-Лапласа]
Если выполнено условие локальной теоремы и $C$ - произвольная положительная константа, то равномерно по $a$ и $b$ из отрезка $[-C,C]$ (пусть $b \geqslant a$)
$$\mathbb{P}\left(a \leqslant \frac{S_{n}-n p}{\sqrt{n p(1-p)}} \leqslant b\right) \underset{n \to +\infty}{\longrightarrow} \frac{1}{\sqrt{2 \pi}} \int\limits_{a}^{b} e^{-\frac{x^{2}}{2}} d x.$$
\end{namedthm} 

\begin{defn}
    Случайная величина $\xi$ имеет {\it нормальное (гауссовское) распределение} с параметрами $a$ и $\sigma^2$, где $a \in \mathbb{R}, \sigma > 0$, если $\xi$ имеет следующую плотность распределения: 
$$f(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-a)^{2}}{2 \sigma^{2}}}, \quad x \in \mathbb{R}.$$
\end{defn}

\subsubsection{Матожидание и дисперсия нормального распределения}

Найдем матожидание и дисперсию для {\it стандартного} нормального распределения, т.е. для нормального распределения с параметрами $\alpha = 0$ и $\sigma^2 = 1$:
%$$\mathbb{E} \xi=\int\limits_{-\infty}^{\infty} x f_{\xi}(x) %d x=\frac{1}{\sqrt{2 \pi}} \int\limits_{-\infty}^{\infty} x %e^{-x^{2} / 2} d x=0,$$
\begin{equation*}
    \mathbb{E}\xi = 
    \int\limits_{-\infty}^{\infty} x f_{\xi}(x) dx =
    \frac{1}{\sqrt{2\pi}} \int\limits_{-\infty}^{\infty} x e^{\frac{-x^2}{2}} dx = 
    -\frac{1}{\sqrt{2\pi}} \int\limits_{-\infty}^{\infty} d\left( e^{\frac{-x^2}{2}}\right) = 
    \left. -e^{\frac{-x^2}{2}}\right|_{-\infty}^{\infty} = 0
\end{equation*}

%так как под интегралом стоит нечётная функция. Далее,
Далее, 
\begin{multline*}
    \mathbb{E} \xi^{2}=\frac{1}{\sqrt{2 \pi}} \int\limits_{-\infty}^{\infty} x^{2} e^{-x^{2} / 2} d x=\frac{2}{\sqrt{2 \pi}} \int\limits_{0}^{\infty} x^{2} e^{-x^{2} / 2} d x=-\frac{2}{\sqrt{2 \pi}} \int\limits_{0}^{\infty} x d e^{-x^{2} / 2}= \\
    =-\left.\frac{2 x}{\sqrt{2 \pi}} e^{-x^{2} / 2}\right|_{0} ^{\infty}+2 \int\limits_{0}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{-x^{2} / 2} d x=0+\int\limits_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{-x^{2} / 2} d x=1.
\end{multline*}

Поэтому $\mathbb{D}\xi = \mathbb{E}\xi^2 - (\mathbb{E}\xi)^2 = 1 - 0 = 1.$

Теперь рассмотрим случайную величину $\eta$ с нормальным распределением в общем виде (с параметрами $\alpha$ и $\sigma^2$). Тогда $\xi = \frac{\eta - \alpha}{\sigma}$ - случайная величина со {\it стандартным} нормальным распределением. Далее, т.к. $\mathbb{E}\xi = 0$, $\mathbb{D}\xi = 1$, то 
\begin{gather*}
    \mathbb{E}\eta = \mathbb{E}(\sigma \xi + \alpha) = \sigma \mathbb{E} \xi + \alpha = \alpha, \\
    \mathbb{D}\eta = \mathbb{D}(\sigma \xi + \alpha) = \sigma^2 \mathbb{D}\xi = \sigma^2.
\end{gather*}

\section{Совокупности случайных величин. Совместная функция распределения. Независимость случайных величин. Критерии независимости. Ковариация, коэффициент корреляции}

\subsubsection{Совместное распределение, его свойства}

Пусть случайные величины $\xi_1, \ldots, \xi_n$ заданы на одном вероятностном пространстве $(\Omega, \mathcal{F}, \mathbb{P})$.
\begin{defn}
    {\it Совместное распределение} случайных величин $(\xi_1, \ldots, \xi_n)$~--- функция $\mathbb{P}: \mathfrak{B}(\mathbb{R}^{n}) \to \mathbb{R}^{n}$:
    \begin{equation*}
        \mathbb{P}(\xi \in B) = \mathbb{P}(\omega \colon (\xi_{1}(\omega), \ldots, \xi_{n}(\omega)) \in B),~ B \subset \mathfrak{B}(\mathbb{R}^{n})
    \end{equation*}
\end{defn}
\begin{defn}
    {\it Функция совместного распределения} случайных величин $(\xi_1, \ldots, \xi_n)$~--- функция $F: \mathbb{R}^{n} \to \mathbb{R}^{n}$:
    \begin{equation*}
        F(x_{1}, \ldots, x_{n})=\mathbb{P}(\xi_{1}<x_{1}, \ldots, \xi_{n}<x_{n})
    \end{equation*}
\end{defn}

\begin{rmrk}
    Для функции совместного распределения выполняются свойства, аналогичные одномерному случаю. При этом частные функции распределения восстанавливаются по совместной следующим образом:
    \begin{equation*}
        \lim_{\substack{x_{k} \to +\infty \\ k \neq i}}  F(x_{1}, \ldots, x_{i}, \ldots, x_{n}) = F_{i}(x), \quad i = \overline{1,n}
    \end{equation*}
\end{rmrk}
Далее рассматриваем совместные распределения двух случайных величин.

\subsubsection{Виды многомерных распределений}

\begin{defn}
    Случайные величины $\xi_1, \xi_2$ имеют дискретное совместное распределение, если существует не более чем счётный набор пар неотрицательных чисел $\{a_{i}, b_{j}\}$ такой, что
    \begin{equation*}
        \sum\limits_{i=1}^{\infty} \sum\limits_{j=1}^{\infty} \mathbb{P}\left(\xi_{1}=a_{i}, \xi_{2}=b_{j}\right)=1
    \end{equation*}
    Таблицу, на пересечении $i$-й строки и $j$-го столбца которой стоит вероятность $\mathbb{P}\left(\xi_{1}=a_{i}, \xi_{2}=b_{j}\right)$, называют {\it таблицей совместного распределения} случайных величин $\xi_1$ и $\xi_2$.
\end{defn}
\begin{defn}
    Случайные величины $\xi_1, \xi_2$ имеют абсолютно непрерывное совместное распределение, если существует неотрицательная функция $f_{\xi_{1}, \xi_{2}}(x, y)$ такая, что для любого борелевского множества $B \in \mathfrak{B}\left(\mathbb{R}^{2}\right)$ имеет место равенство
    \begin{equation*}
        \mathbb{P}\left(\left(\xi_{1}, \xi_{2}\right) \in B\right)=\iint\limits_{B} f_{\xi_{1}, \xi_{2}}(x, y) d x d y
    \end{equation*}
    Если такая функция $f_{\xi_{1}, \xi_{2}}(x, y)$ существует, она называется {\it плотностью совместного распределения} случайных величин $\xi_1, \xi_2$.
    
    Функция совместного распределения в этом случае имеет вид:
    \begin{equation*}
        F(x, y)=\mathbb{P}(\xi_{1}<x, \xi_{2}<y)=\int\limits_{-\infty}^{x}\left(\int\limits_{-\infty}^{y} f_{\xi_{1}, \xi_{2}}(u, v) d v\right) d u
    \end{equation*}
\end{defn}

\begin{rmrk}
    Плотность совместного распределения имеет те же свойства, что и плотность распределения одной случайной величины: неотрицательность и нормированность:
    \begin{equation*}
        f(x, y) \geqslant 0~ \forall x,y \in \mathbb{R}; \quad \iint\limits_{\mathbb{R}^{2}} f(x, y) dx dy = 1
    \end{equation*}

    По функции совместного распределения его плотность находится как смешанная частная производная (в точках, где она существует):
    \begin{equation*}
        f(x, y)=\frac{\partial^{2}}{\partial x \partial y} F(x, y)
    \end{equation*}
\end{rmrk}

\begin{rmrk}
    Из существования плотностей $\xi_1$ и $\xi_2$ не следует абсолютная непрерывность совместного распределения этих случайных величин. Например, вектор $(\xi, \xi)$ принимает значения только на диагонали в $\mathbb{R}^2$ и уже поэтому не имеет плотности распределения (его распределение сингулярно). Обратное же свойство, как показывает следующая теорема, всегда верно.
\end{rmrk}

\begin{thm*}
    Если случайные величины $\xi_1$ и $\xi_2$ имеют абсолютно непрерывное совместное распределение с плотностью $f(x, y)$, то $\xi_1$ и $\xi_2$ в отдельности также имеют абсолютно непрерывное распределение с плотностями:
    \begin{equation*}
        f_{\xi_{1}}(x)=\int\limits_{-\infty}^{\infty} f(x, y) d y ; \quad f_{\xi_{2}}(y)=\int\limits_{-\infty}^{\infty} f(x, y) d x
    \end{equation*}
    Для $n > 2$ плотности случайных величин $\xi_1, \ldots, \xi_n$ находятся по плотностиих совместного распределения $f(x_1, \ldots, x_n)$ находятся интегрированием функции $f$ по всем <<лишним>> координатам.
\end{thm*}
\begin{proof}
\begin{equation*}
    F_{\xi_{1}}\left(x_{1}\right)
    = \lim _{x_{2} \rightarrow+\infty} F_{\xi_{1}, \xi_{2}}\left(x_{1}, x_{2}\right)
    = \int\limits_{-\infty}^{x_{1}}\left(\int\limits_{-\infty}^{\infty} f(x, y) d y\right) d x
    = \int\limits_{-\infty}^{x_{1}} f_{\xi_{1}}(x) d x
\end{equation*}
\end{proof}

\subsubsection{Независимость случайных величин}
\begin{defn}
    Случайные величины $\xi_1, \ldots, \xi_n$ называют {\it независимыми в совокупности}, если для любого набора борелевских множеств $B_{1}, \ldots, B_{n} \in \mathfrak{B}(\mathbb{R})$:
    \begin{equation*}
        \mathbb{P}\left(\xi_{1} \in B_{1}, \ldots, \xi_{n} \in B_{n}\right)=\mathbb{P}\left(\xi_{1} \in B_{1}\right) \cdot \ldots \cdot \mathbb{P}\left(\xi_{n} \in B_{n}\right)
    \end{equation*}
\end{defn}
\begin{namedthm}[Критерий независимости]
    Случайные величины $\xi_1, \ldots, \xi_n$ независимы в совокупности $\Leftrightarrow$ имеет место равенство:
    \begin{equation*}
        F_{\xi_{1}, \ldots, \xi_{n}}\left(x_{1}, \ldots, x_{n}\right)=F_{\xi_{1}}\left(x_{1}\right) \cdot \ldots \cdot F_{\xi_{n}}\left(x_{n}\right)
    \end{equation*}
    В частности, в случае дискретного совместного распределения:
    \begin{equation*}
        \mathbb{P}\left(\xi_{1}=a_{1}, \ldots, \xi_{n}=a_{n}\right)=\mathbb{P}\left(\xi_{1}=a_{1}\right) \cdot \ldots \cdot \mathbb{P}\left(\xi_{n}=a_{n}\right) \quad \forall a_1, \ldots, a_n \in \mathbb{R}
    \end{equation*}
    В случае абсолютно непрерывного:
    \begin{equation*}
        f_{\xi_{1}, \ldots, \xi_{n}}\left(x_{1}, \ldots, x_{n}\right)=f_{\xi_{1}}\left(x_{1}\right) \cdot \ldots \cdot f_{\xi_{n}}\left(x_{n}\right)
    \end{equation*}
\end{namedthm}

\subsubsection{Формула свёртки}

Пусть $\xi_1, \xi_2$~--- случайные величины с плотностью совместного распределения $f_{\xi_{1}, \xi_{2}}\left(x_{1}, x_{2}\right)$, задана борелевская функция $g: \mathbb{R}^{2} \rightarrow \mathbb{R}$. Требуется найти функцию распределения (и плотность, если она существует) случайной величины $\eta=g\left(\xi_{1}, \xi_{2}\right)$.
\begin{lem}
    Пусть $x \in \mathbb{R}$, задана область $D_{x} \subseteq \mathbb{R}^{2},~ D_x = \{(u,v): g(u,v) < x\}$ Тогда случайная величина $\eta=g\left(\xi_{1}, \xi_{2}\right)$ имеет функцию распределения
    \begin{equation*}
        F_{\eta}(x)=\mathbb{P}\left(g\left(\xi_{1}, \xi_{2}\right)<x\right)=\mathbb{P}\left(\left(\xi_{1}, \xi_{2}\right) \in D_{x}\right)=\iint\limits_{D_{x}} f_{\xi_{1}, \xi_{2}}(u, v) d u d v
    \end{equation*}
\end{lem}
Далее считаем, что случайные величины $\xi_1$ и $\xi_2$ независимы, т. е. $f_{\xi_{1}, \xi_{2}}(u, v) \equiv f_{\xi_{1}}(u) f_{\xi_{2}}(v)$. В этом случае распределение величины $g\left(\xi_{1}, \xi_{2}\right)$ полностью определяется частными распределениями величин $\xi_1$ и $\xi_2$.
\begin{namedthm}[Формула свёртки]
    Если случайные величины $\xi_1$ и $\xi_2$ независимы и имеют абсолютно непрерывные распределения с плотностями $f_{\xi_{1}}(u)$ и $f_{\xi_{2}}(v)$, то плотность распределения суммы $\xi_{1}+\xi_{2}$ существует и равна <<свёртке>> плотностей $f_{\xi_{1}}$ и $f_{\xi_{2}}$:
    \begin{equation*}
        f_{\xi_{1}+\xi_{2}}(t)=\int\limits_{-\infty}^{\infty} f_{\xi_{1}}(u) f_{\xi_{2}}(t-u) d u=\int\limits_{-\infty}^{\infty} f_{\xi_{2}}(u) f_{\xi_{1}}(t-u) d u
    \end{equation*}
\end{namedthm}

\begin{proof}
    Воспользуемся утверждением вышеуказанной леммы для борелевской функции $g(u, v)=u+v$. Интегрирование по двумерной области $D_{x}=\{(u, v) \colon u+v<x\}$ можно заменить последовательным вычислением двух интегралов: наружного — по переменной $u$, меняющейся в пределах от $-\infty$ до $+\infty$, и внутреннего~--- по переменной $v$, которая при каждом $u$ должна быть меньше, чем $x-u$.Поэтому
    \begin{equation*}
        F_{\xi_{1}+\xi_{2}}(x)=\iint\limits_{D_{x}} f_{\xi_{1}}(u) f_{\xi_{2}}(v) d v d u=\int\limits_{-\infty}^{\infty}\left(\int\limits_{-\infty}^{x-u} f_{\xi_{1}}(u) f_{\xi_{2}}(v) d v\right) d u
    \end{equation*}
    
    Сделаем в последнем интеграле замену $v=t-u$. При этом $v \in(-\infty, x-u) \Leftrightarrow t \in(-\infty, x), d v=d t$. В полученном интеграле меняем порядок интегрирования:
    \begin{equation*}
        F_{\xi_{1}+\xi_{2}}(x)=\int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{x} f_{\xi_{1}}(u) f_{\xi_{2}}(t-u) d t d u=\int\limits_{-\infty}^{x}\left(\int\limits_{-\infty}^{\infty} f_{\xi_{1}}(u) f_{\xi_{2}}(t-u) d u\right) d t
    \end{equation*}
    Из функции распределения $F_{\xi_{1}+\xi_{2}}(x)$ выражается плотность $f_{\xi_{1}+\xi_{2}}(t)$.
\end{proof}

\subsubsection{Ковариация, коэффициент корреляции, их свойства}

Рассмотрим случайные величины $\xi$ и $\eta$. Дисперсия их суммы в общем случае равна
\begin{equation*}
    \mathbb{D}(\xi+\mathrm{n})=\mathbb{D} \xi+\mathbb{D} \eta+2(\mathbb{E}(\xi \eta)-\mathbb{E} \xi \mathbb{E} \eta)
\end{equation*}
\begin{defn}
    Величина $\mathbb{E}(\xi \eta)-\mathbb{E} \xi \mathbb{E} \eta = \operatorname{cov}(\xi, \eta)$ называется {\it ковариацией} случайных величин $\xi$ и $\eta$.
\end{defn}
Если $\xi$ и $\eta$ независимы, то $\operatorname{cov}(\xi, \eta) = 0$. Обратное, вообще говоря, неверно.
\begin{exmp}
    Рассмотрим $\xi \sim \mathbf{U}[0;1]$, случайные величины $\eta_{1}=\cos \xi$ и $\eta_{2}=\sin \xi$.
    \begin{enumerate}
        \item Докажем некореллированность данных случайных величин.
        \begin{gather*}
            \mathbb{E} \eta_{1}=\int\limits_{-\pi}^{\pi} \cos x \cdot \frac{1}{2 \pi} d x=0, \quad \mathbb{E} \eta_{2}=\int\limits_{-\pi}^{\pi} \sin x \cdot \frac{1}{2 \pi} d x=0 \\
            \mathbb{E} \eta_{1} \eta_{2}=\int\limits_{-\pi}^{\pi}(\cos x \sin x) \frac{1}{2 \pi} d x=\frac{1}{4 \pi} \int\limits_{-\pi}^{\pi} \sin 2 x d x=0
        \end{gather*}
        Следовательно, $\operatorname{cov}(\eta_1, \eta_2) = 0$
        \item Докажем зависимость $\eta_1$ и $\eta_2$. Рассмотрим события:
        \begin{equation*}
            A = \left\{\omega \colon \eta_1(\omega) \in \left[0, \frac{1}{2} \right] \right\}, \quad
            B = \left\{\omega \colon \eta_2(\omega) \in \left[0, \frac{1}{2} \right] \right\},
        \end{equation*}
        Проверим по критерию независимости:
        \begin{gather*}
            \mathbb{P}\left\{\eta_{1} \in\left[0, \frac{1}{2}\right]\right\}=\mathbb{P}\left\{\xi \in\left[-\frac{\pi}{2},-\frac{\pi}{3}\right] \cup\left[\frac{\pi}{3}, \frac{\pi}{2}\right]\right\}=\frac{1}{2 \pi} \cdot 2 \cdot \frac{\pi}{6}=\frac{1}{6} \\
            \mathbb{P}\left\{\eta_{2} \in\left[0, \frac{1}{2}\right]\right\}=\mathbb{P}\left\{\xi \in\left[0, \frac{\pi}{6}\right] \cup\left[\frac{5 \pi}{6}, \pi\right]\right\}=\frac{1}{2 \pi} \cdot 2 \cdot \frac{\pi}{6}=\frac{1}{6} \\
            \mathbb{P}\left\{\eta_{1} \in\left[0, \frac{1}{2}\right], \eta_{2} \in\left[0, \frac{1}{2}\right]\right\}=\mathbb{P}\{\varnothing\}=0 \neq \frac{1}{6} \cdot \frac{1}{6}
        \end{gather*}
        Следовательно, $\eta_1$ и $\eta_2$~--- зависимы.
    \end{enumerate}
\end{exmp}

\begin{namedthm}[Свойства ковариации]\leavevmode
    \begin{enumerate}
        \item $\operatorname{cov}(\xi, \xi)=\mathbb{D} \xi$;
        \item $\operatorname{cov}(\xi, \eta)=\operatorname{cov}(\eta, \xi)$;
        \item $\operatorname{cov}(a \xi + b, \eta)=a \operatorname{cov}(\xi, \eta)$, если $a, b \in \mathbb{R}$;
        \item $\operatorname{cov}(\eta + \zeta, \xi) = 
        \operatorname{cov}(\eta, \xi) + \operatorname{cov}(\zeta, \xi)$;
        \item $\operatorname{cov}^2(\xi, \eta) \leqslant \mathbb{D}\xi \, \mathbb{D}\eta$,
        
        $\operatorname{cov}^2(\xi, \eta) = \mathbb{D}\xi \,\mathbb{D}\eta \; \Leftrightarrow \; \xi \overset{\text{п.н.}}{=} a\eta + b, \; a, b \in \mathbb{R}$;
        
        (Аналог неравенства Коши-Буняковского)
    \end{enumerate}
\end{namedthm}

Величина ковариации характеризует меру (линейной) зависимости случайных величин. Однако от умножения на константу (не равную нулю) зависимость случайных величин не изменяется никак, в отличие от ковариации. Введём новый термин.

\begin{defn}
    {\it Коэффициент корреляции} $\rho(\xi,\eta)$ случайных величин $\xi$ и $\eta$, дисперсии которых существуют и отличны от нуля:
    \begin{equation*}
        \rho(\xi, \eta)=\frac{\operatorname{cov}(\xi, \eta)}{\sqrt{\mathbb{D} \xi} \sqrt{\mathbb{D} \eta}}
    \end{equation*}
\end{defn}

\begin{namedthm}[Свойства коэффициента корреляции]\leavevmode
    \begin{enumerate}
        \item Коэффициент корреляции независимых случайных величин равен нулю.
        \item Для любых двух случайных величин (для которых выполнены условия определения) их коэффициент корреляции по модулю не превосходит единицы.
        \item Если $|\rho(X,Y)| = 1$, то с вероятностью один $X$ и $Y$ линейно выражаются друг через друга. То есть,
        \begin{equation*}
            |\rho(X, Y)|=1 \Longrightarrow \exists \: b \neq 0, \, c \in \mathbb{R}: \mathbb{P}(X-b Y=c)=1
        \end{equation*}
        При этом знак коэффициента $b$ совпадает со знаком коэффициента корреляции.
    \end{enumerate}
\end{namedthm}

\begin{proof}
    \begin{enumerate}
    \item В числителе дроби, которой равен коэффициент корреляции,
окажется ноль. В знаменателе нуля быть не должно, это обеспечивается определением.

    \item  Обозначим эти две случайные величины как $\xi$ и $\eta$ и центрируем: $\xi_c = \xi - \mathbb{E}\xi$ и $\eta_c = \eta - \mathbb{E}\eta$. Так как $\operatorname{cov}(\xi, \eta)=\operatorname{cov}\left(\xi_{c}, \eta_{c}\right)$, а дисперсия случайной величины не меняется от смещения случайной величины на константу, коэффициент корреляции не изменится.
    
    Далее, т.к. $\mathbb{E} \xi_{c}=\mathbb{E} \eta_{c}=0$:
    \begin{gather*}
        \mathbb{D} \xi_{c}=\mathbb{E} \xi_{c}^{2}-\left(\mathbb{E} \xi_{c}\right)^{2}=\mathbb{E} \xi_{c}^{2},~ \mathbb{D} \eta_{c}=\mathbb{E} \eta_{c}^{2} \\
        \operatorname{cov}\left(\xi_{c}, \eta_{c}\right)=\mathbb{E}\left(\xi_{c} \eta_{c}\right)-\mathbb{E} \xi_{c} \mathbb{E} \eta_{c}=\mathbb{E}\left(\xi_{c} \eta_{c}\right)
    \end{gather*}
    
    Далее идут те же рассуждения, что часто используются при доказательстве неравенства Коши-Буняковского:
    \begin{equation*}
        \forall a \in \mathbb{R} \quad 0 \leqslant \mathbb{D}\left(\xi_{c}-a \eta_{c}\right)=\mathbb{E}\left(\xi_{c}-a \eta_{c}\right)^{2}-\left(\mathbb{E}\left(\xi_{c}-a \eta_{c}\right)\right)^{2}=\mathbb{E}\left(\xi_{c}-a \eta_{c}\right)^{2}
    \end{equation*}
    
    Полученное неравенство можно рассматривать как квадратное неравенство относительно $a$, а именно
    \begin{equation*}
        \mathbb{E}\left(\xi_{c}-a \eta_{c}\right)^{2}=\mathbb{E} \xi_{c}^{2}-2 a \mathbb{E}\left(\xi_{c} \eta_{c}\right)+a^{2} \mathbb{E} \eta_{c}^{2} \geqslant 0
    \end{equation*}
    
    Поскольку верно это для любого $a$, то дискриминанту нельзя быть больше нуля. То есть:
    \begin{multline*}
        \left(\mathbb{E}\left(\xi_{c} \eta_{c}\right)\right)^{2}-\mathbb{E} \xi_{c}^{2} \mathbb{E} \eta_{c}^{2} \leqslant 0 \Longleftrightarrow\left|\mathbb{E}\left(\xi_{c} \eta_{c}\right)\right| \leqslant \sqrt{\mathbb{E} \xi_{c}^{2} \mathbb{E} \eta_{c}^{2}} \Rightarrow \\
        \Rightarrow\left|\operatorname{cov}\left(\xi_{c}, \eta_{c}\right)\right| \leqslant \sqrt{\mathbb{D} \xi_{c} \mathbb{D} \eta_{c}}
    \end{multline*}
    
    По доказанному выше <<стирание>> индексов не изменит коэффициентов.

    \item Доказательство этого свойства целиком опирается на доказательство предыдущего: если выполнилось равенство $|\operatorname{cov}(\xi, \eta)|=\sqrt{\mathbb{D} \xi \mathbb{D} \eta}$, то квадратное неравенство относительно $a$ может обращаться в равенство при некотором $a = b$. Но это равенство означает, что равна нулю $\mathbb{D}(\xi-b \eta)$, а это сразу говорит о том, что с вероятностью один $\xi - b\eta$ равна константе. Обозначим эту константу за $c$ и получим то, что нужно было доказать.
    
    Знак коэффициента корреляции совпадает с знаком ковариации, так дисперсии по предположению положительны. Выразив $\xi$ через $\eta$, мы можем воспользоваться свойствами ковариации и получить
    $$ \rho(\xi, \eta) = \rho(b\eta + c, \eta)=
    \frac{\operatorname{cov}(b\eta + c, \eta)}
    {\sqrt{\mathbb{D}(b\eta + c) \, \mathbb{D}\eta}} = 
    \frac{b\operatorname{cov}(\eta, \eta)}
    {\sqrt{b^2\, \mathbb{D}\eta \, \mathbb{D}\eta}} = 
    \frac{b\mathbb{D}\eta}
    {|b|\mathbb{D}\eta} = 
    \text{sign}(b).
    $$

\end{enumerate}
\end{proof}

\section{Виды сходимости последовательностей случайных величин}
\begin{defn}
    Последовательность случайных величин $\{\xi_n\}$ {\it почти наверное сходится} к случайной величине $\xi$ ($\xi_n \xrightarrow[]{\text{п.н.}} \xi$), если
    \begin{equation*}
        \mathbb{P}\left(\left\{\omega \colon \lim\limits _{h \rightarrow \infty} \xi_{n}(w)=\xi(w)\right\}\right)=1.
    \end{equation*}
\end{defn}

\begin{defn}
    Последовательность случайных величин $\{\xi_n\}$ {\it сходится по вероятности} к случайной величине $\xi$ ($\xi_n \xrightarrow[]{\text{p}} \xi$), если
    \begin{equation*}
        \forall \varepsilon>0 \quad P\left(\left\{\omega \colon |\xi_{n}(\omega)-\xi(\omega)|>\varepsilon\right\}\right) \xrightarrow[n \to +\infty]{} 0.
    \end{equation*}
\end{defn}

\begin{defn}
    Последовательность случайных величин $\{\xi_n\}$ {\it сходится в среднем} к случайной величине $\xi$ ($\xi_n \xrightarrow[]{\text{(r)}} \xi$), если
    \begin{equation*}
        \mathbb{E}\left|\xi_{n}-\xi\right|^{r} \xrightarrow[n \to +\infty]{} 0.
    \end{equation*}
\end{defn}

\begin{defn}
    Последовательность случайных величин $\{\xi_n\}$ {\it сходится по распределению} к случайной величине $\xi$ ($\xi_n \xrightarrow[]{\text{d}} \xi$), если
    \begin{equation*}
        F_{\xi n}(x) \xrightarrow[n \to +\infty]{} F_{\xi}(x) \quad \forall x, \, \text{в которых}~ F_{\xi} ~\text{непрерывна}.
    \end{equation*}
\end{defn}

\begin{defn}
    Последовательность случайных величин $\{\xi_n\}$ {\it слабо сходится} к случайной величине $\xi$ ($\xi_n \stackrel{\text{w}}{\Rightarrow} \xi$), если
    \begin{equation*}
        \mathbb{E} f\left(\xi_{n}\right) \rightarrow \mathbb{E} f(\xi) \quad \forall~ \text{непрерывной ограниченной}~ f(x).
    \end{equation*}
\end{defn}
\begin{thm*}
    Вышеуказанные виды сходимости последовательностей случайных величин связаны следующими отношениями:
    
    \adjustbox{scale=1.25,center}
    {%
    \begin{tikzcd}[column sep=scriptsize, row sep=tiny]
    \text{п.н.} \arrow[dr, Rightarrow] & &  & \\
    & \text{p} \arrow[r, Rightarrow] & \text{d} \arrow[r, Leftrightarrow] & \text{w} \\
    \text{(r)} \arrow[ur, Rightarrow] & & &
    \end{tikzcd}
   }
\end{thm*}

\begin{proof}
    \begin{itemize}
        \item[$\text{(r)} \Rightarrow \text{p}$] Из \hyperlink{cheb}{обобщённого неравенства Чебышёва}:
    \begin{equation*}
        \mathbb{P}(|\xi_n - \xi| \geqslant \varepsilon) \leqslant \cfrac{\mathbb{E}|\xi_n - \xi|^{r}}{\varepsilon^{r}} \xrightarrow[n \to +\infty]{} 0
    \end{equation*}
    
    \item[$\text{(r)} \nLeftarrow \text{p}$] Рассмотрим последовательность случайных величин:
    \begin{gather*}
        \xi_n = 
        \begin{cases}
            0, & \frac{1}{n} \leqslant \omega \leqslant 1; \\
            \sqrt[r]{n}, & 0 \leqslant \omega \leqslant \frac{1}{n}.
        \end{cases}
        \Rightarrow p_1 = 1 - \frac{1}{n},~ p_2 = \frac{1}{n} \\
        \mathbb{P}(|\xi_n| > \varepsilon) \xrightarrow[n \to +\infty]{},~ \text{однако $~\mathbb{E}|\xi_n|^{r} = 1$}.
    \end{gather*}
    
    \item[$\text{п.н.} \Rightarrow \text{p}$]
    Ограничимся для простоты случаем, когда $\xi_n(\omega) \rightarrow \xi(\omega)$ для любого $\omega$. Зафикисируем $\omega \in \Omega.$ По определению предела, $\xi_n(\omega) \xrightarrow[n \to +\infty]{} \xi(\omega)$, если для всякого $\varepsilon > 0$ найдётся $N = N(\omega, \varepsilon) \geqslant 0$ такое, что для всех $n > N$ выполняется неравенство $|\xi_n(\omega) - \xi(\omega)| < \varepsilon$.
    
    Событие $A = \{n > N(\omega,\varepsilon) \}$ влечёт событие $B = \{|\xi_n(\omega) - \xi(\omega)| < \varepsilon \}$. Тогда 
    $$1 \geqslant \mathbb{P}(B) \geqslant \mathbb{P}(A)=\mathbb{P}(N(\omega, \varepsilon)<n)=F_{N(\varepsilon, \omega)}(n) \xrightarrow[n \to +\infty]{} 1.$$ по свойству функции распределения. Таким образом, было получено, что $\mathbb{P}(B) \rightarrow 1$, т.е. $\xi_n \xrightarrow[]{\text{p}} \xi.$
    
    \item[$\text{п.н.} \nLeftarrow \text{p}$]
    
    Положим $\xi_{2^{k}}=\mathbb{I}\left(\left[0, \frac{1}{2^{k}}\right]\right), \xi_{2^{k}+p}=\mathbb{I}\left(\left[\frac{p}{2^{k}}, \frac{p+1}{2^{k}}\right]\right), 1 \leqslant p<2^{k}.$ Тогда $\xi_n \xrightarrow[]{\text{p}} 0$, т.к. $\mathbb{P}(\xi_n > 0) \leqslant$ длина отрезка в индикаторе $\leqslant \frac{2}{n} \rightarrow 0$, но $\xi_n \overset{\text{п.н.}}{\rightarrow} 0$, т.к. $\forall \omega \quad \exists$ бесконечно много $n$, таких что $\xi_n(\omega) = 1.$
    
    \item[$\text{п.н.} \nLeftarrow \text{(r)}$] См. предыдущий пример.
    
    \item[$\text{(r)} \nLeftarrow \text{п.н.}$]
    
    $\Omega = [0,1], \mathcal{F} = \mathcal{B}([0,1])$, $\mathbb{P}$ - равномерное распределение. Определим для $k \geqslant 1 \quad \xi_k = 2^{k-1} \mathbb{I}\left(\left[0, \frac{1}{2^{k-1}}\right]\right).$ Тогда $\forall k \quad \mathbb{E}\xi_k = 1$, но $\xi = \mathbb{I}(\omega = 0)$.
    
    \item[$\text{p} \Rightarrow \text{d}$]
    
    Пусть $f$ - ограниченная и непрерывная функция, $|f| \leqslant C$. Зафиксируем $\varepsilon > 0$. Т.к. $\mathbb{P}(|\xi| = \infty) = 0$, то $\exists N, \exists \delta$:
    
    \begin{enumerate}
        \item $\mathbb{P}(|\xi| > N) \leqslant \frac{\varepsilon}{6C}$, т.к. $\mathbb{P}(\xi = \infty) = 0$.
        \item $\mathbb{P}(|\xi_n - \xi| > \delta) \leqslant \frac{\varepsilon}{6C}$ из сходимости по вероятности (при достаточно больших $n$).
        \item $\forall x,y |x| < N, |x - y| < \delta|f(x) - f(y)| \leqslant \frac{\varepsilon}{3}$, т.к. $f$ равномерно непрерывна на отрезке $[-N, N]$.
    \end{enumerate}
    
    Рассмотрим следующие события:
    
    $$ A_{1}=\left\{\left|\xi_{n}-\xi\right| \leqslant \delta\right\} \cap\{|\xi|<N\} $$
    $$ A_{2}=\left\{\left|\xi_{n}-\xi\right| \leqslant \delta\right\} \cap\{|\xi| \geqslant N\} $$
    $$ A_{3}=\left\{\left|\xi_{n}-\xi\right|>\delta\right\} $$
    
    Эти события образуют разбиение $\Omega=A_{1} \sqcup A_{2} \sqcup A_{3} $. 
    
    Оценим $|\mathbb{E}f(\xi_n) - \mathbb{E}f(\xi)|$:
    
    $$\left|\mathbb{E} f\left(\xi_{n}\right)-\mathbb{E} f(\xi)\right|=\left|\mathbb{E}\left(f\left(\xi_{n}\right)-f(\xi)|\leqslant \mathbb{E}| f\left(\xi_{n}\right)-f(\xi) |=\right.\right.$$
    $$=\mathbb{E}\left|f\left(\xi_{n}\right)-f(\xi)\right|\left(\mathbb{I}_{A_{1}}+\mathbb{I}_{A_{2}}+\mathbb{I}_{A_{3}}\right) \leq$$
    $$\leqslant \frac{\varepsilon}{3} \mathbb{P}\left(A_{1}\right)+2 C\left(\mathbb{P}\left(A_{2}\right)+\mathbb{P}\left(A_{3}\right)\right) \leqslant \frac{\varepsilon}{3}+2 C\left(\frac{\varepsilon}{6 C}+\frac{\varepsilon}{6 C}\right)=\varepsilon,$$
    
    откуда следует, что $|\mathbb{E}f(\xi_n) - \mathbb{E}f(\xi)| \rightarrow 0 \Rightarrow \xi_n \xrightarrow[]{\text{d}} \xi$.
    
    \item[p $\nLeftarrow d$]
    
    Пусть $\xi_n = \begin{cases}
    1, \; p_1 = \frac{1}{2} \\
    0, \; p_0 = \frac{1}{2}
    \end{cases}, \; 
    \xi = \begin{cases}
    1, \; p_1 = \frac{1}{2} \\
    0, \; p_0 = \frac{1}{2}
    \end{cases}.$ \\
    Тогда $|\xi_n - \xi| = \begin{cases}
    1, \; p_1 = \frac{1}{2} \\
    0, \; p_0 = \frac{1}{2}
    \end{cases},$ \; и не выполняется определение сходимости по вероятности, например, при $\varepsilon_0 = \frac{1}{3}$, т.к. 
    $$ \mathbb{P}\left({|\xi_n - \xi| > \frac{1}{3}}\right) = \frac{1}{2} {\nrightarrow} 0 \text{ при } n \to \infty.
    $$
    
   \item[$\text{p} \nLeftarrow \text{w}$]
    
    Пусть $\Omega = \{\omega_1, \omega_2 \}, \mathbb{P}(\{\omega_i \}) = \frac{1}{2}.$ 
    
    Определим для любого $n$ $\xi_n(\omega_1) = 1, \xi_n(\omega_2) = -1.$ Положим $\xi = -\xi_n.$ Тогда:
    
    $$ \mathbb{E}f(\xi_n) = \frac{f(1) + f(-1)}{2} = \mathbb{E}f(\xi),$$
    
    но $\forall n \quad |\xi_n - \xi| = 2 \Rightarrow \xi_n \overset{\text{p}}{\nrightarrow} \xi.$
    
\end{itemize}    
\end{proof}

\begin{rmrk}
    Cлабая сходимость всё же не есть сходимость случайных величин, и ею нельзя оперировать как сходимостями п.н. и по вероятности, для которых предельная случайная величина единственна (с точностью до значений на множестве нулевой вероятности).
\end{rmrk}

\section{Неравенства Маркова, Чебышёва и Гаусса. Правило «трех сигм». Закон больших чисел в форме Чебышёва}
\begin{namedthm}[Неравенство Маркова]
    Если $\mathbb{E}|\xi| < \infty$, то для любого $x > 0$
    \begin{equation*}
        \myprob{|\xi| \geqslant x} \leqslant \frac{\mathbb{E}|\xi|}{x}
    \end{equation*}
\end{namedthm}

\begin{proof}
\begin{equation*}
    \mathbb{I}(A) \sim \mathbf{Bi}(p),~ p = \myprob{\mathbb{I}(A) = 1} = \myprob{A} = \mathbb{E}\mathrm{I}(A)
\end{equation*}

Индикаторы прямого и противоположного событий связаны равенством $\mathbb{I}(A) + \mathbb{I}(\overline{A}) = 1.$ Поэтому
\begin{equation*}
    |\xi|=|\xi| \cdot \mathbb{I}(|\xi|<x)+|\xi| \cdot \mathbb{I}(|\xi| \geqslant x) \geqslant|\xi| \cdot \mathbb{I}(|\xi| \geqslant x) \geqslant x \cdot \mathbb{I}(|\xi| \geqslant x)
\end{equation*}

Тогда $\mathbb{E}|\xi| \geqslant \mathbb{E}(x \cdot \mathbb{I}(|\xi| \geqslant x))=x \cdot \mathbb{P}(|\xi| \geqslant x)$. Осталось разделить обе части этого неравенства на положительное число $x$.
\end{proof}

\hypertarget{cheb}{}
\begin{crlr}[Обобщённое неравенство Чебышёва] 
Пусть функция $g$ не убывает и неотрицательна на $\mathbb{R}$. Если $\mathbb{E}g(\xi) < \infty$, то для любого $x \in \mathbb{R}$
\begin{equation*}
    \mathbb{P}(\xi \geqslant x) \leqslant \frac{E g(\xi)}{g(x)}
\end{equation*}
\end{crlr}
\begin{proof}
    Заметим, что $\myprob{\xi \geqslant x} \leqslant \myprob{g(\xi) \geqslant g(x)}$, поскольку функция $g$ не убывает. Оценим последнюю вероятность по неравенству Маркова, которое можно применять в силу неотрицательности $g$
    \begin{equation*}
        \mathbb{P}(g(\xi) \geqslant g(x)) \leqslant \frac{\mathbb{E} g(\xi)}{g(x)}
    \end{equation*}
\end{proof}
\begin{crlr}[Неравенство Чебышёва]
    Если $\mathbb{D}\xi$ существует, то для любого $x > 0$
    \begin{equation*}
        \mathbb{P}(|\xi-\mathbb{E} \xi| \geqslant x) \leqslant \frac{\mathbb{D} \xi}{x^{2}}
    \end{equation*}
\end{crlr}
\begin{proof}
    Для $x > 0$ неравенство $|\xi - \mathbb{E}\xi| \geqslant x \Leftrightarrow (\xi - \mathbb{E}\xi)^2 \geqslant x^2$, поэтому
    \begin{equation*}
        \mathbb{P}(|\xi-\mathbb{E} \xi| \geqslant x)=\mathbb{P}\left((\xi-\mathbb{E} \xi)^{2} \geqslant x^{2}\right) \leqslant \frac{\mathbb{E}(\xi-\mathbb{E} \xi)^{2}}{x^{2}}=\frac{\mathbb{D} \xi}{x^{2}}
    \end{equation*}
\end{proof}
\begin{defn}
    В неравенстве Чебышёва в качестве $x$ можно брать любое положительное число. Если взять в качестве $x$ величину $3\sigma$, где $\sigma$~--- стандартное отклонение (то есть именно корень из дисперсии), то получится
    \begin{equation*}
        \mathbb{P}(|X-\mathbb{E} X|>3 \sigma) \leqslant \frac{\mathbb{D} X}{9 \mathbb{D} X}=\frac{1}{9} \Leftrightarrow \mathbb{P}(|X-\mathbb{E} X| \leqslant 3 \sigma) \geqslant 1-\frac{1}{9}=\frac{8}{9}
    \end{equation*}
    Это соотношение называется {\it правилом трёх сигм}.
\end{defn}

\begin{namedthm}[Неравенство Гаусса]
    Пусть $X$ - случайная величина с модой $m$ и пусть $a^2$ - математическое ожидание $(X - m)^2.$ Тогда
    \begin{equation*}
        \mathbb{P}(|X-m|>k) \leq
        \begin{cases}
            \left(\cfrac{2 a}{3 k}\right)^{2}, & \text{если $k \geqslant \frac{2 a}{\sqrt{3}}$;} \\
            1 - \cfrac{k}{a \sqrt{3}}, & \text{если $0 \leqslant k \leqslant \frac{2 a}{\sqrt{3}}$;}
        \end{cases}
    \end{equation*}
\end{namedthm}

\begin{defn}
    Говорят, что последовательность случайных величин $\xi_1, \xi_2, \ldots$ с конечными первыми моментами {\it удовлетворяет закону больших чисел}, если
    \begin{equation*}
        \frac{\xi_{1}+\ldots+\xi_{n}}{n}-\frac{\mathbb{E} \xi_{1}+\ldots+\mathbb{E} \xi_{n}}{n} \stackrel{\mathbb{P}}{\longrightarrow} 0 \: \text {при} \: n \to +\infty
    \end{equation*}
\end{defn}
\begin{namedthm}[Закон больших чисел в форме Чебышёва]
    Для любой последовательности $\xi_1, \xi_2, \ldots$ попарно независимых и одинаково распределённых случайных величин с конечным вторым моментом $\mathbb{E}\xi_1^2 < \infty$ имеет место сходимость
    \begin{equation*}
        \frac{\xi_{1}+\ldots+\xi_{n}}{n} \stackrel{\mathbb{P}}{\longrightarrow} \mathbb{E} \xi_{1}
    \end{equation*}
\end{namedthm}

\begin{proof}
    Обозначим через $S_n = \xi_1 + \ldots + \xi_n$ сумму первых $n$ случайных величин. Из линейности матожидания получим
    \begin{equation*}
        \mathbb{E}\left(\frac{S_{n}}{n}\right)=\frac{\mathbb{E} \xi_{1}+\ldots+\mathbb{E} \xi_{n}}{n}=\frac{n \mathbb{E} \xi_{1}}{n}=\mathbb{E} \xi_{1}
    \end{equation*}
    
    Пусть $\varepsilon > 0.$ Воспользуемся неравенством Чебышёва:
    \begin{multline*}
        \mathbb{P}\left(\left|\frac{S_{n}}{n}-\mathbb{E}\left(\frac{S_{n}}{n}\right)\right| \geqslant \varepsilon\right) \leqslant \frac{\mathbb{D}\left(\frac{S_{n}}{n}\right)}{\varepsilon^{2}}
        = \frac{\mathbb{D} S_{n}}{n^{2} \varepsilon^{2}}
        = \frac{\mathbb{D} \xi_{1}+\ldots+\mathbb{D} \xi_{n}}{n^{2} \varepsilon^{2}}= \\
        = \frac{n \mathbb{D} \xi_{1}}{n^{2} \varepsilon^{2}}
        = \frac{\mathbb{D} \xi_{1}}{n \varepsilon^{2}} \xrightarrow[n \to +\infty]{} 0,
    \end{multline*}
    так как $\mathbb{D}\xi_1 < \infty$. Дисперсия суммы превратилась в сумму дисперсий в силу попарной независимости слагаемых, из-за которой все ковариации $\operatorname{cov}(\xi_i, \xi_j)$ по свойству ковариации обратились в нуль при $i \neq j$.
\end{proof}

\section{Характеристические функции и их свойства}
\begin{defn}
    {\it Характеристическая функция случайной величины} $\xi$~--- функция $\varphi_{\xi}: \mathbb{R} \rightarrow \mathbb{C}$:
    \begin{equation*}
        \varphi_{\xi}(t)
        = \mathbb{E} e^{i t \xi}
        = \mathbb{E} \cos (t \xi)+i \mathbb{E} \sin (t \xi) = \int\limits_{\mathbb{R}}^{} e^{\mathrm{i} t x} d F_{\xi}(x),
    \end{equation*}
    где интеграл справа называется {\it интегралом Фурье-Стильтьеса}.
    
    Для абсолютно непрерывного распределения характеристическая функция имеет вид
    \begin{equation*}
        \varphi_{\xi}(t)=\int\limits_{\mathbb{R}} e^{i t x} f(x) d x
    \end{equation*}
    Для дискретного, соответственно
    \begin{equation*}
        \varphi_{\xi}(t)=\sum\limits_{i} e^{i t x_{i}} \mathbb{P}\left\{\xi=x_{i}\right\}
    \end{equation*}
\end{defn}

\begin{exmp}
    Характеристическая функция случайной величины $\xi \sim \mathbf{N}(0;1)$:
    \begin{multline*}
        \varphi_{\xi}(t) 
        = \frac{1}{\sqrt{2 \pi}} \int\limits_{-\infty}^{\infty} e^{i t x} e^{-x^{2} / 2} d x
        = \frac{1}{\sqrt{2 \pi}} \int\limits_{-\infty}^{\infty} e^{-t^{2} / 2} e^{-x^2/2 \,+\, itx \,+\, t^2/2} d x = \\
        = \frac{1}{\sqrt{2 \pi}} \int\limits_{-\infty}^{\infty} e^{-t^{2} / 2} e^{-(x-i t)^{2} / 2} d x =  
         e^{-t^{2} / 2} \frac{1}{\sqrt{2 \pi}} \int\limits_{-\infty}^{\infty} e^{-(x-i t)^{2} / 2} d(x-i t)
        = e^{-t^{2} / 2}
    \end{multline*}
\end{exmp}

\begin{namedthm}[Свойства характеристической функции]\leavevmode
    \begin{enumerate}
        \item Характеристическая функция существует для любой случайной величины $\xi$.
        \item $\forall~ \xi,~ \forall~ a, b \in \mathbb{R}$:
        \begin{equation*}
            \varphi_{a \xi + b}(t) = e^{itb} \varphi_{\xi}(at)
        \end{equation*}
        \item $|\varphi_{\xi}(t)|=|\mathbb{E} e^{i t \xi}| \leqslant 1,~ \varphi_{\xi}(0) = 1, ~\overline{\varphi_{\xi}(t)} = \varphi_{\xi}(-t) = \varphi_{-\xi}(t) ~\forall t \in \mathbb{R}$
        
        \mycon{}
        Если характеристическая функция вещественнозначна, то она является чётной.
        
        \item Если случайные величины $\xi$ и $\eta$ независимы, то
        \begin{equation*}
            \varphi_{\xi + \eta}(t) = \varphi_{\xi}(t) \varphi_{\eta}(t)
        \end{equation*}
        \item Характеристическая функция равномерно непрерывна.
        \item Если существует абсолютный момент $k$-го порядка $\mathbb{E}|\xi|^{k} < \infty,~ k \geqslant 1$, то существует непрерывная $k$-я производная характеристической функции:
        \begin{equation*}
            \left.\cfrac{\partial^{k} \varphi_{\xi}(t)}{\partial t^{k}}\right|_{t=0}= i^{k} \mathbb{E} \xi^{k}
        \end{equation*}
        
        Если существует непрерывная производная характеристической функции порядка $k = 2n, n \in \mathbb{N}$, то существует абсолютный момент порядка $k = 2n: \; \mathbb{E}|\xi|^k = \mathbb{E}\xi^k$ (а следовательно, и все предыдущие) и его можно вычислить по той же формуле.
        
        \item Характеристическая функция случайно величины $\xi$ однозначно определяет её функцию распределения $F_{\xi}$. Функция распределения восстанавливаются по характеристической функции с помощью обратных преобразований Фурье.
        
        Дискретное распределение:
        \begin{equation*}
            \mathbb{P}(\xi=k)=\frac{1}{2 \pi} \int\limits_{-\pi}^{\pi} e^{-i t k} \varphi_{\xi}(t) d t, k \in \mathbb{Z}
        \end{equation*}
        
        Абсолютно непрерывное распределение:
        \begin{equation*}
            f_{\xi}(x)=\frac{1}{2 \pi} \int\limits_{-\infty}^{\infty} e^{-i t x} \varphi_{\xi}(t) d t, x \in \mathbb{R}
        \end{equation*}
    \end{enumerate}
\end{namedthm}

\begin{proof}
    \begin{enumerate}
        \item Существование характеристической функции равносильно равномерной сходимости соответствующего интеграла. Докажем её по признаку Вейерштрасса:
        \begin{equation*}
            \left|\varphi_{\xi}(t)\right|=\left|\int\limits_{\mathbb{R}} e^{i t x} d F(x)\right| \leqslant \int\limits_{\mathbb{R}}\left|e^{i t x}\right| d F(x)=\int\limits_{\mathbb{R}} d F(x)=1
        \end{equation*}
        \item $\varphi_{a \xi+b}(t) 
        = \mathbb{E}e^{i t(a \xi+b)}
        = e^{i t b} \mathbb{E}e^{i t a \xi}
        = e^{i t b} \varphi_{\xi}(t a)$
        \item Первое неравенство доказано в пункте 1, второе равенство очевидно.
        \begin{equation*}
            \varphi_{\xi}(-t) = \mathbb{E}cos(-t \xi) + i\mathbb{E}sin(-t \xi) = \mathbb{E}cos(t \xi) - i\mathbb{E}sin(t \xi) = \overline{\varphi_{\xi}(t)}
        \end{equation*}
        Оставшиеся равенства следуют из свойства линейности.
        \item $\varphi_{\xi + \eta}(t) 
        = \mathbb{E}e^{it(\xi + \eta)} 
        = \mathbb{E}e^{it\xi}\mathbb{E}e^{it\eta}
        = \varphi_{\xi}(t)\varphi_{\eta}(t)$
        \item Выберем сколь угодно малое $\varepsilon > 0$ и оценим разность значений характеристической функции в точках $t$ и $t + h$:
        \begin{multline*}
            |\varphi(t+h)-\varphi(t)| 
            = \left|\int\limits_{\mathbb{R}} \left(e^{\mathrm{i}(t+h) x}-e^{\mathrm{i} tx}\right) d F(x)\right|
            = \left|\int\limits_{\mathbb{R}} e^{\mathrm{i} t x}\left(e^{\mathrm{i} h x}-1\right) d F(x)\right| \leqslant \\
            \leqslant \int\limits_{\mathbb{R}} \left|e^{\mathrm{i} h x}-1\right| d F(x)=\int\limits_{|x| \leqslant R}\left|e^{\mathrm{i} h x}-1\right| d F(x)+\int\limits_{|x|>R}\left|e^{\mathrm{i} h x}-1\right| d F(x)
        \end{multline*}
        Теперь выберем $R$ настолько большим, чтобы $\mathbb{P}(|X|>R) < \frac{\varepsilon}{4}$. Поскольку $\left|e^{i h x}-1\right| \leqslant 2$, второй интеграл при этом не превосходит по величине $\frac{\varepsilon}{2}$. После этого выберем $h$ столь малым, чтобы $\left|e^{\mathrm{i} h x}-1\right|<\frac{\varepsilon}{2}~$ при всех $|x| \leqslant R$. Тогда и первый интеграл не превосходит $\frac{\varepsilon}{2}$ и, таким образом, по заданному $\varepsilon > 0$ подобрано столь малое $h >0$, что $|\varphi(t+h)-\varphi(t)|<\varepsilon~ \forall t \in \mathbb{R}$.
        \item Если существует $\mathbb{E}\xi^{k}<\infty,~ k \geqslant 1$, то для всех $m = \overline{1, k}$ существуют $\mathbb{E}\xi^{m}<\infty$. Следовательно,
        \begin{equation*}
            \left|\int\limits_{\mathbb{R}}(i x)^{m} e^{i t x} d F(x)\right| \leqslant \int\limits_{\mathbb{R}}|x|^{k} d F(x)=\mathbb{E}|\xi|^{m}<\infty \quad \forall m = \overline{1, k}
        \end{equation*}
        Т.е. интегралы $\int\limits_{\mathbb{R}}(i x)^{m} e^{i t x} d F(x)$ сходятся равномерно по $t$, а значит, дифференцирование по $t$ можно менять местами с операцией интегрирования, откуда
        \begin{equation*}
            \varphi_{\xi}^{(m)}(t)=i^{m} \int\limits_{\mathbb{R}} x^{m} e^{i t x} d F(x),~ \varphi_{\xi}^{(m)}(0)=i^{m} \int\limits_{\mathbb{R}} x^{m} d F(x)=i^{m} \mathbb{E}\xi^{m}
        \end{equation*}
        
        Пусть у характеристической функции существует непрерывная производная чётного порядка $k$. Характеристическая функция и её производные непрерывны, функция $e^{itx}$ бесконечно (а значит, и нужные нам $k$ раз) дифференцируема по $t$, и можно показать, что при этих условиях можно поменять знаки интегрирования и дифференцирования местами*. Тогда мы получим, что 
        $$\varphi^{(k)}(0) = i^k \left.\mathbb{E}\xi^k e^{itx}\right|_{t=0} = i^k \mathbb{E}\xi^k = 
        \int\limits_{\mathbb{R}} x^k dF(x)
        $$
        В силу чётности $k ~~ x^k \geqslant 0$, а значит, указанный интеграл сходится абсолютно, что и означает существование искомого математического ожидания.
        
        *Достаточно обозначить интеграл $\int\limits_{\mathbb{R}}{x^{m} e^{itx}dF(x)}, \, m = \overline{0, k-1}$ за $G(t)$ и расписать его производную по $t$ по определению, воспользовавшись тем, что $\int\limits_{\mathbb{R}} o(\Delta t) dF(x) = o(\Delta t) \int\limits_{\mathbb{R}} dF(x) = o(\Delta t)$. 
        
    \end{enumerate}
\end{proof}

\section{Закон больших чисел в форме Хинчина}
\begin{thm*}
    Для любой последовательности $\xi_{1}, \xi_{2}, \ldots$ независимых и одинаково распределённых случайных величин с конечным первым моментом $E\left|\xi_{1}\right|<\infty$ имеет место сходимость:
    \begin{equation*}
        \frac{S_{n}}{n} = \frac{\xi_{1}+\ldots+\xi_{n}}{n} \overset{\text{P}}{\longrightarrow} \mathbb{E} \xi_{1}
    \end{equation*}
\end{thm*}
Для доказательства теоремы нам потребуется следующая лемма.
\begin{lem}
    Если $\xi_{n} \Rightarrow c=const$, то $\xi_{n} \xrightarrow[]{\text{p}} c$
\end{lem}
\begin{proof}
    Пусть $\xi_{n} \Rightarrow c$, т.е.
    \begin{equation*}
        F_{\xi_{n}}(x) \rightarrow F_{c}(x) =
        \begin{cases}
            0, & x \leqslant c; \\
            1, & x > c.
        \end{cases}
    \end{equation*}
    при любом $x$, являющемся точкой непрерывности предельной функции $F_{c}(x)$, т. е. $\forall~ x \neq c$.
    
    Возьмём произвольное $\varepsilon>0$ и докажем, что $\mathbb{P}\left(\left|\xi_{n}-c\right|<\varepsilon\right) \rightarrow 1$:
    \begin{multline*}
        \mathbb{P}\left(-\varepsilon<\xi_{n}-c<\varepsilon\right)=\mathbb{P}\left(c-\varepsilon<\xi_{n}<c+\varepsilon\right) \geqslant \mathbb{P}\left(c-\varepsilon / 2 \leqslant \xi_{n}<c+\varepsilon\right)= \\
        \quad=F_{\xi_{n}}(c+\varepsilon)-F_{\xi_{n}}(c-\varepsilon / 2) \rightarrow F_{c}(c+\varepsilon)-F_{c}(c-\varepsilon / 2)=1-0=1
    \end{multline*}
    поскольку в точках $c+\varepsilon$ и $c-\varepsilon / 2$ функция $F_{c}$ непрерывна, и, следовательно, имеет место сходимость последовательностей $F_{\xi_{n}}(c+\varepsilon)$ к $F_{c}(c+\varepsilon)=1$ и $F_{\xi_{n}}(c-\varepsilon / 2)$ к $F_{c}(c-\varepsilon / 2)=0$.
    
    Осталось заметить, что $\mathbb{P}\left(\left|\xi_{n}-c\right|<\varepsilon\right)$ не бывает больше $1$, так что по свойству предела зажатой последовательности $\mathbb{P}\left(\left|\xi_{n}-c\right|<\varepsilon\right) \rightarrow 1$.
\end{proof}

Перейдём к доказательству теоремы.

\begin{proof}
    По вышеприведённому свойству сходимость по вероятности к постоянной эквивалентна слабой сходимости. Так как $a$~--- постоянная, достаточно доказать слабую сходимость $\frac{S_{n}}{n}$ к $a$. По теореме о непрерывном соответствии, эта сходимость имеет место тогда и только тогда, когда для любого $t \in \mathbb{R}$ сходятся характеристические функции
    \begin{equation*}
        \varphi_{S_{n} / n}(t) \rightarrow \varphi_{a}(t)=\mathbb{E} e^{i t a}=e^{i t a}
    \end{equation*}
    
    Найдём характеристическую функцию случайной величины $\frac{S_{n}}{n}$. Пользуясь свойствами характеристической функции, получаем
    \begin{equation*}
        \varphi_{S_{n} / n}(t)=\varphi_{S_{n}}\left(\frac{t}{n}\right)=\left(\varphi_{\xi_{1}}\left(\frac{t}{n}\right)\right)^{n}
    \end{equation*}
    
    Вспомним, что первый момент $\xi_{1}$ существует, поэтому мы можем разложить $\varphi_{\xi_{1}}(t)$ в ряд Тейлора в окрестности нуля:
    \begin{equation*}
        \varphi_{\xi_{1}}(t)=1+i t \mathbb{E} \xi_{1}+o(|t|)=1+i t a+o(|t|)
    \end{equation*}
    
    В точке $\frac{t}{n}$ соответственно:
    \begin{gather*}
        \varphi_{\xi_{1}}\left(\frac{t}{n}\right)=1+\frac{i t a}{n}+o\left(\left|\frac{t}{n}\right|\right) \\
        \varphi_{S_{n} / n}(t)=\left(\varphi_{\xi_{1}}\left(\frac{t}{n}\right)\right)^{n}=\left(1+\frac{i t a}{n}+o\left(\left|\frac{t}{n}\right|\right)\right)^{n}
    \end{gather*}
    
    При $n \to +\infty$ воспользуемся <<замечательным пределом>> $\left(1+\frac{x}{n}\right)^{n} \rightarrow e^{x}$ и получим:
    \begin{equation*}
        \varphi_{S_{n} / n}(t)=\left(1+\frac{i t a}{n}+o\left(\left|\frac{t}{n}\right|\right)\right)^{n} \rightarrow e^{i t a}
    \end{equation*}
\end{proof}

\section{Центральная предельная теорема}
\begin{namedthm}[Центральная предельная теорема]
    Пусть $\xi_{1}, \xi_{2}, \ldots$~--- последовательность независимых одинаково распределенных (невырожденных) случайных величин с $\mathbb{E} \xi_{1}^{2}<\infty$ и $S_{n}=\xi_{1}+\ldots+\xi_{n}$. Тогда
    \begin{equation*}
        \mathbb{P}\left(\frac{S_{n}-\mathbb{E} S_{n}}{\sqrt{\mathbb{D} S_{n}}} \leqslant x\right)
        \xrightarrow[n \to +\infty]{}
        \Phi(x) = \frac{1}{\sqrt{2 \pi}} \int\limits_{-\infty}^{x} e^{-\frac{z^{2}}{2}} dz~ \forall x \in \mathbb{R}
    \end{equation*}
\end{namedthm}
\begin{proof}
Пусть $\mathbb{E} \xi_{1}=m,\, \mathbb{D} \xi_{1}=\sigma^{2}$. Введём $X = \xi_1 - m$ и $\varphi_X(t)=\mathbb{E} e^{i tX}$. Введём также
\begin{equation*}
    \varphi_{n}(t)=\mathbb{E} e^{i t \frac{S_{n}-\mathbb{E} S_{n}}{\sqrt{\mathbb{D} S_{n}}}} = 
    \left[\varphi\left(\frac{t}{\sigma \sqrt{n}}\right)\right]^{n}
\end{equation*}

В силу разложения характеристической функции
\begin{equation*}
    \varphi_{X}(t)=1+i t \mathbb{E} X+\ldots+\frac{(i t)^{n}}{n !} \mathbb{E} X^{n}+R_{n}(t)
\end{equation*}

При $n=2$ получим 
\begin{equation*}
    \varphi(t)=1-\frac{\sigma^{2} t^{2}}{2}+\overline{o}\left(t^{2}\right), \quad t \rightarrow 0
\end{equation*}

Следовательно, для любого $t \in \mathbb{R}$ при $n \to +\infty$
\begin{equation*}
    \varphi_{n}(t)=\left[1-\frac{\sigma^{2} t^{2}}{2 \sigma^{2} n}+\overline{o}\left(\frac{1}{n}\right)\right] \rightarrow e^{-\frac{t^{2}}{2}}
\end{equation*}

Функция $e^{-\frac{t^{2}}{2}}$ является характеристической функцией $\mathbf{N}(0,1)$. В силу теорем о непрерывном соответсвии между функциями распределения и характеристическими функциями центральная предельная теорема доказана.
\end{proof}

\section{Условное математическое ожидание}

Пусть $\xi$ и $\eta$~--- две случайные величины на некотором вероятностном пространстве, причём $E|\xi|<\infty$; $L=L(\eta)$~--- множество, в котором собраны все случайные величины, имеющие вид $\zeta=g(\eta)$, где $g(x)$~--- произвольная борелевская функция. Скалярным произведением двух случайных величин $\varphi$ и $\zeta$ назовём $(\varphi, \zeta)=\mathbb{E}(\varphi \cdot \zeta)$, если это математическое ожидание существует.

Условное математическое ожидание $\mathbb{E}(\xi | \eta)$ случайной величины $\xi$ относительно $\eta$ можно представлять себе как результат ортогонального проектирования случайной величины $\xi$ на пространство $L$.

Результат проектирования — такая случайная величина $\mathbb{E}(\xi | \eta)=\widehat{\xi}$, для которой выполнено основное и единственное свойство ортопроекции: её разность с $\xi$ ортогональна всем элементам $L$. Ортогональность означает, что для любой $g(\eta) \in L$ обращается в нуль (если вообще существует) скалярное произведение $(\xi-\widehat{\xi}, g(\eta))$, т. е.
\begin{equation*}
    \mathbb{E}((\xi-\widehat{\xi}) g(\eta))=0~ \text{или}~ \mathbb{E}(\xi g(\eta))=\mathbb{E}(\widehat{\xi} g(\eta))
\end{equation*}
Это свойство называют {\it тождеством ортопроекции}. Чтобы матожидание существовало всегда, достаточно брать лишь ограниченные функции $g(y)$.

\begin{defn}
    Пусть $\mathbb{E}|\xi|<\infty$, $L=L(\eta)$~--- множество всех борелевских функций от случайной величины $\eta$. Условным математическим ожиданием $\mathbb{E}(\xi | \eta)$ называется случайная величина $\widehat{\xi} \in L$, удовлетворяющая тождеству ортопроекции.
\end{defn}

\begin{namedthm}[Свойства условного математического ожидания]\leavevmode
    \begin{enumerate}
        \item Все свойства математического ожидания, как, например, линейность, однако борелевские функции от случайной величины $\eta$ выносятся из-под знака математического ожидания как постоянные.
        \item Пусть $\mathbb{E} \xi^{2}<\infty$.Тогда расстояние от $\xi$ до её ортопроекции $\widehat{\xi}=\mathbb{E}(\xi | \eta)$ является наименьшим из расстояний от $\xi$ до всех «точек» множества $L$:
        \begin{equation*}
            \min_{g(\eta) \in L} \mathbb{E}(\xi-g(\eta))^{2}=\mathbb{E}(\xi-\widehat{\xi})^{2}
        \end{equation*}
        \item Если $f(\eta) \in L$ такова, что $\mathbb{E}|f(\eta) \cdot \xi|<\infty$, то
        \begin{equation*}
            \mathbb{E}(f(\eta) \cdot \xi | \eta)
            \stackrel{\text{п.н.}}{=}
            f(\eta) \cdot \mathbb{E}(\xi | \eta)
        \end{equation*}
        \begin{proof}
            Рассмотрим только случай, когда $f(\eta)$ ограничена. Проверим, что $\zeta=f(\eta) \cdot \mathbb{E}(\xi | \eta)$ удовлетворяет тождеству ортопроекции: для любой ограниченной $g(\eta) \in L$
            \begin{equation*}
                \mathbb{E}(f(\eta) \xi \cdot g(\eta))=\mathbb{E}(\zeta \cdot g(\eta))
            \end{equation*}
            Обозначим $h(\eta)=f(\eta) g(\eta) \in L$. Эта функция ограничена, поэтому
            \begin{equation*}
                \mathbb{E}(\xi f(\eta) \cdot g(\eta))=\mathbb{E}(\xi h(\eta))=\mathbb{E}(\widehat{\xi} h(\eta))=\mathbb{E}(\zeta \cdot g(\eta))
            \end{equation*}
        \end{proof}
        \item Если $\mathbb{E}|g(\xi, \eta)|<\infty$, то
        \begin{equation*}
            \mathbb{E} g(\xi, \eta)=\mathbb{E}\left[\left.\mathbb{E}(g(\xi, y) | \eta)\right|_{y=\eta}\right]
        \end{equation*}
        \item Если $\xi$ и $\eta$ независимы, то $\mathbb{E}(\xi | \eta)=\mathbb{E} \xi$.
    \end{enumerate}
\end{namedthm}

\subsubsection{Вычисление условного матожидания}
Возьмём функцию $h(y)=\mathbb{E}(\xi | \eta=y)$, для которой $\mathbb{E}(\xi | \eta)=h(\eta)$ и рассмотрим, что такое условное математическое ожидание относительно события $\{\eta=y\}$ для двух случаев: когда обе случайные величины имеют дискретное распределение и когда их совместное распределение абсолютно непрерывно.
\begin{enumerate}
    \item Пусть $\xi$ принимает значения $a_{1}, a_{2}, \ldots$, а $\eta$~--- значения $b_{1}, b_{2}, \ldots$ Тогда $h(\eta)$ может принимать только значения $h\left(b_{1}\right), h\left(b_{2}\right), \ldots$, где
    \begin{equation*}
        h(y)=\sum\limits_{i} a_{i} \mathbb{P}\left(\xi=a_{i} | \eta=y\right)
    \end{equation*}
    Иначе говоря, при каждом фиксированном $y$ начение $h(y)$ определяется как математическое ожидание дискретного распределения со значениями $a_{i}$ и вероятностями $\mathbb{P}\left(\xi=a_{i} | \eta=y\right)$. Такое распределение называется {\it условным распределением случайной величины $\xi$ при условии $\eta = y$}.
    
    \item Во втором случае пусть $f_{\xi, \eta}(x, y)$~--- плотность совместного распределения, $f_{\eta}(y)$~--- плотность распределения величины $\eta$.Тогда положим
    \begin{equation*}
        h(y)=\int\limits_{\mathbb{R}} x \frac{f_{\xi, \eta}(x, y)}{f_{\eta}(y)} d x
    \end{equation*}
    При фиксированном $y$ число $h(y)$ есть математическое ожидание абсолютно непрерывного распределения с плотностью распределения
    \begin{equation*}
        f(x | y)=\frac{f_{\xi, \eta}(x, y)}{f_{\eta}(y)}=\frac{f_{\xi, \eta}(x, y)}{\int\limits_{\mathbb{R}} f_{\xi, \eta}(x, y) d x}
    \end{equation*}
    Такое распределение называется {\it условным распределением величины $\xi$ при условии $\eta = y$}, а функция $f(x | y)$~--- {\it условной плотностью}.
\end{enumerate}

    Убедимся формально (скажем, в абсолютно непрерывном случае), что определённая выше $h(\eta)$ удовлетворяет тождеству ортопроекции и, следовательно, является условным матожиданием $\mathbb{E}(\xi | \eta)$. Для любой $g(\eta) \in L$ (такой, что соответствующее математическое ожидание существует) левая часть тождества ортопроекции равна
    \begin{equation*}
        \mathbb{E}(\xi \cdot g(\eta))=\iint_{\mathbb{R}^{2}} x g(y) f_{\xi, \eta}(x, y) d x d y
    \end{equation*}
    
    Правая часть равна
    \begin{equation*}
        \mathbb{E}(h(\eta) g(\eta))=\int\limits_{\mathbb{R}} h(y) g(y) f_{\eta}(y) d y=\iint_{\mathbb{R} \mathbb{R}} x \frac{f_{\xi, \eta}(x, y)}{f_{\eta}(y)} d x \cdot g(y) f_{\eta}(y) d y
    \end{equation*}
    
    Сокращая $f_{\eta}(y)$, получаем равенство левой и правой частей.

\chapter{Математическая статистика}

\section{Статистическая структура. Выборка. Статистика. Порядковые статистики. Вариационный ряд. Эмпирическая функция распределения}

\begin{defn}
{\it Статистическая структура}~--- совокупность $(\Omega, \mathcal{A}, \mathcal{P})$, где $\Omega$~---множество элементарных исходов, $\mathcal{A}$~--- $\sigma$-алгебра событий, $\mathcal{P}$~--- семейство вероятностных мер, определённых на $\mathcal{A}$, параметризованное одно- или многомерным числовым параметром: $\mathcal{P} = (\mathcal{P}_{\theta}:\theta \in \Theta \subset R^{m})$.
\end{defn}

\begin{defn}
{\it Выборка} $\mathbf{X} = (X_{1}, \ldots, X_{n})$ объёма $n$~--- набор из $n$ независимых и одинаково распределённых случайных величин, имеющих такое же распределение, как и наблюдаемая случайная величина $\xi$.

\end{defn}

До того, как эксперимент проведён, выборка~--- набор случайных величин, после~--- набор чисел из множества возможных значений случайной величины. Числовой набор $\mathbf{X}(\omega_0) = (X_{1}(\omega_0), \ldots, X_{n}(\omega_0)) = (x_1, \ldots, x_n)$~--- {\it реализация выборки} на элементарном исходе $\omega_0$.

\begin{defn}
{\it Статистика}~--- измеримая функция от выборки $T(\mathbf{X})$.
\end{defn}

\begin{defn}
{\it Вариационный ряд}~--- выборка $X_{1}, \ldots, X_{n}$, упорядоченная по возрастанию на каждом элементарном исходе.
\end{defn}
Вариационный ряд строится следующим образом:
\begin{multline*}
    X_{(1)}(\omega)=\min (X_{1}(\omega), \ldots, X_{n}(\omega)); X_{(n)}(\omega)=\max (X_{1}(\omega), \ldots, X_{n}(\omega)); \\
    X_{(k)}(\omega)=\{\forall \omega \in \Omega \Rightarrow \exists m \leqslant i_{1}, \ldots, i_{k-1}, i_{k}, i_{k+1}, \ldots, i_{n} \leqslant n, i_{j} \neq i_{m}~ (j \neq m): \\ 
    X_{(k)}(\omega)=X_{i_{k}}(\omega);
    X_{i_{1}}(\omega), \ldots, X_{i_{k-1}}(\omega) \leqslant X_{i_{k}}(\omega); X_{i_{k+1}}(\omega), \ldots, X_{i_{n}}(\omega)>X_{i_{k}}(\omega)\}, \\
    k = \overline{2, n-1}
\end{multline*}
Элемент $X_{(k)}$~--- {\it $k$-я порядковая статистика}.

\begin{defn}
{\it Эмпирическая функция распределения}, построенная по выборке $X_{1}, \ldots, X_{n}$ объёма $n$~--- случайная функция $F_{n}^{*}$, определяемая следующим образом:
\begin{equation*}
    F_{n}^{*}(y) =\frac{1}{n} \sum\limits_{i=1}^{n} \mathrm{I}\left(X_{i}<y\right) \quad \forall y \in \mathbb{R}
\end{equation*}
\end{defn}

Эмпирическая функция распределенния строится по вариационному ряду следующим образом:

\begin{equation*}
    F_{n}^{*}(y)=\left\{\begin{array}{ll}
    0, & \text { если } y \leqslant X_{(1)} \\
    k/n, & \text { если } X_{(k)}<y \leqslant X_{(k+1)} \\
    1, & \text { если } y>X_{(n)}
    \end{array}\right.
\end{equation*}

\begin{exmp}
Найдём эмпирические функции распределения для крайних порядковых статистик.

\begin{gather*}
    \begin{aligned}
        F_{(1)}(x)=\mathbb{P}(X_{(1)} < x) 
    = 1 - \mathbb{P} (\mathrm{X}_{(1)} \geqslant x) 
    = 1 - \mathbb{P}(x_{1} \geqslant x, \ldots, x_{n} \geqslant x) = \\
    = 1 - \prod_{i=1}^{n} \mathbb{P}(x_{i} \geqslant x) 
    = 1 - (\mathbb{P}({x}_{1} \geqslant x))^{n} 
    = 1 - (1 - F(x))^{n} 
    \end{aligned} \\
    \begin{aligned}
        F_{(n)}(x) 
        = \mathbb{P}(X_{(n)} < x) 
        = \mathbb{P}(x_{1} < x, \ldots, x_{n} < x) = \\
        = \prod_{i=1}^{n} \mathbb{P}(x_{i} < x) 
        = (\mathbb{P}({x}_{1} < x))^{n} 
        = F^{n}(x)
    \end{aligned}
\end{gather*}
\end{exmp}

\begin{namedthm}[Свойства эмпирической функции распределения]\leavevmode
\begin{enumerate}
    \item Пусть $X_{1}, \ldots, X_{n}$~--- выборка из распределения $\mathcal{P}$ с функцией распределения $F$ и пусть $F_{n}^{*}$ — эмпирическая функция распределения, построенная по этой выборке. Тогда $F_{n}^{*}(y) \xrightarrow[n \to \infty]{\mathbb{P}} F(y)$ для любого $y \in \mathbb{R}.$
    \item Для любого y $\in \mathbb{R}$:
    \begin{enumerate}[label={\arabic*)}]
        \item $\mathbb{E} F_{n}^{*}(y)=F(y)$, т.е. $F_{n}^{*}(y)$~--- несмещённая оценка для $F(y)$.
        \item $\mathbb{D} F_{n}^{*}(y)=\cfrac{F(y)(1-F(y))}{n}$
        \item $\sqrt{n}(F_{n}^{*}(y)-F(y)) \Rightarrow \mathbf{N}(0, (1-F(y))F(y))$, т.е. $F_{n}^{*}(y)$~--- асимптотически нормальная оценка для $F(y)$.
        \item $n F_{n}^{*}(y) \sim \mathbf{B}(n, F(y))$
    \end{enumerate}
\end{enumerate}
\end{namedthm}
\pagebreak
\begin{proof}\leavevmode
\begin{enumerate}
    \item $F_{n}^{*}(y)=\frac{1}{n} \sum\limits_{i=1}^{n} \mathrm{I}(X_{i}<y)$, при этом случайные величины $\mathrm{I}(X_{1}<y)$, $\mathrm{I}(X_{2}<y), \ldots$ независимы и одинаково распределены, их математическое ожидание конечно:
    \begin{equation*}
        \mathbb{E}\mathrm{I}(X_{1}<y)=1 \cdot \mathbb{P}(X_{1}<y)+0 \cdot \mathbb{P}(X_{1} \geqslant y)=\mathbb{P}(X_{1}<y)=F(y)<\infty
    \end{equation*}
    Следовательно, применим ЗБЧ в форме Хинчина:
    \begin{equation*}
        F_{n}^{*}(y)=\cfrac{\sum\limits_{i=1}^{n} \mathrm{I}(X_{i}<y)}{n} \xrightarrow[]{\mathbb{P}} \mathbb{E}\mathrm{I}(X_{1}<y)=F(y) 
    \end{equation*}
    \item Заметим:
    \begin{gather*}
        \mathrm{I}(X_{1}<y) \sim  \mathbf{Bi}(F(y)) \Rightarrow \mathbb{E}\mathrm{I}(X_{1}<y) = F(y) \\
        \mathbb{D}\mathrm{I}(X_{1}<y) = F(y)(1-F(y))
    \end{gather*}
    \begin{enumerate}[label={\arabic*)}]
        \item Случайные величины $\mathrm{I}(X_{i}<y)$ одинаково распределены, поэтому:
        \begin{equation*}
            \mathbb{E} F_{n}^{*}(y)=\mathbb{E} \cfrac{\sum\limits_{i=1}^{n} \mathrm{I}(X_{i}<y)}{n}=\cfrac{\sum\limits_{i=1}^{n} \mathbb{E}\mathrm{I}(X_{i}<y)}{n}=\cfrac{n \mathbb{E}\mathrm{I}(X_{1}<y)}{n}=F(y)  
        \end{equation*}
        
        \item Случайные величины $\mathrm{I}(X_{i}<y)$ независимы и одинаково распределены, поэтому:
        \begin{multline*}
            \mathbb{D}\mathrm{I}_{n}^{*}(y)
            = \mathbb{D} \cfrac{\sum\limits_{i=1}^{n} \mathrm{I}(X_{i}<y)}{n}
            = \cfrac{\sum\limits_{i=1}^{n} \mathbb{D}\mathrm{I}(X_{i}<y)}{n^{2}}
            = \\
            = \cfrac{n\mathbb{D}\mathrm{I}(X_{1}<y)}{n^{2}}
            = \cfrac{F(y)(1-F(y))}{n}
        \end{multline*}
        \item Применим ЦПТ:
        \begin{multline*}
            \sqrt{n}\left(F_{n}^{*}(y)-F(y)\right)
            = \sqrt{n}\left(\cfrac{\sum \mathrm{I}(X_{i}<y)}{n}-F(y)\right) 
            = \\
            = \cfrac{\sum\limits_{i=1}^{n} \mathrm{I}(X_{i}<y)-n F(y)}{\sqrt{n}} 
            = \cfrac{\sum\limits_{i=1}^{n} \mathrm{I}(X_{i}<y)-n \mathbb{E}\mathrm{I}(X_{1}<y)}{\sqrt{n}} 
            \Rightarrow \\
            \Rightarrow \mathbf{N}(0, \mathbb{D}\mathrm{I}(X_{1}<y))
            = \mathbf{N}(0, (1-F(y))F(y))
        \end{multline*}
        \item Следует из устойчивости по суммированию биномиального распределения. Поскольку $\mathrm{I}\left(X_{i}<y\right)$ независимы и имеют распределение Бернулли $\mathbf{Bi}(F(y))$, то их сумма
        \begin{equation*}
            n F_{n}^{*}(y)=\mathrm{I}\left(X_{1}<y\right)+\ldots+\mathrm{I}\left(X_{n}<y\right)
        \end{equation*}
        имеет биномиальное распределение $\mathbf{B}(n, F(y))$.
    \end{enumerate}
\end{enumerate}  
\end{proof}

\section{Выборочные моменты. Их свойства}

Рассмотрим случайную величину $\xi^{*}$ с эмпирическим распределением, введём для последнего числовые характеристики.

\begin{defn}
{\it Выборочное математическое ожидание:} 
\begin{equation*}
    \tilde{\mathbb{E}} \xi^{*}=\sum\limits_{i=1}^{n} \frac{1}{n} X_{i}=\frac{1}{n} \sum\limits_{i=1}^{n} X_{i}=\overline{X}
\end{equation*}

Выборочное матожидание функции $g(\xi^{*})$:
\begin{equation*}
    \tilde{\mathbb{E}} g\left(\xi^{*}\right)=\frac{1}{n} \sum\limits_{i=1}^{n} g\left(X_{i}\right)=\overline{g(\mathbf{X})}
\end{equation*}
\end{defn}

\begin{defn}
{\it Выборочная дисперсия:}
\begin{equation*}
    \tilde{\mathbb{D}} \xi^{*}=\sum\limits_{i=1}^{n} \frac{1}{n}(X_{i}-\tilde{\mathbb{E}} \xi^{*})^{2}=\frac{1}{n} \sum\limits_{i=1}^{n}(X_{i}-\overline{X})^{2}=S^{2}
\end{equation*}
\end{defn}

\begin{defn}
{\it Несмещённая выборочная дисперсия:} 
\begin{equation*}
    S_{0}^{2}=\frac{1}{n-1} \sum\limits_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2} = \frac{n}{n-1} S^2
\end{equation*}
\end{defn}

\begin{defn}
{\it Выборочный момент $k$-го порядка:}
\begin{equation*}
    \tilde{\mathbb{E}}(\xi^{*})^{k}=\sum\limits_{i=1}^{n} \frac{1}{n} X_{i}^{k}=\frac{1}{n} \sum\limits_{i=1}^{n} X_{i}^{k}=\overline{X^{k}}
\end{equation*}
\end{defn}

Все вышеперечисленные характеристики являются случайными величинами как функции от выборки $X_{1}, \ldots, X_{n}$ и оценками для истинных моментов искомого распределения.

\begin{thm*}
Выборочное среднее $\overline{X}$ является несмещённой, состоятельной и асимптотически нормальной оценкой для теоретического среднего (математического ожидания):

\begin{enumerate}[label={\arabic*.}]
    \item Если $\mathbb{E}|X_{1}|<\infty$, то $\mathbb{E}\overline{X}=\mathbb{E} X_{1}=a$
    \item Если $\mathbb{E}|X_{1}|<\infty$, то $\overline{X} \xrightarrow[]{\mathbb{P}} \mathbb{E} X_{1}=a$ при $n \rightarrow \infty$.
    \item Если $\mathbb{D} X_{1}<\infty,~ \mathbb{D} X_{1} \neq 0$, то $\sqrt{n}(\overline{X}-\mathbb{E} X_{1}) \Rightarrow \mathbf{N}(0, \mathbb{D} X_{1})$.
\end{enumerate}
\end{thm*}

\begin{proof}
\begin{enumerate}[label={\arabic*.}]
    \item $\mathbb{E} \overline{X}=\frac{1}{n}(\mathbb{E} X_{1}+\ldots+\mathbb{E} X_{n})=\frac{1}{n} \cdot n \mathbb{E} X_{1}=\mathbb{E} X_{1}=a$
    \item Из ЗБЧ в форме Хинчина:
    \begin{equation*}
        \overline{X}
        = \cfrac{X_{1}+\ldots+X_{n}}{n} \xrightarrow[]{\mathbb{P}} \mathbb{E} X_{1} 
        = a
    \end{equation*}

    \item Из ЦПТ:
    \begin{equation*}
        \sqrt{n}\left(\overline{X}-\mathbb{E} X_{1}\right) 
        = \cfrac{\sum\limits_{i=1}^{n} X_{i}-n \mathbb{E} X_{1}}{\sqrt{n}} 
        \Rightarrow \mathbf{N}(0, \mathbb{D} X_{1})
    \end{equation*}
\end{enumerate}
\end{proof}

\begin{rmrk}
    Аналогичными свойствами обладает выборочный $k$-й момент, являющийся несмещённой, состоятельной и асимптотически нормальной оценкой для теоретического $k$-го момента.
\end{rmrk}

\begin{thm*}
Пусть $\mathbb{D} X_{1}<\infty$.
\begin{enumerate}
    \item Выборочные дисперсии $S^{2}$ и $S^{2}_0$ являются состоятельными оценками для истинной дисперсии:
    \begin{equation*}
        S^{2} \xrightarrow[]{\mathbb{P}} \mathbb{D} X_{1}=\sigma^{2}, \quad S_{0}^{2} \xrightarrow[]{\mathbb{P}} \mathbb{D} X_{1}=\sigma^{2}
    \end{equation*}
    \item Величина $S^{2}$~--- смещённая оценка дисперсии, а $S^{2}_0$~--— несмещённая:
    \begin{equation*}
        \mathbb{E} S^{2}=\frac{n-1}{n} \mathbb{D} X_{1}=\frac{n-1}{n} \sigma^{2} \neq \sigma^{2}, \quad \mathbb{E} S_{0}^{2}=\mathbb{D} X_{1}=\sigma^{2}
    \end{equation*}
    
    \item Если $0 \neq \mathbb{D}(X_{1}-\mathbb{E}X_{1})^{2}<\infty$, то $S^{2}$ и $S^{2}_0$ являются асимптотически нормальными оценками истинной дисперсии:
    \begin{equation*}
        \sqrt{n}\left(S^{2}-\mathbb{D} X_{1}\right) \Rightarrow \mathbf{N}(0, \mathbb{D}(X_{1}-\mathbb{E} X_{1})^{2})
    \end{equation*}
\end{enumerate}
\end{thm*}

\begin{proof}
\begin{enumerate}
    \item $S^{2}=\frac{1}{n} \sum\limits_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}=\overline{X^{2}}-(\overline{X})^{2}$

    Используя состоятельность первого и второго выборочных моментов и свойства сходимости по вероятности, получаем:
    \begin{gather*}
        S^{2}=\overline{X^{2}}-(\overline{X})^{2} \xrightarrow[]{\mathbb{P}} \mathbb{E} X_{1}^{2}-\left(\mathbb{E} X_{1}\right)^{2}=\sigma^{2} \\
        \cfrac{n}{n-1} \rightarrow 1 \Rightarrow S_{0}^{2}=\frac{n}{n-1} S^{2} \xrightarrow[]{\mathbb{P}} \sigma^{2}
    \end{gather*}
    
    \item Используя несмещённость первого и второго выборочных моментов:
    \begin{multline*}
        \mathbb{E} S^{2} = \mathbb{E}\left(\overline{X^{2}}-(\overline{X})^{2}\right)
        = \mathbb{E} \overline{X^{2}}-\mathbb{E}(\overline{X})^{2}
        = \mathbb{E} X_{1}^{2}-\mathbb{E}(\overline{X})^{2} = \\
        = \mathbb{E} X_{1}^{2}-\left((\mathbb{E} \overline{X})^{2}+\mathbb{D} \overline{X}\right)
        = \mathbb{E} X_{1}^{2}-\left(\mathbb{E} X_{1}\right)^{2}-\mathbb{D}\left(\frac{1}{n} \sum\limits_{i=1}^{n} X_{i}\right) = \\
        = \sigma^{2}-\frac{1}{n^{2}} n \mathbb{D} X_{1}
        = \sigma^{2}-\frac{\sigma^{2}}{n}
        = \frac{n-1}{n} \sigma^{2}
    \end{multline*}
    
    Откуда следует:
    \begin{equation*}
        \mathbb{E} S_{0}^{2}=\frac{n}{n-1} \mathbb{E} S^{2}=\sigma^{2}
    \end{equation*}
    
    \item Введём случайные величины $Y_{i}=X_{i}-a$; $\mathbb{E}Y_{i} = 0, \mathbb{D} Y_{1}=\mathbb{D} X_{1}=\sigma^{2}$.
    \begin{gather*}
        S^{2}=\frac{1}{n} \sum\limits_{i=1}^{n}(X_{i}-\overline{X})^{2}=\frac{1}{n} \sum\limits_{i=1}^{n}(X_{i}-a-(\overline{X}-a))^{2}=\overline{Y^{2}}-(\overline{Y})^{2} \\
        \begin{aligned}
            \sqrt{n}(S^{2}-\sigma^{2}) = \sqrt{n}(\overline{Y^{2}}-(\overline{Y})^{2}-\sigma^{2})
            = \sqrt{n}t(\overline{Y^{2}}-\mathbb{E} Y_{1}^{2})-\sqrt{n}(\overline{Y})^{2} = \\
            =\frac{\sum\limits_{i=1}^{n} Y_{i}^{2}-n \mathbb{E} Y_{1}^{2}}{\sqrt{n}}-\overline{Y} \cdot \sqrt{n} \overline{Y} \Rightarrow \mathbf{N}(0, \mathbb{D}(X_{1}-a)^{2}),
    \end{aligned}
    \end{gather*}
    поскольку $\cfrac{\sum\limits_{i=1}^{n} Y_{i}^{2}-n \mathbb{E} Y_{1}^{2}}{\sqrt{n}} \Rightarrow \mathbf{N}(0, \mathbb{D} Y_{1}^{2})$ по ЦПТ, а $\overline{Y} \cdot \sqrt{n} \overline{Y} \Rightarrow 0$ как произведение последовательностей $\overline{Y} \xrightarrow[n \rightarrow \infty]{p} 0$ и $\sqrt{n} \overline{Y} \Rightarrow \mathbf{N}(0, \mathbb{D} X_{1})$.
\end{enumerate}
\end{proof}

\section{Точечная оценка. Несмещённость, состоятельность, оптимальность. Теорема о единственности оптимальной оценки}
\begin{defn}
{\it Статистика} или {\it оценка}~--- измеримая функция от выборки $T(\mathbf{X})$.
\end{defn}

\begin{defn}
{\it Несмещённая оценка} парамаетра $\theta$~--- статистика $T(\mathbf{X})$, т.ч. $\forall \theta \in \Theta: \mathbb{E}T(\theta) = \theta$.
\end{defn}

\begin{defn}
{\it Асимптотически несмещённая оценка} параметра $\theta$~--- статистика $T(\mathbf{X})$, т.ч. $\forall \theta \in \Theta: \mathbb{E}T(\theta) \xrightarrow[n \rightarrow \infty]{} \theta$.
\end{defn}

\begin{defn}
{\it Состоятельная оценка} параметра $\theta$~--- статистика $T(\mathbf{X})$, т.ч. $\forall \theta \in \Theta: T(\theta) \xrightarrow[n \rightarrow \infty]{p} \theta$.
\end{defn}

Оценки также могут вводиться и для функций $\tau(\theta)$ параметра $\theta$; они обладают всеми аналогичными свойствами.

Несмещённость означает отсутствие ошибки «в среднем», т. е. при систематическом использовании данной оценки. Несмещённость является желательным, но не обязательным свойством оценок. Достаточно, чтобы смещение оценки (разница между её средним значением и истинным параметром) уменьшалось с ростом объёма выборки. Поэтому асимптотическая несмещённость является весьма желательным свойством оценок. Свойство состоятельности означает, что последовательность оценок приближается к неизвестному параметру при увеличении количества наблюдений. В отсутствие этого свойства оценка совершенно «несостоятельна» как оценка.

\begin{rmrk}
    Отметим некоторые свойства несмещённых и состоятельных оценок.
    \begin{enumerate}
        \item Несмещённые оценки не единственны.
        
        К примеру в качестве несмещённой оценки для математического ожидания $\mathbb{E} X$ могут выступать $\mathbb{E} X_{1}$ или $\mathbb{E} \overline{\mathbf{X}}$.
        
        \item Несмещённые оценки могут не существовать.
        \begin{exmp}
            Дано распределение $\mathbf{Pois}(\theta)$, над которым произведено одно наблюдение. Найти несмещённую оценку для функции $\tau(\theta) = \cfrac{1}{\theta}$.
                \begin{equation*}
                    \mathbb{E}T(\theta) 
                    = \mathlarger{\mathlarger{\sum}}_{x=0}^{\infty}T(x)e^{-\theta}\cfrac{\theta^{x}}{x!} 
                    = \cfrac{1}{\theta}
                    \Rightarrow \mathlarger{\mathlarger{\sum}}_{x=0}^{\infty}T(x)\cfrac{\theta^{x+1}}{x!}
                    = \mathlarger{\mathlarger{\sum}}_{r=0}^{\infty}\cfrac{\theta^{r}}{r!}
                    \Rightarrow T(x) \equiv \cfrac{1}{\theta}
                \end{equation*}
            Т.к. полученная статистика зависит от $\theta$, искомой несмещённой оценки для $\tau(\theta)$ не существует.
        \end{exmp}
        
    \item Несмещённые оценки могут существовать, но быть бессмысленными.
    \begin{exmp}
        Дано отрицательное биноминальное распределение $\mathbf{N}\mathbf{B}(1, \theta)$, над которым произведено одно наблюдение. Найти несмещённую оценку для параметра $\theta$.
        \begin{gather*}
            \mathbb{E}T(\theta) 
            = \sum\limits_{x=0}^{\infty}T(x)\theta^{x} 
            = \cfrac{\theta}{1-\theta} 
            = \sum\limits_{r=1}^{\infty}\theta^{r} \\
            T(x) = 
            \left\{\begin{array}{ll}
                0, & \text { если } x = 0 \\
                1, & \text { если } x \geqslant 1
            \end{array}\right.
        \end{gather*}
    Значения этой статистики не принадлежат параметрическому множеству $\Theta = (0; 1)$, следовательно, эта оценка бессмысленна.
    \end{exmp}
    
    \item Состоятельные оценки не единственны.
    
    К примеру, выборочная дисперсия $S^{2}$ и несмещённая выборочная дисперсия $S_0^{2}$ являются состоятельными оценками теоретической дисперсии.
    
    \item Состоятельные оценки могут быть смещёнными.
    
    Как было показано ранее, выборочная дисперсия является состоятельной, но смещённой оценкой теоретической дисперсии.
    
    \end{enumerate}
\end{rmrk}

Рассмотрим несмещённые оценки $T(\mathbf{X})$ параметра $\theta$, для которых существует дисперсия: $\mathbb{E}(T(\mathbf{X})-\theta)^{2}=\mathbb{D} T(\mathbf{X})$. Тогда сравнивать две разные несмещённые оценки $T_{1}(\mathbf{X})$ и $T_{2}(\mathbf{X})$, для которых $\mathbb{E}T_{1} = \mathbb{E}T_{2} = \theta$, можно по их дисперсиям, однако в таком случае неравенство $\mathbb{D} T_{1}<\mathbb{D} T_{2}$ должно выполняться при любом $\theta$. Введём понятие оптимальной оценки.
\pagebreak
\begin{defn}
{\it Оптимальная оценка} параметра $\theta$~--- статистика $T(\mathbf{X})$, т.ч.:
\begin{enumerate}
    \item $T(\mathbf{X})$~--- несмещённая.
    \item $T(\mathbf{X})$ имеет равномерно минимальную дисперсию, т.е. для любой другой \textbf{несмещённой} оценки $T_{1}(\mathbf{X})$ параметра $\theta$: $\mathbb{D} T(\mathbf{X}) \leqslant \mathbb{E} T_{1}(\mathbf{X})~ \forall X$.
\end{enumerate}
\end{defn}

\begin{thm*}
Если существует оптимальная оценка параметра $\theta$, то она единственна.
\end{thm*}

\begin{proof}
Предположим обратное: пусть существуют две оптимальные оценки $T_1(\mathbf{X})$ и $T_2(\mathbf{X})$ параметра $\theta$. Тогда в силу их несмещённости: $\mathbb{E} T_{1}(\mathbf{X})=\mathbb{E} T_{2}(\mathbf{X})=T(\theta)$, а а в силу того, что они имеют равномерно минимальную дисперсию: $\mathbb{D} T_{1}(\mathbf{X})=\mathbb{D} T_{2}(\mathbf{X})~ \forall \theta$.

Введём новую статистику: 
\begin{equation}
    T_{3}(\mathbf{X})=\cfrac{T_{1}(\mathbf{X})+T_{2}(\mathbf{X})}{2}
\end{equation}

Так как $\mathbb{E} T_{3}(\mathbf{X})=\cfrac{\mathbb{E} T_{1}(\mathbf{X})+\mathbb{E} T_{2}(\mathbf{X})}{2}=\theta$, то $T_{3}(\mathbf{X})$~--- несмещённая оценка параметра $\theta$.

Имеем также:
\begin{equation*}
    \mathbb{D} T_{3}(\mathbf{X})=\cfrac{\mathbb{D}\left(T_{1}(\mathbf{X})+T_{2}(\mathbf{X})\right)}{4} =
    \cfrac{\mathbb{D} T_{1}(\mathbf{X})+\mathbb{D} T_{2}(\mathbf{X})+2 \operatorname{cov}\left(T_{1}(\mathbf{X}) T_{2}(\mathbf{X})\right)}{4}
\end{equation*}

В силу свойства
\begin{equation*}
    \mathbb{E} \xi^{2}<\infty, \mathbb{E} \eta^{2}<\infty \Rightarrow|\operatorname{cov}(\xi, \eta)| = | \mathbb{E}(\xi-\mathbb{E} \xi)(\eta-\mathbb{E} \eta)| \leqslant \sqrt{\mathbb{D} \xi} \sqrt{\mathbb{D} \eta},
\end{equation*}
где равенство достигается тогда и только тогда, когда $\xi=a \eta+b$, получаем:
\begin{equation*}
    \mathbb{D} T_{3}(\mathbf{X}) \leqslant \cfrac{\mathbb{D} T_{1}(\mathbf{X})+\mathbb{D} T_{2}(\mathbf{X})+2 \sqrt{\mathbb{D} T_{1}(\mathbf{X})} \sqrt{\mathbb{D} T_{2}(\mathbf{X})})}{4} =\mathbb{D} T_{1}(\mathbf{X})
\end{equation*}

В силу того, что $T_1(\mathbf{X})$ и $T_2(\mathbf{X})$ — оптимальные, дисперсия $T_3(\mathbf{X})$ не может быть меньше дисперсии $T_1(\mathbf{X})$, следовательно, справедливо равенство, достигаемое при следующих условиях:
\begin{equation*}
\begin{aligned}
    T_{1}(\mathbf{X})=a T_{2}(\mathbf{X})+b \Rightarrow \mathbb{E} T_{1}(\mathbf{X})
    = a \mathbb{E} T_{2}(\mathbf{X})+b 
    \Leftrightarrow \\
    \Leftrightarrow T(\theta) = a T(\theta)+b~ \forall \theta \Rightarrow a = 1, b = 0
\end{aligned}
\end{equation*}

\end{proof}

\section{Функция правдоподобия. Достаточные статистики, полные статистики. Теорема факторизации}

В зависимости от типа распределения $\mathcal{P}_\theta$ обозначим через $f_{\theta}(y)$ одну из следующих функций:
\begin{equation*}
    f_{\theta}(y) =
    \left\{\begin{array}{ll}
    \text { плотность } f_{\theta}(y), & \text { если } \mathcal{P}_{\theta} \text { абсолютно непрерывно, } \\
    P_{\theta}\left(X_{1}=y\right), & \text { если } \mathcal{P}_{\theta} \text { дискретно. }
    \end{array}\right.
\end{equation*}

\begin{defn}
{\it Функция правдоподобия} выборки $\mathbf{X}$:
\begin{equation*}
    L(\mathbf{X} , \theta)=f_{\theta}\left(X_{1}\right) \cdot f_{\theta}\left(X_{2}\right) \cdot \ldots \cdot f_{\theta}\left(X_{n}\right)=\prod_{i=1}^{n} f_{\theta}\left(X_{i}\right)
\end{equation*}
\end{defn}

В дискретном случае функция правдоподобия принимает вид:
\begin{equation*}
\begin{aligned}
    L(\mathbf{x} , \theta)=\prod_{i=1}^{n} f_{\theta}(x_{i}) 
    = \mathbb{P}_{\theta}(X_{1}=x_{1}) \cdot \ldots \cdot \mathbb{P}_{\theta}(X_{n}=x_{n}) = \\
    = \mathbb{P}_{\theta}(X_{1}=x_{1}, \ldots, X_{n}=x_{n})
\end{aligned}
\end{equation*}

Таким образом, смысл функции правдоподобия~--- вероятность* попасть в заданную точку при соответствующем параметре $\theta$ в дискретном случае; для абсолютно непрерывного аналогично~--- вероятность попасть в куб с центром в $x_1, \ldots, x_n$ и сторонами $dx_1, \ldots, dx_n$.

*На самом деле функция правдоподобия не является вероятностной мерой над $\theta$, хотя бы потому что $\int\limits_{\Theta} L(x, \theta) d\theta \neq 1$. Термин "вероятность" применён здесь в переносном смысле.

\begin{defn}
{\it Достаточная статистика}~--- статистика $T(\mathbf{X})$ такая, что $\forall t \in \mathbb{R}^{n},~ \forall B \in \mathfrak{B}(\mathbb{R}^{n})$ условное распределение $\mathbb{P}(X_1, \ldots, X_n \in B | T=t)$ не зависит от параметра $\theta$.
\end{defn}

Иными словами, если значение статистики $T$ известно и фиксировано, то даже знание её распределения больше не даёт никакой информации о параметре; достаточно лишь вычислить $T$ по выборке.

\begin{namedthm}[Критерий факторизации]
$T(\mathbf{X})$~--- достаточная статистика $\Leftrightarrow$ её функция правдоподобия представима в виде 
\begin{equation*}
    L(\mathbf{X}_{1}, \ldots, X_{n} , \theta) \stackrel{\text{п.н.}}{=} h(\mathbf{X}) \cdot \Psi(T, \theta)
\end{equation*}
\end{namedthm}

\begin{proof}
Рассмотрим только дискретный случай. Пусть $T(\mathbf{X})$~--- достаточная статистика. Если $T(\mathbf{X})=t$, то событие $\{\mathbf{X}=\mathbf{x}\} \subseteq \{T(\mathbf{X})=t\}$. Поэтому
\begin{multline*}
    L(\mathbf{x}, \theta) = \mathbb{P}_{\theta}(\mathbf{X}=\mathbf{x})=\mathbb{P}_{\theta}(\mathbf{X}=\mathbf{x} | T(\mathbf{X})=t) =\\
    = \underbrace{\mathbb{P}_{\theta}(T(\mathbf{X})=t)}_{g(T(\mathbf{x}), \theta)} \underbrace{\mathbb{P}_{\theta}(\mathbf{X}=\mathbf{x} | T(\mathbf{X})=t)}_{h(\mathbf{x}, t)}
\end{multline*}

Пусть теперь функция правдоподобия имеет вид $L(\mathbf{x}, \theta)=g(T(\mathbf{x}, \theta) h(\mathbf{x})$. Тогда, если $x$ таково, что $T(\mathbf{x})=t$, то:
\begin{multline*}
    \mathbb{P}(\mathbf{X}=\mathbf{x} | T(\mathbf{X})=t) =\frac{\mathbb{P}(\mathbf{X}=\mathbf{x}, T(\mathbf{X})=t)}{\mathbb{P}(T(\mathbf{X})=t)}
    =\frac{\mathbb{P}(\mathbf{X}=\mathbf{x})}{\sum\limits_{\mathbf{x}^{\prime}: T(\mathbf{x}^{\prime})=t} \mathbb{P}(\mathbf{X}=\mathbf{x}^{\prime})} = \\
    = \frac{g(t, \theta) h(\mathbf{x})}{\sum\limits_{\mathbf{x}^{\prime}: T(\mathbf{x}^{\prime})=t} g(t, \theta) h(\mathbf{x}^{\prime})}
    = \frac{h(\mathbf{x})}{\sum\limits_{\mathbf{x}^{\prime}: T(\mathbf{x}^{\prime})=t} h(\mathbf{x}^{\prime})}
\end{multline*}
Т.е. вероятность не зависит от $\theta$, а значит, $T(\mathbf{X})$ достаточная.
\end{proof}

\begin{defn}
{\it Полная статистика}~--- статистика $T(\mathbf{X})$, для которой справедлива импликация
\begin{equation*}
    \mathbb{E} \varphi(T)=0~\forall \theta \in \Theta \quad \Rightarrow \quad \varphi(T) \stackrel{\text{п.н.}}{=}0
\end{equation*}
\end{defn}

\section{Неравенство Рао—Крамера. Эффективные оценки}

Пусть $X_1, \ldots, X_n$  —  некоторая выборка с функцией правдоподобия $L(\mathbf{X}, \theta)$ относительно некоторой меры $\mu$. Введём функцию ${\varphi(\theta)=\int\limits_{\mathbf{R}^{n}} T(x) L(x, \theta) \mu(d x)<\infty}$, в дальнейшем считая, что она дифференцируема необходимое число раз.

\begin{defn}
Функция правдоподобия $L(\mathbf{X}, \theta)$ {\it удовлетворяет условиям регулярности для $m$-й производной}, если существует
\begin{equation*}
    \cfrac{d^{m} \varphi(\theta)}{d \theta^{m}}=\int\limits_{\mathbb{R}^{n}} T(x) \cfrac{\partial^{m} L(x, \theta)}{\partial \theta^{m}} \mu(d x),
\end{equation*}
причём множество $\left\{ {x:L(x,\theta) > 0} \right\}$ не зависит от параметра $\theta$.
\end{defn}

\begin{namedthm}[Неравенство Рао-Крамера]
Пусть $X_1, \ldots, X_n$ — выборка, $L(\mathbf{X}, \theta)$ удовлетворяет условиям регулярности для первой производной и $\theta$  —  дифференцируемая функция $\theta$. Тогда:
\begin{enumerate}
    \item Для любой $~T(\mathbf{X})$,~--- несмещённой оценки параметра $\theta$, справедливо неравенство:
    \begin{gather*}
        \mathbb{D} T(\mathbf{X}) \geqslant \cfrac{(\theta^{*})^{2}}{\mathbb{E} U^{2}(X, \theta)}~\forall \theta \in \Theta, \\
        \text{где}~ U(X, \theta)=\cfrac{\partial \ln L(\mathbf{X}, \theta)}{\partial \theta}~\text{(функция вклада)}
    \end{gather*}
    
    \item Равенство достигается $\Leftrightarrow \exists~ a_n(\theta):~ T(\mathbf{X})-\theta=a_{n}(\theta) \cdot U(X, \theta)$
\end{enumerate}
\end{namedthm}

\begin{proof}
$\int L(x, \theta) \mu(d x)=1 \Rightarrow \int \cfrac{\partial L(x, \theta)}{\partial \theta} \mu(d x)=0$

Из условий регулярности $L(\mathbf{X}, \theta)$ для следует:
\begin{equation*}
    \int T(x) L(x, \theta) \mu(d x)=\mathbb{E} T(\mathbf{X})=T(\theta) \Rightarrow \int T(x) \cfrac{\partial L(x, \theta)}{\partial \theta} \mu(d x)=(\theta)^{*}
\end{equation*}

Заметим, что
\begin{equation*}
    \cfrac{\partial L(x, \theta)}{\partial \theta}=\cfrac{\partial \ln L(x, \theta)}{\partial \theta} \cdot L(x, \theta)
\end{equation*}

Откуда следует:
\begin{gather*}
    \int U(x, \theta) L(x, \theta) \mu(d x)=0 \Leftrightarrow \mathbb{E} U(X, \theta)=0 \\
\int T(x) U(x, \theta) L(x, \theta) \mu(d x)=\theta^{*} \Leftrightarrow \mathbb{E} T(\mathbf{X}) U(X, \theta)=\theta^{*}
\end{gather*}

Вычитая из первого равенства, помноженного на $\theta$, второе, получаем:
\begin{equation*}
    \mathbb{E}(T(\mathbf{X})-T(\theta)) U(X, \theta)=\theta^{*}
\end{equation*}

В левой части полученного равенства стоит ковариация случайных величин $T(\mathbf{X})$ и $U(X,\theta)$:
\begin{equation*}
    \operatorname{cov}_{\theta}(T(\mathbf{X}), U(X, \theta))=\theta^{*}
\end{equation*}

Из неравенства Коши-Буняковского:
\begin{equation*}
    \left(\theta^{*}\right)^{2}=\operatorname{cov}_{\theta}^{2}(T(\mathbf{X}) U(X, \theta)) \leqslant \mathbb{D} T(\mathbf{X}) \mathbb{D} U(X, \theta)=\mathbb{D} T(\mathbf{X}) \mathbb{E} U^{2}(X, \theta)
\end{equation*}

...что равносильно п.1 теоремы:
\begin{equation*}
    \mathbb{D} T(\mathbf{X}) \geqslant \cfrac{\left[\theta^{*}\right]^{2}}{\mathbb{E} U^{2}(X, \theta)}
\end{equation*}

Неравенство достигается, если линейно связаны:
\begin{equation*}
    T(\mathbf{X})=\varphi(\theta) U(X, \theta)+\psi(\theta) \Rightarrow T(\theta)=\psi(\theta) \Rightarrow a_{n}(\theta)=\varphi(\theta)
\end{equation*}

\end{proof}

Рассмотрим некоторый класс оценок $K=\left\{\hat{\theta}\left(\mathbf{X}\right)\right\}$ параметра $\theta$.
\begin{defn}
    Говорят, что оценка $\theta^{*}\left(\mathbf{X}\right) \in K$ является эффективной оценкой параметра $\theta$ в классе $K$, если для любой другой оценки $\hat{\theta} \in K$ имеет место неравенство:
    \begin{equation*}
        E\left(\theta^{*}-\theta\right)^{2} \leqslant \mathbb{E}(\hat{\theta}-\theta)^{2}~ \forall \theta \in \Theta
    \end{equation*}
\end{defn}
Обозначим класс несмещённых оценок:
\begin{equation*}
    K_{0}=\left\{\hat{\theta}\left(\mathbf{X}\right): E \hat{\theta}=\theta, \forall \theta \in \Theta\right\}
\end{equation*}
Оценка, эффективная в $K_0$ называется просто {\it эффективной}.

Для оценки $\theta^{*} \in K_{0}$ по определению дисперсии
\begin{equation*}
    \mathbb{E}\left(\theta^{*}-\theta\right)^{2}=\mathbb{E}\left(\theta^{*}-\mathbb{E} \theta^{*}\right)^{2}=\mathbb{D} \theta^{*}
\end{equation*}

\begin{rmrk}
Если в неравенстве Рао---Крамера достигается равенство, то полученная оценка~--- эффективная.

Если существует эффективная оценка для функции $\tau(\theta)$, то ни для какой другой функции от $\theta$, кроме линейного преобразования $\tau(\theta)$, эффективной оценки существовать не будет. 
\end{rmrk}

\section{Теорема Рао—Блекуэлла—Колмогорова. Оптимальность оценок являющихся функцией полной достаточной статистики}

\begin{namedthm}[Теорема Рао—Блекуэлла—Колмогорова] Если оптимальная оценка параметра $\theta$ существует, то она является функцией от достаточной статистики.
\end{namedthm}

\begin{proof}
В доказательстве используются следующие свойства условного матожидания: 
\begin{gather*}
    \mathbb{E} f(x, z)=\mathbb{E}(\mathbb{E}(f(x, z) | z)) \\
    \mathbb{E}(g(z) | z)=g(z)
\end{gather*}

\begin{enumerate}
    \item Пусть $T(\mathbf{X})$~--- достаточная статистика, $T_1(\mathbf{X})$~--- несмещённая оценка параметра $\theta$, т.е. $\mathbb{E} T_{1}(\mathbf{X})=\theta$. Рассмотрим функцию $H(T)=\mathbb{E}\left(T_{1} | T\right)$. Тогда из первого свойства следует:
    \begin{equation*}
        \mathbb{E} H(T)=\mathbb{E}\left(\mathbb{E}\left(T_{1} |     T\right)\right)=\mathbb{E} T_{1}=\theta \Rightarrow H(T)     \text{~--- несмещённая оценка~} \theta
    \end{equation*}

    \item Докажем равномерную минимальность её дисперсии:
    \begin{multline*}
        \mathbb{E}((T_{1}-H(T))(H(T)-\theta)
        = \mathbb{E}(\mathbb{E}((T_{1}-H(T))(H(T)-\theta) | T)) 
        = \\
        = \mathbb{E}((H(T)-H(T))(H(T)-\theta))
        = 0
    \end{multline*}

    Тогда из свойств условного матожидания
    \begin{multline*}
        \mathbb{D}\left(T_{1}\right) 
        = \mathbb{E}\left(T_{1}-\theta\right)^{2}=\mathbb{E}\left(T_{1}-H(T)+H(T)-\theta\right)^{2} =\\
        = \mathbb{E}\left(T_{1}-H(T)\right)^{2}+\mathbb{D}(H(T)) \geqslant \mathbb{D}(H(T))
    \end{multline*}
\end{enumerate}
Таким образом, $H(T)$~--- оптимальная оценка $\theta$.

\end{proof}

\begin{namedthm}[Теорема Колмогорова]
Если $T(\mathbf{X})$~--- полная достаточная статистика, то она является оптимальной оценкой своего математического ожидания.
\end{namedthm}

\begin{proof}
Докажем, что $T(\mathbf{X})$ является единственной несмещённой оценкой для $\mathbb{E}T(\mathbf{X})$. Тогда $T(\mathbf{X})$ будет оптимальной оценкой. Предположим, что $T_1(\mathbf{X})$~--- оптимальная оценка для $\mathbb{E}T(\mathbf{X})$. Из теоремы Рао-Блекуэлла-Колмогорова получаем, что $T_{1}=H(T)$ и $\mathbb{E} T_{1}=\mathbb{E} T$. Тогда:

\begin{equation*}
    \mathbb{E} \underbrace{(T(\mathbf{X})-H(T(\mathbf{X})))}_{\varphi(T)}=0
\end{equation*}

Из условия полноты $T(\mathbf{X})$ следует, что $\varphi(T)=0$ с вероятностью 1, т.е. $T=H(T)$ с вероятностью 1.
\end{proof}

\section{Метод моментов. Свойства оценок, полученных методом моментов}

Пусть $X_1, \ldots, X_n$~--- выборка объёма $n$ из параметрического семейства распределений $\mathcal{P}_\theta$. Выберем функцию $g(y): \mathbb{R} \rightarrow \mathbb{R}$ так, чтобы существовал момент $\mathbb{E} g\left(X_{1}\right)=h(\theta)$ и функция $h(\theta)$ была обратима на $\Theta$. Разрешим полученное уравнение относительно $\theta$, а затем вместо истинного момента возьмём выборочный:

\begin{equation*}
    \theta=h^{-1}\left(\mathbb{E} g\left(X_{1}\right)\right), \quad \theta^{*}=h^{-1}(\overline{g(\mathbf{X})})=h^{-1}\left(\frac{1}{n} \sum\limits_{i=1}^{n} g\left(X_{i}\right)\right)
\end{equation*}

Полученная оценка $\theta^{*}$~--- {\it оценка метода моментов} для параметра $\theta$. Чаще всего берут $g(y)=y^{k}$. В этом случае, при условии обратимости функции $h$ на $\Omega$:
\begin{equation*}
    \mathbb{E} X_{1}^{k}=h(\theta), \quad \theta=h^{-1}\left(\mathbb{E} X_{1}^{k}\right), \quad \theta^{*}=h^{-1}(\overline{X^{k}})=h^{-1}\left(\frac{1}{n} \sum\limits_{i=1}^{n} X_{i}^{k}\right)
\end{equation*}

\begin{thm*}
Пусть $\theta^{*}=h^{-1}(\overline{g(\mathbf{X})})$~--- оценка параметра $\theta$, полученная методом моментов, причём функция $h^{-1}$ непрерывна. Тогда оценка $\theta^{*}$ состоятельна.
\end{thm*}

\begin{proof}
По ЗБЧ Хинчина имеем:

\begin{equation*}
    \overline{g(\mathbf{X})}=\frac{1}{n} \sum\limits_{i=1}^{n} g\left(X_{i}\right) \xrightarrow[]{\mathbb{P}} \mathbb{E} g\left(X_{1}\right)=h(\theta)
\end{equation*}

Ввиду непрерывности функции $h^{-1}$:

\begin{equation*}
    \theta^{*}=h^{-1}(\overline{g(\mathbf{X})}) \xrightarrow[]{\mathbb{P}} h^{-1}\left(\mathbb{E} g\left(X_{1}\right)\right)=h^{-1}(h(\theta))=\theta
\end{equation*}
\end{proof}

\begin{defn}
{\it Асимптотически нормальная оценка} параметра $\theta$ с коэффициентом $\sigma^{2}(\theta)$~--- оценка $\theta^{*}$, т.ч. при $n \rightarrow \infty$ имеет место слабая сходимость к стандартному нормальному распределению: $\sqrt{n}(\theta^{*}-\theta) \Rightarrow \mathbf{N}(0, \sigma^{2}(\theta))$.
\end{defn}

\begin{lem}
Пусть функция $g(y)$ такова, что $0 \neq \mathbb{D} g\left(X_{1}\right)<\infty$. Тогда статистика $\overline{g(\mathbf{X})}$ является асимптотически нормальной оценкой для $\mathbb{E} g\left(X_{1}\right)$ с коэффициентом $\sigma^{2}(\theta)=\mathbb{D} g\left(X_{1}\right)$:

\begin{equation*}
    \sqrt{n} \cfrac{\overline{g(\mathbf{X})}-\mathbb{E} g\left(X_{1}\right)}{\sqrt{\mathbb{D} g\left(X_{1}\right)}} \Rightarrow \mathbf{N}(0;1)
\end{equation*}
\end{lem}

\begin{proof}
Следует непосредственно из ЦПТ.
\end{proof}

\begin{rmrk}
Следующая теорема утверждает асимптотическую нормальность оценок вида

\begin{equation*}
    \theta^{*}=H(\overline{g(\mathbf{X})})=H\left(\cfrac{g\left(X_{1}\right)+\ldots+g\left(X_{n}\right)}{n}\right)
\end{equation*}

которые обычно получаются при использовании метода моментов, при этом всегда $\theta=H\left(\mathbb{E} g\left(X_{1}\right)\right)$.
\end{rmrk}

\begin{thm*}
Пусть функция $g(y)$ такова, что $0 \neq \mathbb{D} g\left(X_{1}\right)<\infty$, функция $H(y)$ дифференцируема в точке $a=\mathbb{E} g\left(X_{1}\right)$ и её производная в этой точке $H^{\prime}(a)=\left.H^{\prime}(y)\right|_{y=a}$ отлична от нуля. Тогда оценка $\theta^{*}=H(\overline{g(\mathbf{X})})$
является асимптотически нормальной
оценкой для параметра $\theta=H\left(\mathbb{E} g\left(X_{1}\right)\right)=H(a)$ с коэффициентом асимптотической нормальности $\sigma^{2}(\theta)=\left(H^{\prime}(a)\right)^{2} \cdot D g\left(X_{1}\right)$.
\end{thm*}

\begin{proof}
Согласно ЗБЧ последовательность $\overline{g(\mathbf{X})}$ стремится к $a=\mathbb{E} g\left(X_{1}\right)$ по вероятности с ростом $n$: Функция

\begin{equation*}
    G(y)=\left\{\begin{array}{ll}
    \cfrac{H(y)-H(a)}{y-a}, & y \neq a \\
    H^{\prime}(a), & y=a
    \end{array}\right.  
\end{equation*}

по условию непрерывна в точке $a$: Поскольку сходимость по веро-
ятности сохраняется под действием непрерывной функции, получим,
что $G(\overline{g(\mathbf{X})}) \xrightarrow[]{\mathbb{P}} G(a)=H^{\prime}(a)$.

Заметим также, что по вышеприведённой лемме величина $\sqrt{n}(\overline{g(\mathbf{X})}-a)$ слабо сходится
к нормальному распределению $\mathbf{N}(0, \mathbb{D} g(X_{1}))$: Пусть $\xi$~--- случайная величина
из этого распределения. Тогда

\begin{equation*}
    \sqrt{n}(H(\overline{g(\mathbf{X})})-H(a))=\sqrt{n}(\overline{g(\mathbf{X})}-a) \cdot G(\overline{g(\mathbf{X})}) \Rightarrow \xi \cdot H^{\prime}(a)
\end{equation*}

Мы использовали следующее свойство слабой сходимости: если $\xi_{n} \Rightarrow \xi$ и $\eta_{n} \xrightarrow[]{\mathbb{P}} c=\mathrm{const}$, то $\xi_{n} \eta_{n} \Rightarrow c \xi$. Но распределение случайной величины $\xi \cdot H^{\prime}(a)$ есть $\mathbf{N}(0,(H^{\prime}(a))^{2} \cdot \mathbb{D} g(X_{1}))$, откуда следует

\begin{equation*}
    \sigma^{2}(\theta)=\left(H^{\prime}(a)\right)^{2} \cdot \mathbb{D} g\left(X_{1}\right)
\end{equation*}

\end{proof}

\section{Метод максимального правдоподобия. Свойства оценок максимального правдоподобия}

\begin{defn}
{\it Оценка максимального правдоподобия $\hat{\theta}$ параметра $\theta$}~--- точка параметрического множества $\Theta$, в которой функция правдоподобия $L(\mathbf{X},\theta)$ при заданном $X$ достигает максимума, т.е.:
\begin{equation*}
    L(\boldsymbol{x}, \hat{\theta})=\sup\limits_{\theta \in \Theta} L(\boldsymbol{x}, \theta)
\end{equation*}
\end{defn}

\begin{rmrk}
Поскольку функция $\operatorname{ln}y$ монотонна, то точки максимума функций $L(\mathbf{X},\theta)$ и $ln L(\mathbf{X},\theta)$ совпадают.
\end{rmrk}

Если для каждого $X$ максимум функции правдоподобия достигается во внутренней точке $\Theta$, и $L(\mathbf{X},\theta)$ дифференцируема по $\theta$, то оценка максимального правдоподобия $\hat{\theta}$ удовлетворяет уравнению:

\begin{equation*}
    \cfrac{\partial \ln L(\mathbf{x}, \theta)}{\partial \theta}=0
\end{equation*}

Если $\theta$~--- векторный параметр: $\theta=\left(\theta_{1}, \ldots, \theta_{n}\right)$, то это уравнение заменяется системой уравнений:

\begin{equation*}
    \cfrac{\partial \ln L(\mathbf{x}, \theta)}{\partial \theta_{i}}=0,~ i=\overline{1, n} 
\end{equation*}


\begin{thm*}
Если существует эффективная оценка $T(\mathbf{X})$ скалярного параметра $\theta$, то она совпадает с оценкой максимального правдоподобия.
\end{thm*}

\begin{proof}
Если оценка $T(\mathbf{X})$ скалярного параметра $\theta$ эффективна, то в неравенстве Рао-Крамера достигается равенство:

\begin{equation*}
    U(X,\theta) = \cfrac{\partial \ln L(\mathbf{x}, \theta)}{\partial \theta}=\cfrac{T(\mathbf{X})-\theta}{a_n(\theta)}
\end{equation*}

\end{proof}

\begin{thm*}
Если $T(\mathbf{X})$ достаточная статистика, а оценка максимального правдоподобия $\hat{\theta}$ существует и единственна, то она является функцией от $T(\mathbf{X})$.
\end{thm*}

\begin{proof}
Из критерия факторизации следует, что если $T=T(\mathbf{X})$ достаточная статистика, то имеет место представление:

\begin{equation*}
    L(\mathbf{X}, \theta)=g(T(\mathbf{X}), \theta) h(\mathbf{X})
\end{equation*}

Таким образом, максимизация $L(\mathbf{X},\theta)$ сводится к максимизации $g(T(\mathbf{X}), \theta)$ по $\theta$, Следовательно $\hat{\theta}$ есть функция от $T(\mathbf{X})$.
\end{proof}

\begin{defn}
    {\it Асимптотически эффективная оценка}~---
\end{defn}

\begin{thm*}
    Пусть выполнены следующие условия:
    \begin{enumerate}
        \item Функция правдоподобия $L(\mathbf{X}, \theta)$ удовлетворяет условиям регулярности для первых двух производных;
        \item $\exists!~ \theta^{*}$~--- оценка максимального правдоподобия для всех $\theta$, которая достигается во внутренней точке $\Theta$.
    \end{enumerate}
    Тогда оценка $\theta^{*}$:
    \begin{enumerate}
        \item асимптотически несмещена
        \item состоятельна
        \item асимптотически эффективна
        \item асимптотически нормальна
    \end{enumerate}
\end{thm*}

\section{Интервальное оценивание. Методы центральной статистики и использования точечной оценки}

\begin{defn}
{\it Доверительный интервал} для параметра $\theta$ с коэффициентом доверия $0 \leqslant \alpha \leqslant 1$~--- интервал $(T_1(\mathbf{X}), T_2(\mathbf{X}))$, т.ч. $\mathbb{P}_{\theta}(T_1(\mathbf{X}) < \theta < T_2(\mathbf{X})) \geqslant \alpha$.
\end{defn}

\begin{exmp}
Пусть $X_1, \ldots, X_n$~--- выборка из $\mathbf{N}(\theta, 1)$. Тогда

\begin{equation*}
    \theta^{*}
    = \overline{X}
    = \frac{1}{n} \sum\limits_{i=1}^{n} X_{i} \sim \mathbf{N}\left(\theta, \frac{1}{n}\right)
    \Rightarrow (\overline{X}-\theta) \sqrt{n} \sim \mathbf{N}(0;1)
\end{equation*}

Для величины, имеющей стандартное нормальное распределение, строим доверительный интервал, т.е. находим такое $t_{\alpha / 2}$, что 

\begin{equation*}
    \mathbb{P}_{\theta}\left(|(\overline{X}-\theta) \sqrt{n}|<t_{\alpha / 2}\right)=\alpha
\end{equation*}

Решаем уравнение относительно $\theta$ и получаем
\begin{equation*}
    \mathbb{P}_{\theta}\left(\overline{X}-\cfrac{t_{\alpha / 2}}{\sqrt{n}}<\theta<\overline{X}+\cfrac{t_{\alpha / 2}}{\sqrt{n}}\right)=\alpha 
\end{equation*}

\end{exmp}

\begin{defn}
{\it Центральная статистика}~--- функция $G(X,\theta)$, т.ч.:
\begin{enumerate}
    \item $G(X,\theta)$ непрерывна и строго монотонна по $\theta$ при любом фиксированном $X$.
    \item $\mathbb{P}_{\theta}(G(X, \theta)<t)=F(t)$ непрерывна и не зависит от $\theta$.
\end{enumerate}
\end{defn}

\begin{rmrk}
Формально определённая выше величина не является статистикой, т.к. зависит от неизвестного параметра $\theta$.
\end{rmrk}

Построение доверительного интервала с помощью центральной статистики:
\begin{enumerate}
    \item Зафиксируем $\alpha_{1}, \alpha_{2} \in \mathbf{R}$, т.ч.
    \begin{equation*}
        \mathbb{P}_{\theta}(\alpha_{1} \leqslant G(X, \theta) \leqslant \alpha_{2})=\alpha~\forall \theta \Leftrightarrow F_{G}(\alpha_{2})-F_{G}(\alpha_{1})=\alpha
    \end{equation*}
    \item Пусть $G(X,\theta)$ возрастает. Из условий
    \begin{equation*}
        \left\{\begin{array}{l}
        G(X, \theta) \leqslant \alpha_{2} \\
        G(X, \theta) \geqslant \alpha_{1}
        \end{array}\right.
    \end{equation*}
    находятся статистики
    \begin{equation*}
        \left\{\begin{array}{l}
            T_{2}(\mathbf{X}): G(X, T_{2}(\mathbf{X}))=\alpha_{2} \\ 
            T_{1}(\mathbf{X}): G(X, T_{1}(\mathbf{X}))=\alpha_{1}
        \end{array} 
        \Leftrightarrow T_{1}(\mathbf{X}) \leqslant \theta \leqslant T_{2}(\mathbf{X})\right.
    \end{equation*}
    откуда $\mathbb{P}_{\theta}\left(T_{1}(\mathbf{X}) \leqslant \theta \leqslant T_{2}(\mathbf{X})\right) \geqslant \alpha~ \forall \theta$.
\end{enumerate}

\begin{defn}
{\it Центральный доверительный предел} для параметра $\theta$ с коэффициентом доверия $0 \leqslant \alpha \leqslant 1$~--- интервал $(T_1(\mathbf{X}), T_2(\mathbf{X}))$, т.ч. 
\begin{gather*}
    \mathbb{P}_{\theta}\left(T_{1}(\mathbf{X})>\theta\right)=\cfrac{1-\alpha}{2} \\
    \mathbb{P}_{\theta}\left(T_{2}(\mathbf{X})<\theta\right)=\cfrac{1-\alpha}{2}
\end{gather*}
\end{defn}

Построение доверительного интервала с помощью точечной оценки:

\begin{enumerate}
    \item Пусть $T(\mathbf{X})$~--- точечная оценка $\theta$. Обозначим $H(t, \theta)=\mathbb{P}_{\theta}(T(\mathbf{X})<t)$. $H(t,\theta)$~--- непрерывная и строго монотонная функция $\theta$ при любом фиксированном $t$. В этом случае
    \begin{equation*}
        \left\{\begin{array}{l}
            \mathbb{P}_{\theta}\left(T(\mathbf{X})>a_{1}(\theta)\right)
            = \cfrac{1-\alpha}{2} \\ 
            \mathbb{P}_{\theta}\left(T(\mathbf{X})<\alpha_{2}(\theta)\right)
            = \cfrac{1-\alpha}{2}
        \end{array}\right. 
        \Leftrightarrow 
        \left\{\begin{array}{l}
            1 - H(\alpha_{1}(\theta), \theta)=\cfrac{1-\alpha}{2} \\ 
            H(\alpha_{2}(\theta), \theta)=\cfrac{1-\alpha}{2}
        \end{array}\right.
    \end{equation*}
    
    \item Рассмотрим вспомогательную лемму.
    \begin{lem}
        Если $H(t, \theta)$ возрастает по $\theta$, то $\alpha_{1}(\theta)$ и $\alpha_{2}(\theta)$ убывают. Если же $H(t, \theta)$ убывает по $\theta$, то $\alpha_{1}(\theta)$ и $\alpha_{2}(\theta)$ возрастают.
    \end{lem}
    \begin{proof}
        Пусть $H(t, \theta)$ возрастает. Предположим, что $\theta_{1}<\theta_{2} \Rightarrow \alpha_{2}\left(\theta_{1}\right) \leqslant \alpha_{2}\left(\theta_{2}\right)$ и рассмотрим $a_{2}(\theta)$, учитывая, что $H(t, \theta)$, как и всякая функция распределения, неубывает по первому аргументу:
        \begin{equation*}
            \frac{1-a}{2} 
            = H\left(\alpha_{2}\left(\theta_{1}\right) \theta_{1}\right)
            < H\left(\alpha_{2}\left(\theta_{1}\right) \theta_{2}\right) 
            \leqslant H\left(\alpha_{2}\left(\theta_{2}\right) \theta_{2}\right)
            = \frac{1-\alpha}{2}
        \end{equation*}
        Полученное противоречие завершает доказательство.
    \end{proof}
    \item Из леммы следует, что для любого $\theta$
    \begin{equation*}
    \begin{aligned}
        \alpha_{1}(\theta) 
        < T(\mathbf{X})
        \Leftrightarrow \theta>\varphi_{1}(T(\mathbf{X}))
        \Rightarrow \mathbb{P}_{\theta}(\theta>\varphi_{1}(T(\mathbf{X})))
        = \cfrac{1-\alpha}{2} \\
        \alpha_{2}(\theta)>T(\mathbf{X}) 
        \Leftrightarrow \theta<\varphi_{2}(T(\mathbf{X})) \Rightarrow \mathbb{P}_{\theta}(\theta<\varphi_{2}(T(\mathbf{X})))
        = \cfrac{1-\alpha}{2} \\
        \Rightarrow P_{\theta}(\underbrace{\varphi_{2}(T(\mathbf{X}))}_{T_{1}(\mathbf{X})} 
        \leqslant \theta 
        \leqslant \underbrace{\varphi_{1}(T(\mathbf{X}))}_{T_{2}(\mathbf{X})})
        = \alpha
    \end{aligned}
    \end{equation*}

\end{enumerate}

\section{Проверка гипотез. Лемма Неймана—Пирсона}

\begin{defn}
{\it Гипотеза $H$}~--- любое предположение о распределении наблюдаемой случайной величины: $H=\left\{\mathcal{P}=\mathcal{P}_{1}\right\}$ или $H=\{\mathcal{P} \in \mathbb{F}\}$, где $\mathbb{F}$~--- некоторое подмножество в множестве всех распределений. Гипотеза называется {\it простой} в первом случае, {\it сложной} во втором. Если гипотез всего две, то одну из них принято называть {\it основной}, а другую~--- {\itальтернативой}.
\end{defn}

\begin{rmrk} Типичные задачи проверки гипотез:
\begin{enumerate}
    \item Гипотезы о виде распределения;
    \item Гипотезы о проверке однородности выборки: дано несколько выборок; основная гипотеза состоит в том, что эти выборки извлечены из одного распределения;
    \item Гипотеза независимости: по выборке $(X_1,Y_1), \ldots, (X_n,Y_n)$ из $n$ независимых наблюдений пары случайных величин проверяется гипотеза $H_{1}=\left\{X_{i} \text { и } Y_{i} \text { независимы }\right\}$ при альтернативе $H_{1}=\left\{H_{1} \text { неверна }\right\}$. Обе гипотезы являются сложными;
    \item Гипотеза случайности: в эксперименте наблюдаются $n$ случайных величин $X_{1}, \ldots, X_{n}$ и проверяется сложная гипотеза $H_{1}=\left\{X_{1}, \ldots, X_{n}~ \text{независимы и одинаково распределены}\right\}$
\end{enumerate}
\end{rmrk}

Пусть дана выборка $X_{1}, \ldots, X_{n}$, относительно распределения которой выдвинуты две простые гипотезы $H_{0}$ и $H_1$.
\begin{defn}
{\it Критерий}~--- правило, согласно которому гипотеза $H_0$ принимается или отвергается.
\end{defn}
Выборка $\mathbf{X} = (X_1, \ldots, X_n$) объёма $n$~--- точка в пространстве $\mathbb{R}^{n}$. Выделим множество $S \subset \mathbb{R}^{n}$~--- {\it критическую область} для гипотезы $H_0$. В этом случае критерий можно сформулировать следующим образом:
\begin{compactlist}
    \item $\varphi(x) = 1 \Rightarrow$ отвергаем $H_0$, принимаем $H_1$;
    \item $\varphi(x) = 0 \Rightarrow$ отвергаем $H_1$, принимаем $H_0$;
\end{compactlist}

\begin{defn}
Говорят, что произошла {\it ошибка 1-го рода}, если критерий отверг верную гипотезу $H_0$. Вероятность ошибки 1-го рода (или {\it уровень значимости критерия}): 
\begin{equation*}
    \alpha(S)=P\left\{\mathbf{X} \in S \colon H_{0}\right\}=P_{0}\left\{\mathbf{X} \in S\right\}
\end{equation*}
Аналогично вероятность ошибки 2-го рода:
\begin{equation*}
    \beta(S)=P\left\{\mathbf{X} \notin S \colon H_{1}\right\}=P_{1}\left\{\mathbf{X} \notin S\right\}
\end{equation*}
\end{defn}

\begin{defn}
{\it Мощность критерия}:
\begin{equation*}
    \gamma(S)=1-\beta(S)=P_{1}\left\{\mathbf{X} \in S\right\}
\end{equation*}
\end{defn}

\begin{center}
\begin{tabular}{|c|c|c|}
\hline \multirow{2}{*} { Истинная гипотеза } & \multicolumn{2}{|c|} { Результат принятия решения } \\
\cline {2-3} & $H_{0}$ отклонена & $H_{0}$ принята \\
\hline$H_{0}$ & $\alpha$ & $1-\alpha$ \\
\hline$H_{1}$ & $1-\beta$ & $\beta$ \\
\hline
\end{tabular}
\end{center}

Если $\gamma(S)<\alpha(S)$, то попасть в $S$ при условии истинности гипотезы $H_1$ труднее, чем при условии истинности гипотезы $H_0$, т.е. $S$~--- критическая область скорее для $H_1$. Следовательно, неравенство должно иметь вид $\gamma(S)>\alpha(S)$.

\begin{defn}
    Критерий называется {\it несмещённым}, если выполняется условие
    \begin{equation*}
        \alpha(S) \leqslant \gamma(S)=1-\beta(S)
    \end{equation*}
\end{defn}

Зададим $\alpha_0$ и и будем иметь дело только с такими критериями, где $\alpha_{0} \geqslant \alpha(S)$ (т.е. вероятность ошибки первого рода не превосходит величины $\alpha_0$) и дополнительно будем решать задачу $\beta(S) \rightarrow \min\limits_{S}$.

Получаем две эквивалентные задачи определения критической области $S$:
\begin{equation*}
    \begin{array}{l}
    \left\{\begin{array}{l}
    \alpha_{0} \geqslant \alpha(S) \\
    \beta(S) \rightarrow \min\limits_{S}
    \end{array}\right.
    \Leftrightarrow~
    \left\{\begin{array}{l}
    \alpha_{0} \geqslant \alpha(S) \\
    \gamma(S) \rightarrow \max\limits_{S}
    \end{array}\right.
    \end{array}
\end{equation*}

Задачи в такой постановке не всегда решаемы, так как требуетсяответить точно <<да>> или <<нет>>. Такие статистические критерии называются {\it нерандомизированными критериями}.

\begin{exmp}
Рассмотрим {\it критическую функцию} $\varphi(x)=I\{x \in S\}$. Тогда критерий примет вид:
\begin{compactlist}
    \item Если $\varphi\left(\mathbf{X}\right)=1$, тогда отвергаем гипотезу $H_0$, принимаем $H_1$.
    \item Если $\varphi\left(\mathbf{X}\right)=0$, тогда отвергаем гипотезу $H_1$, принимаем $H_0$.
\end{compactlist}
\end{exmp}

\begin{exmp}
Рассмотрим другую критическую функцию $\varphi(x)=P\left\{\overline{H}_{0} / \mathbf{X}=x\right\}$. В этом случае $\varphi\left(\mathbf{X}\right) \in[0;1]$~--- условная вероятность отклонения гипотезы $H_0$. При таком определении $\varphi(x)$ приходим к {\itрандомизированному критерию}, то есть, критерию, который при некоторых значениях $s$ может не давать ответа <<да>> или <<нет>> в отношении истинности гипотезы $H_0$. Тогда формулировка критерия следующая:
\begin{compactlist}
    \item с вероятностью $1 - \varphi\left(\mathbf{X}\right)$ следует принимать гипотезу $H_0$;
    \item с вероятностью $\varphi\left(\mathbf{X}\right)$ следует принимать гипотезу $H_0$.
\end{compactlist}
\end{exmp}

\begin{rmrk}
При использовании введенного обозначения вероятность ошибки первого рода, вероятность ошибки второго рода и мощность критерия будем обозначать: $\alpha(\varphi)$, $\beta(\varphi)$ и $\gamma(\varphi)=1-\beta(\varphi)$ соответственно.
\end{rmrk}

Без ограничения общности будем предполагать, что существует плотность $f_{0}(x)$ для функции распределения $F_{0}(x)$, и существует плотность $f_{1}(x)$ для функции распределения $F_{1}(x)$. В дискретном случае все результаты аналогичны.

Если верна гипотеза $H_1$, то функция правдоподобия выборки $X$ имеет вид:
\begin{equation*}
    L_{1}\left(\mathbf{X}\right)=\prod_{i=1}^{n} f_{1}\left(X_{i}\right)
\end{equation*}

Для рандомизированного критерия получаем
\begin{gather*}
    P_{0}\left(\overline{H}_{0}\right)=\int\limits_{\mathbb{R}^{n}} \varphi(x) L_{0}(x) \mu^{n}(d x)=\alpha(\varphi) \\
    P_{1}\left(H_{0}\right)=\int\limits_{\mathbb{R}^{n}}(1-\varphi(x)) L_{1}(x) \mu^{n}(d x)=\beta(\varphi) \\
    \gamma(\varphi)=\int\limits_{\mathbb{R}^{n}} \varphi(x) L_{1}(x) \mu^{n}(d x), \quad \gamma(\varphi)=1-\beta(\varphi)
\end{gather*}

Тогда задача построения статистического критерия сводится к нахождению критической функции $\varphi(x)$ и будет формулироваться следующим образом:
\begin{equation*}
    \begin{array}{l}
    \left\{\begin{array}{l}
    \alpha_{0} \geqslant \alpha(\varphi) \\
    \beta(\varphi) \rightarrow \min\limits_{\varphi}
    \end{array}\right.
    \Leftrightarrow
    \left\{\begin{array}{l}
    \alpha_{0} \geqslant \alpha(\varphi) \\
    \gamma(\varphi) \rightarrow \max\limits_{\varphi}
    \end{array}\right.
    \end{array}
\end{equation*}
Таким образом, задача заключается в том, чтобы найти наиболее мощный критерий, когда вероятность ошибки первого рода не превосходит некоторого заданного порогового значения. Решение сформулированных задач даётся леммой Неймана-Пирсона.

\begin{namedthm}[Лемма Неймана---Пирсона]
Пусть $\alpha_{0} \in(0;1)$, тогда при фиксированной вероятности ошибки первого рода $\alpha_{0}$ наиболее мощный критерий имеет критическую функцию $\varphi^{*}$ вида
\begin{equation*}
    \varphi^{*}(x)=\left\{\begin{array}{ll}
    1, & \text { если } L_{1}(x)>c L_{0}(x) \\
    \varepsilon, & \text { если } L_{1}(x)=c L_{0}(x) \\
    0, & \text { если } L_{1}(x)<c L_{0}(x)
    \end{array}\right.
\end{equation*}
где $L_{j}(x)=\prod_{i=1}^{n} f_{j}\left(x_{i}\right)$ соответствует гипотезе $H_j, j = \overline{1,2}$, константы $c$ и $\varepsilon$ являются решениями уравнения $\alpha\left(\varphi^{*}\right)=\alpha_{0}$.
\end{namedthm}

\begin{proof}
\begin{enumerate}
    \item Покажем, что константы $c$ и $\varepsilon$ могут быть найдены из уравнения $\alpha\left(\varphi^{*}\right)=\alpha_{0}$. Заметим, что
    
    \begin{equation*}
        \begin{aligned} \alpha(\varphi^{*})
        = P_{0}(L_{1}(\mathbf{X}) > c L_{0}(\mathbf{X})) 
        + \varepsilon P_{0}(L_{1}(\mathbf{X}) = c L_{0}(\mathbf{X}))=\\ 
        = P_{0}\left(\frac{L_{1}(\mathbf{X})}{L_{0}(\mathbf{X})} > c\right) 
        + \varepsilon P_{0}\left(\frac{L_{1}(\mathbf{X})}{L_{0}(\mathbf{X})} = c \right) 
        \end{aligned}
    \end{equation*}

Если предположить, что $L_{0}(\mathbf{X})=0$, то

\begin{equation*}
    P_{0}\left\{L_{0}(\mathbf{X}) = 0\right\} = \int\limits_{\left\{x: L_{0}(x)=0\right\}} L_{0}(x) \mu(d x)=0
\end{equation*}

и, следовательно, вышеприведённое равенство корректно. Поэтому рассмотрим случайную величину $\eta(\mathbf{X}) = \cfrac{L_{1}(\mathbf{X})}{L_{0}(\mathbf{X})}$

Положим $F_{H_{0}, \eta}(t)=P\{\eta \leqslant t\}$, тогда
\begin{equation*}
    \alpha\left(\varphi^{*}\right)=1-F_{H_{0}, \eta}(c)+\varepsilon\left(F_{H_{0}, \eta}(c)-F_{H_{0}, \eta}(c-0)\right)
\end{equation*}

Пусть $g(c)=1-F_{H_{0}, \eta}(c)$, константу $c_{\alpha_{0}}$ можно выбрать так, чтобы было выполнено неравенство:
\begin{equation*}
    g(c_{\alpha_{0}}) \leqslant \alpha_{0} \leqslant g(c_{\alpha_{0}}-0)
\end{equation*}

Тогда
\begin{equation*}
    \varepsilon_{\alpha_{0}} = 
    \left\{\begin{array}{ll}
         0, & \text{ если }  g\left(c_{\alpha_{0}}\right)=g\left(c_{\alpha_{0}}-0\right) \\
         \cfrac{\alpha_{0}-g\left(c_{\alpha_{0}}\right)}{g\left(c_{\alpha_{0}}-0\right)-g\left(c_{\alpha_{0}}\right)} \in [0;1], & \text{ если } g\left(c_{\alpha_{0}}\right)<g\left(c_{\alpha_{0}}-0\right)
    \end{array}\right.
\end{equation*}

В обоих случаях выполнено равенство:
\begin{equation*}
    \alpha_{0}=g\left(c_{\alpha_{0}}\right)+\varepsilon_{\alpha_{0}}\left(g\left(c_{\alpha_{0}}-0\right)-g\left(c_{\alpha_{0}}\right)\right)=\alpha\left(\varphi^{*}\right)
\end{equation*}

\item Докажем, что $\varphi^{*}(x)$~--- критическая функция наиболее мощного критерия.

Выберем любую другую критическую функцию $\tilde{\varphi}(x)$ такую, что $\alpha(\tilde{\varphi}) \leqslant \alpha_{0}$, и сравним ее с критической функцией $\varphi^{*}(x)$. Заметим, что для любого $x$ справедливо неравенство:
\begin{equation*}
    \left(\varphi^{*}(x)-\tilde{\varphi}(x)\right)\left(L_{1}(x)-c_{\alpha_{0}} L_{0}(x)\right) \geqslant 0
\end{equation*}

Тогда
\begin{equation*}
    \int\limits_{\mathbb{R}^{n}}\left(\varphi^{*}(x)-\tilde{\varphi}(x)\right)\left(L_{1}(x)-c_{\alpha_{0}} L_{0}(x)\right) \mu^{n}(d x) \geqslant 0
\end{equation*}

Раскроем скобки и преобразуем:

\begin{equation*}
    \begin{array}{l}
\int\limits_{\mathbb{R}^{n}} \varphi^{*}(x) L_{1}(x) \mu^{n}(d x)-\int\limits_{\mathbb{R}^{n}} \tilde{\varphi}(x) L_{1}(x) \mu^{n}(d x) \geqslant \\
\quad \geqslant c_{\alpha_{0}}\left(\int\limits_{\mathbb{R}^{n}} \varphi^{*}(x) L_{0}(x) \mu^{n}(d x)-\int\limits_{\mathbb{R}^{n}} \tilde{\varphi}(x) L_{0}(x) \mu^{n}(d x)\right) \geqslant 0
\end{array}
\end{equation*}

Следовательно, $\gamma\left(\varphi^{*}\right)-\gamma(\tilde{\varphi}) \geqslant c_{\alpha_{0}}\left(\alpha\left(\varphi^{*}\right)-\alpha(\tilde{\varphi})\right)$, откуда получаем неравенство:

\begin{equation*}
    \gamma\left(\varphi^{*}\right) \geqslant \gamma(\tilde{\varphi})
\end{equation*}
\end{enumerate}
\end{proof}

\section{Критерии согласия Колмогорова и $\chi^{2}$}

Пусть для наблюдаемого распределения $\mathbb{P}_{\xi}$ дана выборка $X_1, \ldots, X_n$, проверяется {\it гипотеза согласия} $H_{0}: F_{\xi}=F_{0}$, где $F_{0}$ известна; альтернатива $H_{1}: F_{\xi} \neq F_{0}$.

\subsubsection{Критерий $\chi^{2}$}
Разобьём числовую ось на $k$ промежутков ${-\infty=a_{0}<a_{1}<\ldots<a_{k}=\infty}$, ${\Delta_{i}=\left(a_{i-1}, a_{i}\right]}$ и построим статистику $\overline{\chi}^{2}$:
\begin{equation*}
    \overline{\chi}^{2}(\mathbf{X})=\sum\limits_{i=1}^{k} \frac{\left(n_{i}-n p_{i}^{(0)}\right)^{2}}{n p_{i}^{(0)}},
\end{equation*}
где $n_i$~--- число зафиксированных наблюдений в $i$-м интервале,
$p_{i}^{(0)}=F_{0}\left(a_{i}\right)-F_{0}\left(a_{i-1}\right)$~--- вероятность попадания наблюдения в $i$-й интервал при выполнении гипотезы $H_0$, $n p_{i}^{(0)}$, соответственно, ожидаемое число попаданий в $i$-й интервал.

Формулировка критерия:
\begin{compactlist}
    \item Если верна гипотеза $H_0$, то $\overline{\chi}^{2}\left(\mathbf{X}\right) \xrightarrow[n \to \infty]{\text{d}} \zeta$, где $\zeta$ подчиняется распределению $\chi^{2}$ с $k-1-r$ степенями свободы ($k$~--- число интервалов разбиения, $r$~--- число параметров предполагаемого закона распределения);
    \item Если верна гипотеза $H_1$, то $\overline{\chi}^{2} \xrightarrow[n \to \infty]{\text{п.н.}} \infty$.
\end{compactlist}

Выберем вероятность $\alpha \in (0;1)$. Область $(C(k-1,1-\alpha), \infty)$, где $C(k-1,1-\alpha)$~--- квантиль порядка $1-\alpha$ распределения $\chi^{2}$ с $k-1-r$ степенями свободы, является критической для гипотезы $H_0$.

Правило проверки гипотез:
\begin{compactlist}
    \item Если $\overline{\chi}^{2} \left(\mathbf{X}\right)>C(r-1,1-\alpha)$, то $H_0$ отклоняется;
    \item Если $\overline{\chi}^{2} \left(\mathbf{X}\right) \leqslant C(r-1,1-\alpha)$, то для отклонения $H_0$ нет оснований.
\end{compactlist}
\medskip
\begin{center}
\begin{tikzpicture}
    \begin{axis}[xmin=-0.5,ymin=-0.008,
        xmax=15,ymax=0.1, width=15cm, height=8cm,
        axis line style = thick,
        axis lines = middle,
        enlargelimits=false, axis on top, ticks=none,
        ]
        \addplot[name path=bell, very thick, blue,
        domain=0:15,samples=100]
        {exp(-x/2)*x/8};
        \path [name path=flooor]
        (\pgfkeysvalueof{/pgfplots/xmin},0) --
        (\pgfkeysvalueof{/pgfplots/xmax},0);
        \addplot [blue!20] fill between [
        of=bell and flooor,soft clip={domain=8:15},
        ];
        \addplot[only marks, color = blue, thick,mark=*] plot coordinates {(8, 0)} node[black, below]{$c_{1-\alpha}$};
        \node[draw=blue, text=blue, fill=white] at (9.4, 0.005) {\small$\alpha$};
    \end{axis}
\end{tikzpicture}
\end{center}

Фактически критерий $\chi^{2}$ проверяет значимость расхождения эмпирических (наблюдаемых) и теоретических (ожидаемых) частот. Рассмотрим его применение на следующем примере.
\begin{exmp}
Следующая задача возникла в связи с бомбардировками Лондона во время Второй мировой войны. Для улучшения организации оборонительных мероприятий, необходимо было понять цель противника. Для этого территорию города условно разделили сеткой из 24 горизонтальных и 24 вертикальных линий на 576 равных участков. В течении некторого времени в центре организации обороны города собиралась информация о количестве попаданий снарядов в каждый из участков. В итоге были получены следующие данные:
\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline Число попаданий & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
    \hline Количество участков & 229 & 211 & 93 & 35 & 7 & 0 & 0 & 1 \\
    \hline
    \end{tabular}
\end{center}

Гипотеза $H_0$: стрельба случайна (нет <<целевых>> участков).

Высчитаем теоретические вероятности по закону редких событий (распределение Пуассона):
\begin{equation*}
    p_i^{(0)} = \mathbb{P}\{S=i\}=\frac{\lambda^{i}}{i !} e^{-\lambda}, ~\text{где $S$~--- число попаданий},~ \lambda = \overline{X} \approx 0,932
\end{equation*}
\end{exmp}

Обозначим за $n_i$ количество участков, на которые пришлось $i$ попаданий, и составим новую таблицу для применения критерия.

\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline $i$ & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
    \hline $n_i$ & 229 & 211 & 93 & 35 & 7 & 0 & 0 & 1 \\
    $n_{i} \cdot p_{i}^{(0)}$ & 226,7 & 211,4 & 98,5 & 30,6 & 7,14 & 1,33 & 0,21 & 0,03 \\
    \cline { 6 - 9 }$n_{i} \cdot \tilde{p}_{i}^{(0)}$ & 228,6 & 211,3 & 97,6 & 30,1 & \multicolumn{4}{|c|} {8,46} \\
\hline
\end{tabular}
\end{center}

Прежде чем вычислять статистику $\overline{\chi}^{2}$, мы объединили 4 последних события с низкими частотами в одно и пересчитали новые теоретические вероятности $\tilde{p}_i^{(0)}$ и, соответственно, новые ожидаемые значения. В этом случае $\overline{\chi}^{2} \approx 1,05$. Т.к. $k=5$, то по таблице распределения $\chi^{2}$ находим соответствующий уровень значимости $\alpha = 0,79$. Гипотеза о низкой точности стрельбы принимается.

Обратим внимание на необходимость объединения маловероятных промежутков: если оставить $k = 8$, то $\overline{\chi}^{2} \approx 32,6$, что значительно велико даже на уровне $\alpha = 10^{-5}$. Подобная ошибка критерия $\chi^{2}$ вероятна на всех выборках с низкочастотными событиями. Проблема решается либо отбрасыванием, либо объединением данных событий ({\it коррекция Йетса}).

\subsubsection{Критерий Колмогорова}
Наложим дополнительное условие на исходную задачу: $F_{0}(x) \in C(\mathbb{R})$.

Рассмотрим статистику Колмогорова:
\begin{equation*}
    D_{n}\left(\mathbf{X}\right)=\sup\limits_{x \in R}\left|F_{n}^{*}(x)-F_{0}(x)\right|
\end{equation*}

Формулировка критерия:
\begin{compactlist}
    \item Если верна гипотеза $H_0$, то $D_{n}\left(\mathbf{X}\right) \frac{\text { п.н. }}{n \rightarrow \infty} 0$;
    \item Если верна гипотеза $H_1$, т.е. $F_{\xi} \equiv G \neq F_{0}$, то
    \begin{equation*}
        D_{n}\left(\mathbf{X}\right) \frac{\text { п.н. }}{n \rightarrow \infty} \sup\limits_{x \in R}\left|G(x)-F_{0}(x)\right|>0
    \end{equation*}
\end{compactlist}

\begin{lem}
Если гипотеза $H_0$ верна, и $F_{0}(x) \in C(\mathbb{R}$, то распределение статистики $D_{n}=\sup\limits_{x \in R}\left|F_{n}^{*}\left(x ; x_{[n]}\right)-F_{0}(x)\right|$ не зависит от наблюдаемого распределения.
\end{lem}

При больших $n$ применяется асимптотический подход.
\begin{namedthm}[Теорема Колмогорова]
Если гипотеза $H_0$ верна, и $F_{0}(x) \in C(\mathbb{R}$, то имеет место сходимость:
\begin{equation*}
    P\left\{\sqrt{n} D_{n}\left(\mathbf{X}\right) \leqslant z\right\} \underset{n \rightarrow \infty}{\longrightarrow} K(z)=1+2 \sum\limits_{m=1}^{\infty}(-1)^{m} e^{-2 m^{2} z^{2}}
\end{equation*}
\end{namedthm}

Находим константу $d_{1-\alpha}$ как решение уравнения $K\left(d_{1-\alpha}\right)=1-\alpha$.

Правило проверки гипотез:
\begin{compactlist}
    \item Если $\sqrt{n} D_{n}\left(\mathbf{X}\right) \in\left(d_{1-\alpha}, \infty\right)$, то гипотеза $H_0$ отвергается;
    \item Если $\sqrt{n} D_{n}\left(\mathbf{X}\right) \notin\left(d_{1-\alpha}, \infty\right)$, то гипотеза $H_0$ принимается.
\end{compactlist}

\begin{exmp}
Приведена таблица результатов исследования при $n=100$:
\begin{center}
    \begin{tabular}{|l|l|l|l|l|l|}
    \hline Количество предметов & 1 & 2 & 3 & 4 & 5 \\
    \hline Частота & 18 & 16 & 26 & 22 & 18 \\
    \hline
\end{tabular}
\end{center}
На уровне значимости $\alpha=0,2$ с помощью критерия Колмогорова определить, подчиняются ли данные выборки на интервале $[0;5]$ равномерному закону распределения случайной величины.

Запишем теоретическую функцию распределения:
\begin{equation*}
    F_{0}(x)=\left\{\begin{array}{cc}
    0, & x<0 \\
    x/5, & 0 \leqslant x \leqslant 5 \\
    1, & x>5
    \end{array}\right.
\end{equation*}

Составим следующую таблицу:
\begin{center}
    \begin{tabular}{|c|c|c|c|c|}
    \hline $x_{i}$ & $F(x_{i})$ & $n_{i}$ & $F^{*}_{n}(x_{i})$ & $|F_{0}(x_{i})-F^{*}_{n}(x_{i})|$ \\
    \hline 1 & 0,2 & 18 & 0,18 & 0,02 \\
    \hline 2 & 0,4 & 16 & 0,34 & 0,06 \\
    \hline 3 & 0,6 & 26 & 0,6 & 0 \\
    \hline 4 & 0,8 & 22 & 0,82 & 0,02 \\
    \hline 5 & 1 & 18 & 1 & 0 \\
\hline
\end{tabular}
\end{center}
Отсюда $D_{n}(x)=\sup\limits_{x \in R}\left|F_{n}^{*})-F_{0}(x)\right| = 0,06$, $\sqrt(n)D_{n}(x) = 0,6$, что меньше критического значения $0,65$ функции Колмогорова при уровне значимости $\alpha=0,2$, следовательно, гипотеза о равномерном распределении принимается.
\end{exmp}

\section{Статистические выводы о параметрах нормального распределения. Распределения $\chi^{2}$ и Стьюдента. Теорема Фишера}

\begin{defn}
    Говорят, что случайная величина $\xi$ имеет {\it гамма-распределение} с параметрами $\alpha > 0,~ \lambda > 0$ $(\mathbf{\Gamma}(\alpha, \lambda))$, если $\xi$ имеет следующую плотность распределения:
    \begin{equation*}
        f_{\xi}(x)=\left\{\begin{array}{ll}
        0, & \text { если } x \leqslant 0 \\
        c \cdot x^{\lambda-1} e^{-\alpha x}, & \text { если } x>0
        \end{array}\right.,
    \end{equation*}
    где постоянная $c$ вычисляется из свойства нормировки плотности:
    \begin{equation*}
        1=\int_{-\infty}^{\infty} f_{\xi}(x) d x=c \int_{0}^{\infty} x^{\lambda-1} e^{-\alpha x} d x=\frac{c}{\alpha^{\lambda}} \int_{0}^{\infty}(\alpha x)^{\lambda-1} e^{-\alpha x} d(\alpha x)=\frac{c}{\alpha^{\lambda}} \Gamma(\lambda),
    \end{equation*}
    откуда $c=\alpha^{\lambda} / \Gamma(\lambda)$.
\end{defn}

\begin{lem}
    Пусть $\xi_{1}, \ldots, \xi_{n}$ независимы, и $\xi_i \sim \mathbf{\Gamma}(a, \lambda_i), i=\overline{1,n}$. Тогда их сумма $S_{n}=\xi_{1}+\ldots+\xi_{n} \sim \mathbf{\Gamma}(a, \lambda_1 + \ldots + \lambda_n)$
\end{lem}
\begin{lem}
    Если $\xi \sim \mathbf{N}(0,1)$, то $\xi^2 \sim \mathbf{\Gamma}(1/2, 1/2)$.
\end{lem}
\begin{crlr}
    Если $\xi_{1}, \ldots, \xi_{k}$ независимы и $\xi_i \sim \mathbf{N}(0,1)$, то случайная величина $\chi^{2}=\xi_{1}^{2}+\ldots+\xi_{k}^{2} \sim \mathbf{\Gamma}(1/2, k/2)$
\end{crlr}

\begin{defn}
    Распределение суммы $k$ квадратов независимых случайных величин со стандартным нормальным распределением называется {\it распределением хи-квадрат} с $k$ степенями свободы ($\chi^2(k)$).
\end{defn}
Плотность распределения $\chi^2(k)$ имеет вид
\begin{equation*}
    f(y)=\left\{\begin{array}{ll}
    \cfrac{1}{2^{k / 2} \Gamma(k / 2)} y^{\frac{k}{2}-1} e^{-y / 2}, & \text { если } y>0 \\
    0, & \text { если } y \leqslant 0
    \end{array}\right.
\end{equation*}
\begin{rmrk}
    $\chi^2(2) = \mathbf{\Gamma}(1/2,1) = \mathbf{Exp}(1/2)$
\end{rmrk}
\begin{namedthm}[Свойства распределения $\chi^{2}$]\leavevmode
\begin{enumerate}
    \item Если случайные величины $\xi_1 \sim \chi^{2}(k)$ и $\xi_2 \sim \chi^{2}(m)$ независимы, то их сумма $\xi_1+\xi_1 \sim \chi^{2}{k+m}$;
    \item $\mathbb{E} \chi^{2}=k, \quad \mathbb{D} \chi^{2}=2 k$
    \item Пусть дана последовательность случайных величин $\chi_{n}^{2}$. Тогда при $n \to \infty$:
    \begin{equation*}
        \frac{\chi_{n}^{2}}{n} \stackrel{\mathbb{P}}{\longrightarrow} 1, \quad \frac{\chi_{n}^{2}-n}{\sqrt{2 n}} \Rightarrow \mathrm{N}_{0,1}
    \end{equation*}
    \item Пусть случайные величины $\xi_1, \ldots, \xi_n$ независимы и $\xi_i \sim \mathbf{N}(a,\sigma^{2})$. Тогда
    \begin{equation*}
        \sum\limits_{i=1}^{k}\left(\frac{\xi_{i}-a}{\sigma}\right)^{2} \sim \chi^{2}(k)
    \end{equation*}
\end{enumerate}
\end{namedthm}

\begin{defn}
    Пусть $\xi_{0}, \xi_{1}, \ldots, \xi_{k}$ независимы и $\xi_i \sim \mathbf{N}(0,1)$. Распределение случайной величины
    \begin{equation*}
        t_{k}
        = \frac{\xi_{0}}{\sqrt{\frac{\xi_{1}^{2} + \ldots + \xi_{k}^{2}}{k}}} 
        = \frac{\xi_0}{\sqrt{x_{k}^{2} / k}}
    \end{equation*}
    называется {\it распределением Стьюдента ($t$-распределением} с $k$ степенями свободы ($\mathbf{T}(k)$).
\end{defn}
Плотность распределения $\mathbf{T}(k)$ имеет вид
\begin{equation*}
    f_{k}(y)=\frac{\Gamma((k+1) / 2)}{\sqrt{\pi k} \Gamma(k / 2)}\left(1+\frac{y^{2}}{k}\right)^{-(k+1) / 2}
\end{equation*}

\begin{namedthm}[Свойства распределения Стьюдента]
\begin{enumerate}
    \item Распределение Стьюдента симметрично, т.е. если $t_k \sim \mathbf{T}(k)$, то $-t_k \sim \mathbf{T}(k)$.
    \item $\mathbf{T}(k) \Rightarrow \mathbf{N}(0,1)$ при $n \to \infty$.
    \item У распределения Стьюдента $\mathbf{T}(k)$ существуют только моменты порядка $m < k$, при этом все существующие моменты нечётного порядка равны нулю.
\end{enumerate}
\end{namedthm}

\begin{namedthm}[Теорема Фишера]
Пусть случайные величины $\xi_1, \ldots, \xi_n$ независимы и ${\xi_i \sim \mathbf{N}(a,\sigma^{2})}$. Тогда:
\begin{enumerate}
    \item $\sqrt{n} \frac{\overline{X}-a}{\sigma} \sim \mathrm{N}(0,1)$
    \item $\frac{(n-1) S_{0}^{2}}{\sigma^{2}}=\sum\limits_{i=1}^{n} \frac{\left(X_{i}-\overline{X}\right)^{2}}{\sigma^{2}} \sim \chi^{2}(n-1)$
    \item Случайные величины $\overline{X}$ и $S_{0}^{2}$ независимы.
\end{enumerate}
\end{namedthm}
\begin{crlr}
    Пусть случайные величины $\xi_1, \ldots, \xi_n$ независимы и ${\xi_i \sim \mathbf{N}(a,\sigma^{2})}$. Тогда:
    \begin{enumerate}
        \item $\sqrt{n} \frac{\overline{X}-a}{\sigma} \sim \mathrm{N}(0,1)$ (для $a$ при известном $\sigma^{2}$)
        \item $\sum\limits_{i=1}^{n}\left(\frac{X_{i}-a}{\sigma}\right)^{2} \sim \chi^{2}(n)$ (для $\sigma^{2}$ при известном $a$)
        \item $\frac{(n-1) S_{0}^{2}}{\sigma^{2}} \sim \chi^{2}(n-1)$ (для $\sigma^{2}$ при неизвестном $a$)
        \item $\sqrt{n} \frac{\overline{X}-a}{S_{0}} \sim \mathrm{T}(n-1)$ (для $a$ при неизвестном $\sigma^{2}$)
    \end{enumerate}
\end{crlr}

\subsubsection{Статистические выводы о параметрах нормального распределения}

Пусть $X_{1}, \ldots, X_{n}$~--- выборка объёма $n$ из распределения $\mathrm{N}_{a, \sigma^{2}}$. Построим точные доверительные интервалы (ДИ) с уровнем доверия $\alpha$ для параметров нормального распределения, используя следствие из теоремы Фишера.
\begin{enumerate}
    \item ДИ для $a$ при известном $\sigma^{2}$:
    \begin{equation*}
        \mathbb{P}\left(\overline{X}-\frac{\tau \sigma}{\sqrt{n}}<a<\overline{X}+\frac{\tau \sigma}{\sqrt{n}}\right)=\alpha, ~ \text {где} ~ \varphi_{0,1}(\tau)=\frac{1 + \alpha}{2}
    \end{equation*}
    \item ДИ для $\sigma^{2}$ при известном $a$:
    \begin{equation*}
        \frac{n S_{1}^{2}}{\sigma^{2}} \sim \chi^{2}(n),~ \text {где}~ S_{1}^{2}=\frac{1}{n} \sum\limits_{i=1}^{n}\left(X_{i}-a\right)^{2}
    \end{equation*}
    Пусть $g_1$ и $g_2$~--- квантили распределения $\chi^{2}(n)$ уровней $\frac{1-\alpha}{2}$ и $\frac{1+\alpha}{2}$ соответственно. Тогда
    \begin{equation*}
        \frac{(n-1) S_{0}^{2}}{\sigma^{2}} \sim \chi^{2}(n-1),~ \text {где}~ S_{0}^{2}=\frac{1}{n-1} \sum\limits_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}
    \end{equation*}
    \item ДИ для $\sigma^{2}$ при неизвестном $a$:
    \begin{equation*}
        \frac{(n-1) S_{0}^{2}}{\sigma^{2}} \sim \chi^{2}(n-1),~ \text {где}~ S_{0}^{2}=\frac{1}{n-1} \sum\limits_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}
    \end{equation*}
    Пусть $g_1$ и $g_2$~--- квантили распределения $\chi^{2}(n-1)$ уровней $\frac{1-\alpha}{2}$ и $\frac{1+\alpha}{2}$ соответственно. Тогда
    \begin{equation*}
        \alpha=\mathbb{P}\left(g_{1}<\frac{(n-1) S_{0}^{2}}{\sigma^{2}}<g_{2}\right)=\mathbb{P}\left(\frac{(n-1) S_{0}^{2}}{g_{2}}<\sigma^{2}<\frac{(n-1) S_{0}^{2}}{g_{1}}\right)
    \end{equation*}
    \item ДИ для $a$ при неизвестном $\sigma^{2}$:
    \begin{equation*}
        \sqrt{n} \frac{\overline{X}-a}{S_{0}} \sim \mathrm{T}(n-1)
    \end{equation*}
    Пусть $c$~--- квантиль распределения $\mathrm{T}(n-1)$ уровня $\frac{1-\alpha}{2}$. Распределение Стьюдента симметрично, поэтому
    \begin{equation*}
        \alpha=\mathbb{P}\left(-c<\sqrt{n} \frac{\overline{X}-a}{S_{0}}<c\right)=\mathbb{P}\left(\overline{X}-\frac{c S_{0}}{\sqrt{n}}<a<\overline{X}+\frac{c S_{0}}{\sqrt{n}}\right)
    \end{equation*}
\end{enumerate}

\begin{thebibliography}{1}
	\bibitem{cher_terover}
		Н.И.Чернова.
		Теория вероятностей,
		2007
	\bibitem{char_stat}
		Н.И.Чернова.
		Математическая статистика,
		2014
	\bibitem{ushakov_tvims}
		В.Г. Ушаков.
		Теория вероятностей и математическая статистика,
		2014
	\bibitem{ulyanov_tvims}
		В.В. Ульянов.
		Конспект лекций по теории вероятностей и математической статистике
	\bibitem{mpti_teorver}
		А. Марков.
		Теория вероятностей,
		2017
\end{thebibliography}
\end{document}
