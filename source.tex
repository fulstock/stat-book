\documentclass[oneside,final,14pt]{extreport}

\usepackage[nottoc,notlot,notlof]{tocbibind}
\usepackage{cmap}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{relsize}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\usepackage{hhline}
\usepackage{multirow}

\usepackage{etoolbox}
    \makeatletter
    \patchcmd{\@makechapterhead}{\vspace*{50\p@}}{}{}{}% Removes space above \chapter head
    \patchcmd{\@makeschapterhead}{\vspace*{50\p@}}{}{}{}% Removes space above \chapter* head
    \makeatother

\newcommand\mydef{{\bf Опр.}}
\newcommand\mynote{{\bf Замеч.}}
\newcommand\myst{{\bf Утв.}}
\newcommand\mycon{{\bf Следствие.}}
\newcommand\myth{{\bf Теорема.}}
\newcommand\myqed{{\bf Док-во.}}
\newcommand\myex{{\bf Пример.}}
\newcommand\myprob[1]{{\mathbb{P}(#1)}}
\newcommand\mydes{{\bf Обозн.}}

\renewcommand{\qedsymbol}{$\blacksquare$}
\renewenvironment{proof}{{\bfseries Доказательство.}}{\qed}

\newtheorem{thm}{Теорема}[section]
\newtheorem{lem}[thm]{Лемма}
\newtheorem*{rmrk}{Замечание}
\newtheorem*{crlr}{Следствие}
\newtheorem*{symb}{Обозначение}
\theoremstyle{definition}
\newtheorem{defn}{Определение}[section]
\newtheorem*{exmp}{Пример}

\newcommand\sbullet[1][.5]{\mathbin{\vcenter{\hbox{\scalebox{#1}{$\bullet$}}}}}
\newenvironment{compactlist}{
\begin{list}{{$\sbullet[.75]$}}{
\setlength\partopsep{0pt}
\setlength\parskip{0pt}
\setlength\parsep{0pt}
\setlength\topsep{0pt}
\setlength\itemsep{0pt}
}
}{
\end{list}
}

\setpapersize{A4}
\setmarginsrb{2cm}{1.5cm}{2cm}{1.5cm}{0pt}{0mm}{0pt}{13mm}
\linespread{1.05}

\usepackage{indentfirst}
\sloppy

\usepackage{graphicx} 

\begin{document}
\begin{titlepage}
    \centering
    \vfill
    {\scshape\large
        Московский государственный университет\\
        Факультет вычислительной математики и кибернетики\\
    }
    \vskip1cm
    {\scshape\huge
        Теория вероятностей\\
        Математическая статистика\\
    }
    \vskip0.5cm
    {\upshape\large
        Рожков И., Рыгин А.
    }    
    \vfill
%    \includegraphics[width=8cm]{pic.png}
    \vfill
    {\upshape\large
        Москва\\
        ~2020
    }
\end{titlepage}

\tableofcontents
\chapter{Теория вероятностей}

\section{Вероятностное пространство. Операции над событиями. Свойства вероятности}
\begin{defn}
    {\it Пространство элементарных исходов}~--- любое непустое множество \( \Omega \ne \varnothing \). Элементы \( \omega \in \Omega \)~--- {\it элементарные исходы}.
\end{defn}

\begin{defn}
{\it Алгебра} \( \mathcal{A} \)~--- множество подмножеств \( \Omega \), обладающее следующими свойствами:

\begin{enumerate}
    \item \( \Omega \in \mathcal{A} \)
    \item \( A \in \mathcal{A} \Rightarrow \overline{A} \in \mathcal{A} \) (здесь $\overline{A}$~--- дополнение к $A$, т.е. $\overline{A} = \Omega \setminus A$)
    \item \( A, B \in \mathcal{A} \Rightarrow A \cup B \in \mathcal{A} \) (по индукции: \( A_1, A_2, ..., A_n \in \mathcal{A} \Rightarrow \bigcup\limits_{i=1}^n A_i \in \mathcal{A} \))
\end{enumerate}
\end{defn}

\begin{defn}
\( \sigma \text{\it{-алгебра~}} \mathcal{F} \)~--- множество подмножеств \( \Omega \), обладающее следующими свойствами:

\begin{enumerate}
    \item \( \Omega \in \mathcal{F} \)
    \item \( A \in \mathcal{F} \Rightarrow \overline{A} \in \mathcal{F} \)
    \item \( A_1, A_2,..., A_n,... \in \mathcal{F} \Rightarrow \bigcup\limits_{i=1}^\infty A_i \in \mathcal{F} \)
\end{enumerate}
\end{defn}

\begin{rmrk}
    Если \( A, B \in \mathcal{A} \), то \( A \cap B \equiv \overline{\overline{A} \cup \overline{B}} \in \mathcal{A} \)
\end{rmrk}

\begin{rmrk}
    Любая \( \sigma \text{-алгебра} \) является алгеброй. Первые два пункта определений идентичны, рассмотрим третий. Для любой конечной последовательности \( A_1, A_2,..., A_n \in \mathcal{A}\) составим соответствующую счётную последовательность \( A_1, A_2, ..., A_n, A_{n+1}=\varnothing, A_{n+2}=\varnothing,... \in \mathcal{A} \). По определению \( \sigma \text{-алгебры} \): \( \bigcup\limits_{i=1}^\infty A_i \in \mathcal{F} \Rightarrow \bigcup\limits_{i=1}^n A_i \in \mathcal{F} \), следовательно, выполнен третий пункт определения алгебры.
\end{rmrk}

\begin{defn}
    {\it Случайное событие} \(A\)~--- элемент \( \sigma \text{-алгебры~} \mathcal{F} \). \(A=\varnothing\)~---{\it невозможное событие}, \(A=\Omega\)~--- {\it достоверное событие}. Событие \( \overline{A} \)~--- {\it противоположное} \(A\), т.е. происходит тогда и только тогда, когда не происходит \(A\).
\end{defn}

Операции над событиями:

\begin{compactlist}
    \item {\it Объединение} \(A \cup B \)~--- происходит тогда и только тогда, когда происходят или \(A\), или \(B\), или оба вместе.
    \item {\it Пересечение} \(A \cap B \) (или \(AB \))~--- происходит тогда и только тогда, когда происходят и \(A\) и \(B\) вместе.
    \item {\it Разность} \(A \setminus B \)~--- происходит тогда и только тогда, когда происходит \(A\) и не происходит \(B\).
    \item {\it Симметрическая разность} \(A \triangle B \) ~--- происходит тогда и только тогда, когда либо происходит \(A\) и не происходит \(B\), либо происходит \(B\) и не происходит \(A\).
\end{compactlist}

\begin{defn}
    \( \sigma \text{-алгебра} \) {\it порождена классом \(K\)}, если она является пересечением всех \( \sigma \text{-алгебр}\), содержащих \(K\), т.е. является {\it минимальной \( \sigma \text{-алгеброй} \)}, содержащей \(K\).
\end{defn}

\begin{exmp}
    Пусть \( K = \{A\} \), тогда \( \sigma (K) = \{\varnothing, \Omega, A, \overline{A}\} \).
\end{exmp}

\begin{defn}
    {\it Вероятностная мера}~--- функция \( \mathbb{P}: \mathcal{F} \rightarrow \mathbb{R} \), обладающая следующими свойствами:

\begin{enumerate}
    \item \( \myprob{A} \geqslant 0~\forall A \in \mathcal{F} \) ({\it неотрицательность})
    \item \( \myprob{\Omega} = 1 \) ({\it нормировка})
    \item \( \forall A_1, A_2, ..., A_n... \in \mathcal{F},~ A_{i}A_{j} = \varnothing~ \forall i, j \in \mathbb{N}, i \ne j \Rightarrow \myprob{\bigcup\limits_{i=1}^\infty A_i} = \sum\limits_{i=1}^\infty \myprob{A_i} \) ({\it счётная аддитивность})
\end{enumerate}
\end{defn}

\begin{thm}
    Свойства вероятности:
    \begin{enumerate}
       \item \( \myprob{\varnothing}=0 \)
        \item \( A, B \in \mathcal{F}, B \subset A \Rightarrow \myprob{A} \geqslant \myprob{B} \)
       \item \( \myprob{A \setminus B} = \myprob{A} - \myprob{AB} \)
       \item \( \myprob{A \cup B} = \myprob{A} + \myprob{B} - \myprob{AB} \)
    \end{enumerate}
\end{thm}

\begin{proof}
    \begin{enumerate}
    \item Рассмотрим последовательность событий \( \Omega, \varnothing, \varnothing, ...\): \( \bigcup\limits_{i=1}^\infty A_i = \Omega \Rightarrow \myprob{\bigcup\limits_{i=1}^\infty A_i} = \myprob{\Omega} = 1 \). При этом \( A_{i}A_j = \varnothing~(i \ne j) \), следовательно, по п.3 определения вероятности: \( \sum\limits_{i=2}^\infty \myprob{\varnothing} = 0 \Rightarrow \myprob{\varnothing} = 0 \).
    \item \( B \subset A \Rightarrow A = (A \setminus B) \cup B \). Из неотрицательности вероятности и того, что \( (A \setminus B) \cap B = \varnothing \), следует, что \( \myprob{A} = \myprob{A \setminus B} + \myprob{B} \geqslant \myprob{B} \). Кроме того, \( \myprob{A \setminus B} = \myprob{A} \ \myprob{B})\).
    \item Следует из того, что \( A = (A \ B) \cup AB, (A \ B) \cap AB = \varnothing \).
    \item Следует из того, что \( A \cup B = (A \setminus AB) \cup B, (A \setminus AB) \cap B = \varnothing \).
\end{enumerate}
\end{proof}

\begin{defn}
    Тройка \( (\Omega, \mathcal{F}, \mathbb{P}) \)~--- {\it вероятностное пространство}.
\end{defn}
\begin{rmrk}
    Вероятностное пространство не является пространством в функциональном смысле.
\end{rmrk}

\begin{defn}
    Пусть \( \Omega = \{ w_1, w_2,..., w_n\} \)~--- конечное непустое множество, \( \mathcal{F} \)~--- множество всех подмножеств \( \Omega \). Положим \( \myprob{\{w_i\}} = p_i \). Вероятностное пространство, определённое таким образом,~--- {\it дискретное вероятностное пространство}. Тогда для любого события \( A = \{ w_1,...w_k\} \) его вероятность \( \myprob{A} = \sum\limits_{i=1}^k p_i \).
\end{defn}

\begin{defn}
    {\it Классическое определение вероятности}~--- \(p_1 = p_2 =...=p_n=\cfrac{1}{n} \). В этом случае \( \myprob{A} = \cfrac{k}{n} \).
\end{defn}

\section {Условная вероятность. Независимость событий. Критерий независимости. Формула полной вероятности. Формула Байеса.}

\begin{defn}
    Пусть задано вероятностное пространство \( (\Omega, \mathcal{F}, \mathbb{P}), A, B \in \mathcal{F}, \myprob{B} > 0 \). {\it Условная вероятность события \( A \) при событии \(B\)}:
    \begin{equation*}
        \myprob{A|B}=\cfrac{\myprob{AB}}{\myprob{B}}
    \end{equation*}
\end{defn}

\begin{thm}
    Условная вероятность \( \myprob{A|B} \)~--- вероятность, заданная на \( \mathcal{F} \).
\end{thm}

\begin{proof}
    Проверим три аксиомы из определения вероятности.

\begin{enumerate}
    \item  \( \myprob{\Omega|B} = \cfrac{\myprob{B \cap \Omega}}{\myprob{B}} = \cfrac{\myprob{B}}{\myprob{B}} = 1\)
    \item \( \forall A \in \mathcal{F}~ \myprob{A|B} \geq 0, \text{т.к.}~ \myprob{AB} \geqslant 0,~ \myprob{B} > 0 \)
    \item Пусть дана некоторая последовательность $\it A_1, A_2, \ldots A_n, \ldots; A_i \cap A_j = \varnothing ({\it i \ne j})$. Тогда: 
    \begin{multline*}
        \myprob{(\bigcup\limits_{i=1}^\infty A_i)|B}  = \cfrac{\myprob{(\bigcup\limits_{i=1}^\infty A_i) \cap B}}{\myprob{B}} = \cfrac{\myprob{\bigcup\limits_{i=1}^\infty(A_i \cap B)}}{\myprob{B}} = \cfrac{\sum\limits_{i=1}^\infty \myprob{A_i \cap B}}{\myprob{B}} = \\
        = \sum\limits_{i=1}^\infty \myprob{A_i | B}.
    \end{multline*}
\end{enumerate}
\end{proof}

Некоторые свойства условной вероятности:
\begin{enumerate}
    \item Если $A \cap B = \varnothing,$ то $\myprob {A | B} = 0.$
    \item Если $A \subset B$, то $\myprob{A|B} = 1.$ Например, $\myprob{B|B} = 1.$
\end{enumerate}

\myex{} Игральная кость подбрасывается один раз. {\it Известно}, что
выпало более трёх очков. Какова {\itпри этом} вероятность того, что выпало
нечётное число очков?

Пусть событие $B = \{4, 5, 6\}$ означает, что выпало более трёх очков, событие $A = \{1, 3, 5\}$ - выпало нечётное число очков. Как понимать вероятность события $A$, если известно, что $B$ случилось? Знаем, что произошло событие $B$, но всё равно не знаем, что именно выпало на кости. Однако теперь возможностей осталось только три: могло выпасть 4, 5 или 6 очков. Событию $A$ из этих равновозможных исходов благоприятен единственный исход: выпадение пяти очков. Поэтому искомая вероятность равна $\frac{1}{3}$.

\myex{} Имеется 1000 урн, среди которых 999 содержат только
чёрные шары, а одна — только белые. Сначала выберем наугад урну, а затем
из неё достанем один за другим два шара. Пусть событие $B$ означает, что первый вынутый шар оказался белым, событие $A$ - второй шар белый. Вероятность этим событиям случиться одновременно мала: $\myprob{A \cap B} = \myprob{A} = \myprob{B} = 0.001$, так как только в одном случае из 1000 мы выберем урну с белыми шарами.

Однако условная вероятность $\myprob{A | B}$ равна единице: если первый шар
белый, то мы уже выбрали урну с одними белыми шарами, и второй обязательно будет белым. При вычислении условной вероятности $\myprob{A | B}$ не
играет никакой роли то, насколько мала или велика вероятность события $B$.

\subsubsection{Независимость событий}

\begin{defn}
    
\end{defn}
    Пусть есть вероятностное пространство $(\Omega, \mathcal{F}, \mathbb{P})$. События $A_1, \ldots, A_n \in \mathcal{F}$ называются {\it независимыми в совокупности}, если $\forall~2 \leqslant k \leqslant n$ и $\forall~1 \leqslant i_1 \leqslant i_2 \leqslant ... \leqslant i_k \leqslant n$ выполняется 
    \begin{equation*}
        \myprob {\bigcap\limits_{j=1}^k A_{i_j}} = \prod\limits_{j=1}^k \myprob{A_{i_j}}
    \end{equation*}

В частности, при $n = 2$: события $A_1$ и $B$ независимы, если $\myprob{A \cap B} = \myprob{A}\myprob{B}$.

Некоторые свойства:
\begin{enumerate} 
    \item Если $A = \varnothing$, то для любого $B$ с ненулевой вероятностью, $A$ и $B$ независимы. Действительно, $AB = \varnothing \rightarrow 0 = \myprob{AB} = \myprob{A}\myprob{B} = 0 * \myprob{B} = 0$. Аналогично для $\myprob{A} = 0$.
    \item Если $\myprob{A} = 1$, то $A$ и $B$ независимы для любого $B$ с ненулевой вероятностью. Д-во аналогично.
    \item Пусть $A$ и $B$ независимы. Тогда события $\overline{A}$ и $B$, $A$ и $\overline{B}$, $\overline{A}$ и $\overline{B}$ также независимы. Докажем независимость $\Bar{A}$ и $B$. Для события $B$ справедливо представление $B = AB \cup \overline{A}B.$ Тогда $\myprob{B} = \myprob{AB} + \myprob{\overline{A}B}$, но $\myprob{AB} = \myprob{A}\myprob{B},$ следовательно, $\myprob{\overline{A}B} = \myprob{B} - \myprob{A}\myprob{B} = \myprob{B} (1 - \myprob{A}) = \myprob{\overline{A}}\myprob{B}$, и независимость $\overline{A}$ и $B$ доказана. Аналогично доказываются остальные утверждения.
    \item Пусть $A \subset B$ и $\myprob{A} > 0, \myprob{B} < 1$. Тогда $A$ и $B$ зависимы. Действительно, предположим, что они независимсы. Тогда $\myprob{AB} = \myprob{A}\myprob{B},$ но $\myprob{AB} = \myprob{A}$, следовательно, $\myprob{B} = 1$, что противоречит условию.
    \item Если события $A$ и $B$ независимы и $\myprob{B} > 0$, то условная вероятность $A$ при условии $B$ равна вероятности $A$. Действительно, 
    \begin{equation*}
        \myprob{A | B} = \cfrac{\myprob{AB}}{\myprob{B}} = \cfrac{\myprob{A}\myprob{B}}{\myprob{B}} = \myprob{A}
    \end{equation*}
\end{enumerate}

\begin{rmrk}
    В общем случае из попарной независимости событий $A_1, \ldots, A_n$ не следует их независимость в совокупности.
    \begin{exmp}
        Рассмотрим вероятностное пространство, в котором всего 4 различных элементарных исхода: $\Omega = \{ \omega_1, \omega_2, \omega_3, \omega_4 \}$. Пусть $\mathcal{F}$~--- множество всех подмножеств $\Omega,~\myprob{\{\omega_i\}} = \cfrac{1}{4},~i = \overline{1,4}$.
        
        Рассмотрим три события 
        \begin{equation*}
            A_1 = \{ \omega_1, \omega_4 \},~ 
            A_2 = \{ \omega_2, \omega_4 \},~
            A_3 = \{ \omega_3, \omega_4 \}
        \end{equation*}
        Их пересечения имеют вид:
        \begin{equation*}
            A_1A_2 = A_2A_3 = A_3A_1 = \{ \omega_4 \},~
            A_1A_2A_3 = \{ \omega_4 \}
        \end{equation*}
        Докажем, что события $A_1, A_2, A_3$ не являются независимыми в совокупности:
        \begin{gather*}
            \myprob{A_1} = \myprob{A_2} = \cfrac{1}{2}, \myprob{A_1A_2} = \myprob{A_2A_3} = \myprob{A_3A_1} = \cfrac{1}{4}, \\ \myprob{A_1A_2A_3} = \cfrac{1}{4} \neq \cfrac{1}{8} = \myprob{A_1}\myprob{A_2}\myprob{A_3}
        \end{gather*}
    \end{exmp}
\end{rmrk}

\subsubsection{Критерий независимости}

\begin{symb}
    \begin{equation*}
        A_{i}^{(\delta)}=\left\{\begin{array}{ll}A_{i}, & \delta=1 \\ \overline{A_{i}}, & \delta=0\end{array}\right.
    \end{equation*}
\end{symb}

\begin{thm}[Критерий независимости]
    События $A_1, ..., A_n$ независимы в совокупности $\Leftrightarrow \forall ~ \delta_1, \delta_2, ... \delta_n \in \{0, 1\}$ выполнено равенство
    \begin{equation*}
        \myprob{\bigcap_{i=1}^{n} A_{i}^{\left(\delta_{j}\right)}}=\prod_{i=1}^{n}\myprob{A_{i}^{\left(\delta_{i}\right)}}
    \end{equation*}
\end{thm}

\subsubsection{Формула полной вероятности}
\begin{thm}
    Пусть даны события $A, B_1, B_2, \ldots, B_n, \ldots; \myprob{B_i} > 0, $ причём $B_i \cap B_j = \varnothing~(i \neq j)$ и $\bigcup\limits_{i=1}^{\infty}B_i \supset A~$ (например, $\bigcup\limits_{i=1}^{\infty}B_i = \Omega$). Тогда справедлива {\it формула полной вероятности:}
\begin{equation*}
    \mathbb{P}(A)=\sum_{i=1}^{\infty} \mathbb{P}\left(B_{i}\right) \cdot \mathbb{P}\left(A | B_{i}\right)
\end{equation*}
\end{thm}

\begin{proof}
    Достаточно заметить, что при вышеперечисленных условиях $A = \bigcup\limits_{i=1}^{\infty}(AB_i),$ и $AB_i \cap AB_j = \varnothing ~(i \neq j).$ Тогда, учитывая $\myprob{B_i} > 0$, получаем
    \begin{equation*}
        \mathbb{P}(A)=\sum_{i=1}^{\infty} \mathbb{P}\left(A B_{i}\right)=\sum_{i=1}^{\infty} \mathbb{P}\left(B_{i}\right) \frac{\mathbb{P}\left(A B_{i}\right)}{\mathbb{P}\left(B_{i}\right)}=\sum_{i=1}^{\infty} \mathbb{P}\left(B_{i}\right) \cdot \mathbb{P}\left(A | B_{i}\right)
    \end{equation*}
\end{proof}

\myex{} Два игрока по очереди подбрасывают правильную игральную кость. Выигрывает тот, кто первым выкинет шесть очков. Найти вероятность победы игрока, начинающего игру.

События $H_1 = \text{\{при первом броске выпало 6 очков\}}$ и противоположное к нему $H_2$ образуют полную группу событий. Пусть событие $A$
означает победу игрока, начинающего игру. Обозначим через $x$ вероятность
события $A$. Для игрока, бросающего кость вторым, вероятность одержать
победу во всей игре равна $1 - x$. Тогда $\myprob{A | H_1} = 1, \myprob{A | H_2} = 1 - x$. Действительно, если при первом броске не выпало 6 очков, право броска переходит ко второму игроку. Игрок, начинавший игру, оказывается теперь в положении второго, и его вероятность выиграть становится равной $1 - x$. По формуле полной вероятности
$$x=\mathrm{P}(A)=\mathrm{P}\left(H_{1}\right) \mathrm{P}\left(A | H_{1}\right)+\mathrm{P}\left(H_{2}\right) \mathrm{P}\left(A | H_{2}\right)=\frac{1}{6}+\frac{5}{6}(1-x)=1-\frac{5}{6} x.$$

Отсюда $x = \frac{6}{11}$ - вероятность победы первого игрока.

\subsubsection{Формулы Байеса}
\begin{thm}
    Пусть даны события $A, B_1, B_2, \ldots, B_n, \ldots; \myprob{B_i} > 0$, причём $B_i \cap B_j = \varnothing ~(i \neq j)$ и $\bigcup\limits_{i=1}^\infty B_i \supset A$ (например, $\bigcup\limits_{i=1}^{\infty}B_i = \Omega$). Пусть также $\myprob{A} > 0$. Тогда справедливы {\it формулы Байеса} для $i = 1, 2, \ldots$:
    \begin{equation*}
        \mathbb{P}\left(B_{i} | A\right)=\frac{\mathbb{P}\left(B_{i}\right) \cdot \mathbb{P}\left(A | B_{i}\right)}{\sum\limits_{j=1}^{\infty} \mathbb{P}\left(B_{j}\right) \cdot \mathbb{P}\left(A | B_{j}\right)}
    \end{equation*}
\end{thm}

\begin{proof}
    Согласно формуле полной вероятности в знаменателе дроби стоит вероятность $A$. Тогда
    \begin{equation*}
        \frac{\mathbb{P}\left(B_{i}\right) \cdot \mathbb{P}\left(A | B_{i}\right)}{\mathbb{P}(A)}=\frac{\mathbb{P}\left(B_{i}\right) \cdot \mathbb{P}\left(A B_{i}\right)}{\mathbb{P}(A) \cdot \mathbb{P}\left(B_{i}\right)}=\frac{\mathbb{P}\left(A B_{i}\right)}{\mathbb{P}(A)}=\mathbb{P}\left(B_{i} | A\right) 
    \end{equation*}
\end{proof}

Вероятности $P(H_i)$, вычисленные заранее, до проведения эксперимента, называют {\it априорными вероятностями} (a’priori~--- «до опыта»). Условные вероятности $\myprob{H_i | A}$ называют {\it апостериорными вероятностями} (a’posteriori~--- «после опыта»). Формула Байеса позволяет переоценить заранее известные вероятности после того, как получено знание о результате эксперимента.

\myex{} Два стрелка подбрасывают монетку и выбирают, кто из
них будет стрелять по мишени (одной пулей). Первый стрелок попадает по
мишени с вероятностью $1$, второй стрелок — с вероятностью $10^{-5}$.

Можно сделать два предположения об эксперименте: $H_1$ - стреляет 1-й
стрелок (выпал герб) и $H_2$ - стреляет 2-й стрелок (выпала решка). Априорные вероятности этих гипотез одинаковы: $\mathrm{P}\left(H_{1}\right)=\mathrm{P}\left(H_{2}\right)=\frac{1}{2}.$

Как изменятся вероятности гипотез после проведения опыта? Рассмотрим
событие $A$ — пуля попала в мишень. Известно, что
$$\mathrm{P}\left(A | H_{1}\right)=1, \quad \mathrm{P}\left(A | H_{2}\right)=10^{-5}.$$
По формуле полной вероятности, вероятность пуле попасть в мишень равна
$$\mathrm{P}(A)=\frac{1}{2} \cdot 1+\frac{1}{2} \cdot 10^{-5}.$$
Предположим, что событие A произошло. Тогда по формуле Байеса

$$\mathrm{P}\left(H_{1} | A\right)=\frac{\frac{1}{2} \cdot 1}{\frac{1}{2} \cdot 1+\frac{1}{2} \cdot 10^{-5}}=\frac{1}{1+10^{-5}} \approx 0,99999,$$

$$\mathrm{P}\left(H_{2} | A\right)=\frac{\frac{1}{2} \cdot 10^{-5}}{\frac{1}{2} \cdot 1+\frac{1}{2} \cdot 10^{-5}}=\frac{10^{-5}}{1+10^{-5}} \approx 0,00001.$$

Попадание пули в мишень сделало выпадение герба в $10^5$ раз более вероятным, чем выпадение решки.

\section{Случайная величина. Порождённое и индуцированное вероятностные пространства. Функция распределения, ее свойства}

\begin{defn}
    {\it \( \text{Борелевская~} \sigma \text{-алгебра~} \mathcal{B} \) }~--- \( \sigma \text{-алгебра}\), порождённая множеством всех открытых интервалов на $\mathbb{R}$. Элемент \(B \in \mathcal{B}\)~--- {\it борелевское множество}.
\end{defn}


\begin{defn}
    {\it Борелевская функция} - функция $f: \mathbb{R} \mapsto \mathbb{R}$ такая, что $\forall B \in \mathcal{B} \quad f^{-1}(B) \in \mathcal{B}$.
\end{defn}

\begin{exmp}
    Функция Дирихле $D: \mathbb{R} \rightarrow\{0,1\}$
    \begin{equation*}
        D(x)=\left\{\begin{array}{ll}
        1, & x \in \mathbb{Q} \\
        0, & x \in \mathbb{R} \backslash \mathbb{Q}
        \end{array}\right.
    \end{equation*}
является борелевской. 
\end{exmp}

\begin{defn}
    \( (\Omega, \mathcal{A}) \) и \( (\Omega, \mathcal{F}) \)~--- {\it измеримые пространства}, элементы \(\mathcal{A}\) и \(\mathcal{F}\)~---{\it измеримые множества}, где $\Omega$~--- пространство элементарных исходов, $\mathcal{A}$~--- алгебра, $\mathcal{F}$~--- $\sigma$-алгебра.
\end{defn}

\begin{defn}
    Функция $\xi$: $\Omega \rightarrow \mathbb{R}^{(n)}$ называется {\it измеримой}, если полный прообраз борелевского множества $B$ лежит в $\sigma$-алгебре, т.е. 
    \begin{equation*}
        \xi^{-1}(B) = \{ \omega \,|\, \xi(\omega) \in B \} \in \mathcal{F} \quad \forall B \in \mathcal{B}
    \end{equation*}
\end{defn}

\subsubsection{Случайные величины}
\begin{defn}
    Пусть даны $(\Omega, \mathcal{F})$~--- измеримое пространство и $(\mathbb{R}, \mathcal{B})$, где $\mathcal{B}$~--- борелевская $\sigma$-алгебра множеств на числовой прямой $\mathbb{R}$. Тогда измеримая функция $\xi: \Omega \to \mathbb{R}$ называется {\it случайной величиной}.
\end{defn}

\begin{rmrk}
\begin{equation*}
    \xi^{-1}(B) = \{\omega \,|\, \xi(\omega) \in B\} \in \mathcal{A} \quad \forall B \in \mathcal{B},
\end{equation*}
    т.е. прообраз случайной величины~--- событие.
\end{rmrk}

\begin{exmp} (неизмеримая функция) Пусть $\xi$ задана как:
\begin{equation*}
    \xi(\omega)=\left\{\begin{array}{ll}
    1, & \omega \in [0; \frac{1}{2}] \\
    0, & \omega \in (\frac{1}{2}; 1]
\end{array}\right.
\end{equation*}
$\Omega = [0; 1], \mathcal{F} = \{\varnothing, \Omega\}$~--- минимальная $\sigma$-алгебра.  Заметим, что $\myprob{\{\xi < \frac{1}{2}\}}$ - не существует, т.к.
\begin{equation*}
    \xi^{-1}((-\infty;\frac{1}{2})) = (\frac{1}{2}; 1] \notin \mathcal{F},
\end{equation*}
т.е. нарушено условие измеримости, следовательно, $\xi$~--- не случайная величина.
\end{exmp} 

\begin{thm}
    Пусть $\xi$~--- случайная величина, а $g: \mathbb{R} \rightarrow \mathbb{R}$~--- борелевская функция. Тогда $g(\xi)$~--- случайная величина.
\end{thm}

\begin{proof}
    Проверим, что прообраз любого борелевского множества при отображении $g(\xi) : \Omega \rightarrow \mathbb{R}$ является событием. Возьмём произвольное $B \in \mathcal{B}(\mathbb{R})$ и положим $B_1 = g^{-1}(B)$. Множество $B_1$~--- борелевское, так как функция $g$ борелевская. Но тогда $(g(\xi))^{-1}(B) = \xi^{-1}(B_1)$. Это множество принадлежит $\mathcal{F}$, поскольку $B_1$~--- борелевское множество и $\xi$~--- случайная величина. 
\end{proof}

\begin{thm}
    Пусть $\mathcal{E}$ - класс подмножеств вещественной прямой, $\sigma(\mathcal{E}) = \mathcal{B}$ (например, множество интервалов).

    Тогда $\xi$~--- случайная величина $\Leftrightarrow$ $\forall E \in \mathcal{E}: ~\xi^{-1}(E) \in \mathcal{A}$.
\end{thm}

\begin{proof}
    $\Leftarrow$) Пусть $\mathcal{D} = \{D \,|\, D \in \mathcal{B}, \xi^{-1}(D) \in \mathcal{A} \}$. Тогда $\mathcal{E} \subseteq \mathcal{D}$. Далее, в силу свойств прообразов и случайной величины $\xi$:
    \begin{gather*}
        \xi^{-1}(\bigcup\limits_\alpha A_\alpha) = \bigcup\limits_\alpha \xi^{-1}(A_\alpha) \\
        \xi^{-1}(\overline{A}) = \overline{\xi^{-1}(A)} \\
        \xi^{-1}(\bigcap\limits_\alpha A_\alpha) = \bigcap\limits_\alpha \xi^{-1}(A_\alpha) \\
    \end{gather*}
    $ \Rightarrow \mathcal{D}$~--- $\sigma$-алгебра. Следовательно, т.к. $\mathcal{B} = \sigma(\mathcal{E}) \subseteq \sigma(\mathcal{D}) = \mathcal{D} \subseteq \mathcal{B}.$, то $\mathcal{B} = \mathcal{D}.$
    
    $\Rightarrow)$ Следует непосредственно из определения случайной величины.
\end{proof}

\begin{crlr}
    $\xi$~--- случайная величина $\Leftrightarrow$ $\forall x \in \mathbb{R}:~ \{\omega \,|\, \xi(\omega) < x \} \in \mathcal{A}$. Причем вместо знака $<$ может стоять любой другой знак неравенства, как строгого, так и нестрогого.
\end{crlr}

\subsubsection{Порождённое и индуцированное вероятностные пространства}
\begin{symb}
    \begin{equation*}
        \mathcal{F}_\xi = \{ \xi^{-1}(\omega), B \in \mathcal{B} \}
    \end{equation*}
\end{symb}

Отметим следующие факты:
\begin{enumerate}
    \item $\mathcal{F}_\xi \subset \mathcal{F}.$
    \item $\mathcal{F}_\xi$~--- $\sigma$-алгебра. Действительно
    \begin{gather*}
        \xi^{-1}(\overline{B}) = \overline{\xi^{-1}(B)} \\
        \xi^{-1}\left(\bigcup\limits_{i=1}^{\infty}B_i\right) = \bigcup\limits_{i=1}^\infty \xi^{-1}(B_i),
    \end{gather*}
   если $B_i$ попарно не пересекаются.
\end{enumerate}

\begin{defn}
    Вероятностное пространство $(\Omega,\mathcal{F}_\xi,\mathbb{P})$ называется {\it порожденным случайной величиной $\xi$}.
\end{defn}

\begin{defn}
    Вероятностное пространство $(\mathbb{R}, \mathcal{B}, \mathbb{P}_\xi)$ называется {\it индуцированным случайной величиной $\xi$}.
\end{defn}

\begin{defn}
    {\it Распределение случайной величины} $\xi$~--- функция $\mathbb{P}_\xi: \mathcal{B} \rightarrow \mathbb{R}$ такая, что
    \begin{equation*}
        \myprob{B}_\xi = \myprob{\xi^{-1}(B)} = \myprob{\xi \in B}
    \end{equation*}
\end{defn}
\subsubsection{Функция распределения, её свойства}
\begin{defn}
    {\it Функция распределения} $F_\xi (x)$ случайной величины $\xi$~--- функция $F_\xi: \mathbb{R} \rightarrow \mathbb{R}$ такая, что
    \begin{equation*}
        F_{\xi}(x)=\mathbb{P}_{\xi}((-\infty, x))=\mathbb{P}(\xi<x)
    \end{equation*}
\end{defn}

\begin{thm}
    $F_\xi(x)$ однозначно определяет $\mathbb{P}_\xi(B)$.
\end{thm}
\begin{proof}
    Действительно, любое борелевское множество может быть представлено в виде разности числовой оси, одной или двух полупрямых и не более чем счётного объединения отрезков. В силу однозначности определения $\mathbb{P}_\xi([a,b]) = F_\xi(b + 0) - F_\xi(a)$ утверждение теоремы справедливо.
\end{proof}

\begin{thm}
    Свойства функции распределения:
\begin{enumerate}
    \item $\forall x~ 0 \leqslant F_\xi(x) \leqslant 1$;
    \item $F_\xi(x)$ --- монотонно убывает (т.е. для $x_1 < x_2 \Rightarrow F_\xi(x_1) \leqslant F_\xi(x_2))$;
    \item $\lim\limits_{x \rightarrow +\infty} F_\xi(x) = 1, \lim\limits_{x \rightarrow -\infty} F_\xi(x) = 0$;
    \item $F_\xi(x)$ непрерывна слева (т.е. $F_\xi(x_0 - 0) = \lim\limits_{x \rightarrow x_0 - 0}F_\xi(x) = F_\xi(x_0)$).
\end{enumerate}
\end{thm}

\begin{proof}
\begin{enumerate}
    \item Следует из свойств вероятности.
    \item 
        $x_1 < x_2 \Rightarrow \{ \xi < x_1 \} \subseteq \{ \xi < x_2\}$; из монотонности вероятности $\myprob{\xi < x_1} = F_\xi(x_1) \leqslant \myprob{\xi < x_2} = F_\xi(x_2)$.
    \item 
        Пределы существуют в силу монотонности и ограниченности $F_\xi(x)$. Докажем, что $F_\xi(-n) \xrightarrow[n \rightarrow +\infty]{} 0$.
        
        Рассмотрим последовательность вложенных событий $B_n = \{ \xi < -n \}$, 
        $B_{n+1} = \{\xi < -(n+1) \} \subseteq B_n = \{ \xi < -n \} ~ \forall n ~ \geq 1.$
        \begin{equation*}
            \bigcap\limits_{j = 1}^{\infty}B_j = \{ \omega | \xi(\omega) < x, \forall x \in \mathbb{R} \} \Rightarrow \bigcap\limits_{j = 1}^{\infty}B_j = \varnothing
        \end{equation*}
    
        $F_\xi(-n) = \myprob{B_n} \xrightarrow[n \rightarrow +\infty]{} \myprob{B} = 0$ (в силу непрерывности меры)
    
        Теперь докажем, что $F_\xi(n) \xrightarrow[n \rightarrow +\infty]{} 1 \Leftrightarrow 1 - F_\xi(n) = \myprob{\xi \geq n} \xrightarrow[n \rightarrow +\infty]{} 0$.
    
        Аналогично, пусть $B_n = \{\xi \geq n\}; B_{n+1} = \{ \xi \geq (n+1) \} \subseteq B_n, ...$
        \begin{equation*}
            \bigcap\limits_{j = 1}^{\infty}B_j = \varnothing \Rightarrow \xi(w) > x~\forall x \in \mathbb{R}
        \end{equation*}
        Следовательно, 
        \begin{equation*}
            1 - F_\xi(n) = \myprob{B_n} \xrightarrow[n \rightarrow +\infty]{} \myprob{B} = 0 \Rightarrow F_\xi(n) \xrightarrow[n \rightarrow +\infty]{}1
        \end{equation*}
    \item
        Докажем, что $F_\xi(x_0 - \frac{1}{n}) \xrightarrow[n \rightarrow +\infty]{} F_\xi(x_0)$, что равносильно 
        \begin{multline*}
            F_\xi(x_0) - F_\xi \left(x_0 - \frac{1}{n} \right) 
            = \myprob{\xi < x_0} - \myprob{\xi < x_0 - \frac{1}{n}} = \\ 
            = \myprob{x_0 - \frac{1}{n} \leqslant \xi \leqslant x_0} \xrightarrow[n \to +\infty]{} 0
                \end{multline*}
        Сходимость выполняется в силу непрерывности вероятности.
\end{enumerate}
\end{proof}

\section{Дискретные, сингулярные и абсолютно непрерывные функции распределения и случайные величины. Плотность распределения. Теорема Лебега о разложении функции распределения}

\begin{defn}
    Распределение $\xi$ называется {\it дискретным}, если существует не более чем счётное множество $B$, т.ч. $\mathbb{P}_\xi(B) = 1$. Дискретная функция распределения имеет вид:
    \begin{equation*}
        F_\xi(x) = \mathbb{P}(\xi \leqslant x) = \sum_{x_i \leqslant x}{}p_{i} = \sum_{x_i \leqslant x}{}\mathbb{P}(\xi = x_{i})
    \end{equation*}
\end{defn}

\begin{rmrk}
    Для любой дискретной функции распределения $F_\xi(x)$ число скачков~--- не более чем счётное.
    
    Действительно, можно перенумеровать все скачки следующим образом:
    \begin{equation*}
        \Delta_{n}=\left\{t \,|\, F_{x}(t+0)-F_{x}(t)>\frac{1}{n}\right\},~ |\Delta_{n} | \leqslant n
    \end{equation*}
    Множество точек разрыва представимо в виде $\bigcup\limits_{n} \Delta_{n}$, т.е. не более чем счётно.
\end{rmrk}

\begin{defn}
    Распределение $\xi$ называется {\it абсолютно непрерывным}, если существует $f(x) \overset{\text{п.н.}}{\geqslant} 0$ такая, что для любого борелевского множества $B$ справедливо
    \begin{equation*}
        \mathbb{P}_\xi(B) = \int\limits_B f(x) \lambda(dx),
    \end{equation*}
    где $f(x)$~--- {\it плотность распределения}, $\lambda$~--- мера Лебега. Абсолютно непрерывная функция распределения имеет вид:
    \begin{equation*}
        F_\xi(x) = \mathbb{P}(\xi \leqslant x) = \int\limits_{-\infty}^x f(t)dt
    \end{equation*}
\end{defn}
\begin{rmrk}
    В случае абсолютно непрерывного распределения вероятность попасть в конкретную точку равна нулю. Действительно,
    \begin{equation*}
        \mathbb{P}(\xi = x) = \int\limits_{x}^{x} f(t)dt = 0
    \end{equation*}
\end{rmrk}
\begin{thm}
Свойства плотности:
\begin{enumerate}
    \item $f(x) = \cfrac{\partial F(x)}{\partial x}$
    \item $f_\xi(x) \geqslant 0~ \forall x$ (свойство неотрицательности);
    \item $\int\limits_{-\infty}^{+\infty} f_\xi(t) dt = 1$ (свойство нормировки).
\end{enumerate}
\end{thm}
\begin{proof}
    Первое свойство очевидно из свойств интегралов с переменным верхним пределом, второе выполнено в силу определения плотности распределения. Рассмотрим третье. Если в определении абсолютно непрерывного распределения в качестве борелевского множества взять всю числовую прямую, получим: 
    \begin{equation*}
        \mathrm{P}(\xi \in \mathbb{R})=1=\int\limits_{\mathbb{R}} f_{\xi}(x) dx
    \end{equation*}
\end{proof}

\mydef{} {\it Точкой роста} функции распределения $F_\xi(x)$ назовём такую точку $x_0$, что
\begin{equation*}
    \forall \varepsilon > 0~ F_\xi(x_0 + \varepsilon) - F_\xi(x_0 - \varepsilon) > 0
\end{equation*}

\begin{rmrk}
    Возможен случай, когда точка роста является точкой разрыва:
    \begin{multline*}
    \lim _{\varepsilon \to 0} F_{\xi}\left(x_{0}+\varepsilon\right)-F_{\xi}\left(x_{0}-\varepsilon\right)=F_{\xi}\left(x_{0}+0\right)-F_{\xi}\left(x_{0}\right)>0 \Leftrightarrow \\
    \Leftrightarrow \mathbb{P}\left(x_{0}-\varepsilon \leqslant \xi<x_{0}+\varepsilon\right)=\mathbb{P}\left(\xi=x_{0}\right)>0
    \end{multline*}
\end{rmrk}

\begin{defn}
    Функция распределения $F_\xi(x)$ называется {\it сингулярной}, если она непрерывна и множество точек её роста имеет нулевую меру Лебега.
\end{defn}

\begin{thm}[Теорема Лебега о разложении функции распределения]
    Пусть $\xi$~--- случайная величина с функцией распределения $F_\xi(x).$ Тогда существуют и единственны три функции $F_{ac}(x), F_s(x), F_d(x)$, соответственно абсолютно непрерывная, сингулярная и дискретная функции распределения, три числа $p_1, p_2, p_3 \geq 0, p_1 + p_2 + p_3 = 1$ такие, что 
    \begin{equation*}
        F_{\xi}(x)=p_{1} F_{ac}(x)+p_{2} F_{s}(x)+p_{3} F_{d}(x)
    \end{equation*}
\end{thm}

\section{Числовые характеристики случайных величин: моменты, математическое ожидание, дисперсия. Их свойства.}

\subsubsection{Математическое ожидание случайной величины}

\mydef{} {\it Математическим ожиданием (средним значением, первым моментом)} случайной величины $\xi$, имеющей дискретное распределение со значениями $a_1, a_2, ...$, называется число
$$\mathbb{E} \xi=\sum_{i} a_{i} p_{i}=\sum_{i} a_{i} \mathrm{P}\left(\xi=a_{i}\right),$$
если данный ряд абсолютно сходится, т.е. если $\sum |a_i|p_i < \infty.$ В противном случае говорят, что математическое ожидание не существует.

\mydef{} {\it Математическим ожиданием} случайной величины $\xi$, имеющей абсолютно непрерывное распределение с плотностью распределения $f(x)$, называется число
$$\mathbb{E} \xi=\int\limits_{-\infty}^{\infty} x f(x) dx,$$
если этот интеграл абсолютно сходится, т.е. если $\int\limits_{-\infty}^{+\infty}|x|f(x)dx < \infty.$ Иначе математическое ожидание не существует.

\subsubsection{Свойства математического ожидания}
\mynote{} Везде далее предполагается, что рассматриваемые математические ожидания существуют.
\begin{enumerate}
    \item Для произвольной функции $g(x)$ со значениями в $\mathbb{R}$
    $$\mathbb{E} g(\xi)=\left\{\begin{array}{l}\sum\limits_{k} g\left(a_{k}\right) \mathrm{P}\left(\xi=a_{k}\right), \text { если распределение } \xi \text { дискретно; } \\ \int\limits_{-\infty}^{+\infty} g(x) f_{\xi}(x) d x, \text { если распределение } \xi \text { абсолютно непрерывно. }\end{array}\right.$$
    
    Такое же свойство верно и для числовых функций нескольких аргументов $g(x_1, ...,x_n)$, если $\xi$ - вектор из $n$ случайных величин, а в сумме и в интеграле участвует их совместное распределение. Например, для $g(x,y) = x + y$ и для случайных величин $\xi$ и $\eta$ с плотностью совместного распределения $f(x,y)$ верно: 
    \begin{equation}
        \mathbb{E}(\xi+\eta)=\int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty}(x+y) f(x, y) d x d y
    \end{equation}
    
    \item Математическое ожидание постоянной равно ей самой: $\mathbb{E} c=c.$
    \item Постоянный множитель можно вынести за знак математического ожидания: $\mathbb{E}(c\xi) = c\mathbb{E}\xi.$
    
    Это следует из свойства 1. при $g(x) = cx$.
    \item Математическое ожидание суммы {\it любых} случайных величин равно сумме их математических ожиданий: $\mathbb{E}(\xi + \eta) = \mathbb{E}\xi + \mathbb{E}\eta.$
    
    \myqed{} Воспользуемся равенством (1) и теоремой о совместном распределении:
    $$\begin{aligned}
    \mathbb{E}(\xi+\eta) &=\int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty}(x+y) f(x, y) d x d y=\\
    &=\int\limits_{-\infty}^{\infty} x d x \int\limits_{-\infty}^{\infty} f(x, y) d y+\int\limits_{-\infty}^{\infty} y d y \int\limits_{-\infty}^{\infty} f(x, y) d x=\\
    &=\int\limits_{-\infty}^{\infty} x f_{\xi}(x) d x+\int\limits_{-\infty}^{\infty} y f_{\eta}(y) d y=\mathbb{E} \xi+\mathbb{E} \eta ~~~ \square.
    \end{aligned}$$
    \item Если $\xi \geq 0$, то $\mathbb{E}\xi \geq 0.$
    
    \myqed{} Неотрицательность $\xi$ означает, что $a_i \geq 0$ при всех $i$ в случае дискретного распределения, либо $f_\xi(x) = 0$ при $x < 0$ - для абсолютно непрерывного распределения. И в том, и в другом случае имеем:
    $$\mathbb{E} \xi=\sum a_{i} p_{i} \geqslant 0 \quad \text { или } \quad \mathbb{E} \xi=\int\limits_{0}^{\infty} x f(x) d x \geqslant 0 ~~~ \square.$$
    
    \mycon{} Если $\xi \leqslant \eta$, то $\mathbb{E}\xi \leqslant \mathbb{E}\eta.$
    
    \mycon{} Если $a \leqslant \xi \leqslant b$, то $a \leqslant \mathbb{E}\xi \leqslant b$.
    
    \item Математическое ожидание произведения {\it независимых} случайных величин равно произведению их математических ожиданий.
    
    \myqed{} В равенстве (1) заменим сложение умножением и плотность совместного распределениея произведением плотностей (это возможно в силу независимости случайных величин):
    $$\begin{aligned}
    \mathbb{E}(\xi \eta) &=\int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} x y f_{\xi}(x) f_{\eta}(y) d x d y=\\
    &=\int\limits_{-\infty}^{\infty} x f_{\xi}(x) d x \int\limits_{-\infty}^{\infty} y f_{\eta}(y) d y=\mathbb{E} \xi \mathbb{E} \eta ~~~ \square.
    \end{aligned}$$
    
    \mynote{} Обратное неверно: из равенства $\mathbb{E}(\xi \eta) = \mathbb{E}\xi \mathbb{E} \eta$ {\it не следует} независимость величин $\xi$ и $\eta$.
\end{enumerate}

\subsubsection{Дисперсия и моменты старших порядков}

\mydef{}. Пусть $\mathbb{E}|\xi|^k < \infty.$ Число $\mathbb{E}\xi^k$ называется {\it моментом порядка $k$ или $k$-м моментом} случайной величины $\xi$, число $\mathbb{E}|\xi|^k$ - {\it абсолютным $k$-м моментом}, число $\mathbb{E}(\xi - \mathbb{E}\xi)^k$ - {\it центральным $k$-м моментом}, и число $\mathbb{E}|\xi - \mathbb{E}\xi|^k$ - {\it абсолютным центральным $k$-м моментом} случайной величины $\xi$. 

\mydef{} Число $\mathrm{D}\xi = \mathbb{E}(\xi - \mathbb{E}\xi)^2$ (центральный момент второго порядка) называется {\it дисперсией} случайной величины $\xi$.

\mydef{} Число $\sigma = \sqrt{\mathrm{D}\xi}$ называют {\it среднеквадратичным отклонением} случайной величины $\xi$.

\myth{} Если существует момент порядка $t > 0$ случайной величины $\xi$, то существует и ее момент порядка $s$, где $0 < s < t$.

\myqed{} Заметим, что $|\xi|^s \leqslant |\xi|^t + 1.$ В силу следствия из свойства 5 для математического ожидания можно получить из неравенства для случайных величин такое же неравенство для их математических ожиданий: $\mathbb{E}|\xi|^s \leqslant \mathbb{E}|\xi|^t + 1 < \infty. ~~~ \square$

\myth{} \textbf{Неравенство Йенсена}. (без доказательства)

Пусть функция $g(x)$ на своей области определения выпукла, т.е. область над графиком этой функции есть выпуклое множество. Тогда для любой случайно величины $\xi$ верно неравенство: 
$$\mathbb{E}g(\xi) \geq g(\mathbb{E}\xi).$$
Для вогнутых функций знак неравенства меняется на противоположный.

В частности,
$$\begin{array}{ccc}
\mathbb{E} e^{\xi} \geqslant e^{\mathbb{E} \xi}, & \mathbb{E} \xi^{2} \geqslant(\mathbb{E} \xi)^{2}, & \mathbb{E}|\xi| \geqslant|\mathbb{E} \xi| \\
\mathbb{E} \ln \xi \leqslant \ln (\mathbb{E} \xi), & \mathbb{E} \frac{1}{\xi} \geqslant \frac{1}{\mathbb{E} \xi}, & \mathbb{E} \sqrt{\xi} \leqslant \sqrt{\mathbb{E} \xi}
\end{array}$$

Последние три неравенства верны для положительных $\xi$.

\subsubsection{Свойства дисперсии}

\mynote{} Во всех свойствах предполагается существование вторых моментов случайных величин. Тогда (в силу вышеописанной теоремы) существуют и сами матожидания.

\begin{enumerate}
    \item Дисперсия может быть вычислена по формуле: $\mathrm{D}\xi = \mathbb{E}\xi^2 - (\mathbb{E}\xi)^2$.
    
    \myqed{} Обозначим для удобства $a = \mathbb{E}\xi.$ Тогда
    $$\mathrm{D} \xi=\mathbb{E}(\xi-a)^{2}=\mathbb{E}\left(\xi^{2}-2 a \xi+a^{2}\right)=\mathbb{E} \xi^{2}-2 a \mathbb{E} \xi+a^{2}=\mathbb{E} \xi^{2}-a^{2}. ~~~ \square$$
    
    \item При умножении случайной величины на постоянную $c$ дисперсия увеличивается в $c^2$ раз: $\mathrm{D}(c\xi) = c^2\mathrm{D}\xi.$
    \item Дисперсия всегда неотрицательна: $\mathrm{D}\xi \geq 0.$
    
    \myqed{} Пусть $a = \mathbb{E}\xi.$ Дисперсия есть математичекое ожидание неотрицательной случайной величины $(\xi - a)^2$, откуда (и из свойства 5 матожидания) следует неотрицательность дисперсии. $ ~~~ \square$
    
    \item Дисперсия обращается в нуль лишь для вырожденного распределения: если $\mathrm{D}\xi = 0$, то $\xi = const$, и наоборот.
    
    \myqed{} $\mathrm{D}\xi = 0 \Rightarrow (\xi - a)^2 = 0, \xi = a = const.$ И наоборот: если $\xi = c$, то $\mathrm{D} \xi=\mathbb{E}(c-\mathbb{E} c)^{2}=\mathbb{E} 0=0. ~~~ \square$
    
    \item Дисперсия не зависит от сдвига случайной величины на постоянную: $\mathrm{D}(\xi + c) = \mathrm{D}\xi.$
    
    \item Если $\xi$ и $\eta$ независимы, то $\mathrm{D}(\xi + \eta) = \mathrm{D}\xi + \mathrm{D}\eta.$
    
    \myqed{} Действительно, применяя свойство (6) матожидания, получим:
    $$\begin{aligned}
    \mathrm{D}(\xi+\eta) &=\mathbb{E}(\xi+\eta)^{2}-(\mathbb{E}(\xi+\eta))^{2}=\\
    &=\mathbb{E} \xi^{2}+\mathbb{E} \eta^{2}+2 \mathbb{E}(\xi \eta)-(\mathbb{E} \xi)^{2}-(\mathbb{E} \eta)^{2}-2 \mathbb{E} \xi \mathbb{E} \eta=\mathrm{D} \xi+\mathrm{D} \eta
    \end{aligned}. ~~~ \square$$
    
    \mynote{} Обратное, аналогично замечанию к свойству (6) матожидания, неверно.
    
    \mycon{} Если $\xi$ и $\eta$ независимы, то $\mathrm{D}(\xi-\eta)=\mathrm{D} \xi+\mathrm{D} \eta$. 
    
    \myqed{} Из свойств (6) и (2) получим: 
    $$\mathrm{D}(\xi-\eta)=\mathrm{D}(\xi+(-\eta))=\mathrm{D} \xi+\mathrm{D}(-\eta)=\mathrm{D} \xi+(-1)^{2} \mathrm{D} \eta=\mathrm{D} \xi+\mathrm{D} \eta. ~~~ \square$$
    
    \mycon{} Для произвольных случайных величин $\xi$ и $\eta$ имеет место равенство:
    $$\mathrm{D}(\xi \pm \eta)=\mathrm{D} \xi+\mathrm{D} \eta \pm 2(\mathbb{E}(\xi \eta)-\mathbb{E} \xi \mathbb{E} \eta).$$
    
\end{enumerate}

\section {Числовые характеристики случайных величин: квантили. Медиана и ее свойства. Интерквартильный размах.}

\mydef{} {\it Медианой} распределения случайной величины $\xi$ называется любое из {\it чисел} $\mu$ таких, что
$$\myprob{\xi \leqslant \mu} \geqslant \frac{1}{2}, ~~~ \myprob{\xi \geqslant \mu} \geqslant \frac{1}{2}.$$

\mynote{} Медиана распределения всегда существует, но может быть не единственна.

\mydef{} Пусть, для простоты, функция распределения $F$ непрерывна и строго монотонна. Тогда {\it квантилью} уровня $\delta$, где $\delta \in (0, 1), $ называется решение $x_\delta$ уравнения $F(x_\delta) = \delta.$ 

Квантиль уровня $\delta$ отрезает от области под графиком плотности область с площадью с площадью $\delta$ слева от себя. Справа от $x_\delta$ площадь области равна $1 - \delta$.

\mydef{} Квантили уровней, кратных $0,01$, называют {\it процентилями}, квантили уровней, кратных $0,1$, — {\it децилями}, уровней, кратных $0,25$, — {\it квартилями}.

\mynote{} Медиана является одной из квантилей распределения, уровня $\frac{1}{2}.$

\subsubsection{Свойства медианы}

Экстремальные свойства:

\begin{enumerate}
    \item Медиана $\mathrm{Med}~\xi$ случайной величины $\xi$ минимизирует средний модуль её отклонения:
    $$\mathbb{E}|\xi-\operatorname{Med} \xi|=\min _{a} \mathbb{E}|\xi-a|$$
    
    \myqed{}
    $$\mathbb{E}|\xi - a| - \mathbb{E}|\xi| = a\myprob{\xi \leqslant a} - a\myprob{\xi > a} - 2\mathbb{E}(\xi; 0 < \xi \leqslant a) \geqslant$$ $$\geqslant a\myprob{\xi \leqslant a} - a\myprob{\xi > a} - 2a\myprob{0 < \xi \leqslant a} = a(\myprob{\xi \leqslant 0} - \myprob{\xi > 0}) \geqslant 0. ~~~\square $$
\end{enumerate}

\subsubsection{Интерквантильный размах}

\mydef{} {\it Интерквартильным размахом} называется разность между третьим и первым квартилями, то есть ${\displaystyle x_{0{,}75}-x_{0{,}25}}.$

\section {Испытания Бернулли. Биномиальное распределение. Теорема Пуассона. Распределение Пуассона.}

\mydef{} {\it Схемой Бернулли} называется последовательность независимых испытаний, в каждом из которых возможны лишь два исхода - <<успех>> и <<неудача>>, при этом успех в каждом испытании происходит с одной и той же вероятностью $p \in (0,1)$, а неудача - с вероятностью $q = 1 - p.$

\mydes{} $\mathrm{v}_n$ - число успехов, случившихсся в $n$ испытаниях схемы Бернулли. 

\myth{} (формула Бернулли). Для любого $k = 0, 1, ..., n$ вероятность получить в $n$ испытаниях ровно $k$ успехов равна
\begin{equation}
    \mathbb{P}\left(\mathrm{v}_{n}=k\right)=C_{n}^{k} p^{k} q^{n-k}.
\end{equation}

\mydef{} Набор вероятностей (2) называется {\it биномиальным распределением} вероятностей.

\myqed{} Событие $A = \{ \mathrm{v}_n = k \}$ означает, что в $n$ испытаниях схемы Бернулли произошло ровно $k$ успехов. Рассмотрим один элементарный исход из события $A$:
$$(\underbrace{y, y, \ldots, y}_{k}, \underbrace{\text{\it н}, \text{\it н}, \ldots,\text{\it н}}_{n-k}),$$
когда первые $k$ испытаний завершились успехом (у), остальные неудачей (н). Поскольку испытания независимы, вероятность такого элементарного исхода равна $p^k(1 - p)^{n-k}.$ Другие элементарные исходы из события $A$ отличаются лишь расположением $k$ успехов на $n$ местах. Поэтому событие $A$ состоит из $C_n^k$ элементарых исходов, вероятность каждого из которых равна $p^kq^{n-k}.~~~ \square$

\myth {} \textbf{(Пуассона)}. Пусть проводится $n$ обобщённых испытаний Бернулли
(то есть, вероятность успеха испытания зависит от $n$) с вероятностью успеха
$p_n$, $S_n$ — количество успехов в этих испытаниях и $n p_{n} \underset{n \to \infty}{\longrightarrow} a$. Тогда

$$\forall k \in \mathbb{Z}: 0 \leqslant k \leqslant n \quad \mathrm{P}\left(S_{n}=k\right) \underset{n \to \infty}{\longrightarrow} \frac{a^{k}}{k !} e^{-a}.$$

\mydef{} Набор вероятностей $\{ \frac{\lambda^k}{k!} e^{-\lambda} \}$, где $k$ принимает значения $0, 1, 2, ...$, называется {\it распределением Пуассона} с параметром $\lambda > 0$.

\myqed{} 

$$\begin{aligned}
\mathrm{P}\left(S_{n}=k\right) &=C_{n}^{k} p_{n}^{k}\left(1-p_{n}\right)^{n-k}=\frac{p_{n}^{k} n !}{k !(n-k) !}\left(1-p_{n}\right)^{n}\left(1-p_{n}\right)^{-k} \\
&=\frac{\left(n p_{n}\right)^{k}}{k !} \cdot \frac{n !}{(n-k) ! n^{k}} \cdot \frac{\left(1-p_{n}\right)^{n}}{\left(1-p_{n}\right)^{k}}
\end{aligned},$$
$k$ - константа. Поэтому
$$\lim\limits_{n \to \infty} \frac{\left(n p_{n}\right)^{k}}{k !}=\frac{a^{k}}{k !}.$$
Далее,
$$\lim _{n \to \infty} \frac{n !}{(n-k) ! n^{k}}=\lim _{n \to \infty}(1)\left(1-\frac{1}{n}\right)\left(1-\frac{2}{n}\right) \cdot \ldots \cdot\left(1-\frac{k-2}{n}\right)\left(1-\frac{k-1}{n}\right) =$$
$$=\lim _{n \to \infty}\left(1-\frac{1}{n}\right) \lim _{n \to \infty}\left(1-\frac{2}{n}\right) \cdot \ldots \cdot \lim _{n \to \infty}\left(1-\frac{k-2}{n}\right) \lim _{n \to \infty}\left(1-\frac{k-1}{n}\right)=1.$$
При выяснении предела третьей дроби требуется второй замечательный предел и
непрерывность показательной функции. Кроме того, отметим, что раз $n p_{n} \underset{n \to \infty}{\longrightarrow} a$, то $p_{n} \underset{n \to \infty}{\longrightarrow} 0$. Таким образом
$$\begin{aligned}
\lim _{n \to \infty} \frac{\left(1-p_{n}\right)^{n}}{\left(1-p_{n}\right)^{k}} &=\frac{\lim _{n \to \infty}\left(1-p_{n}\right)^{n}}{\lim _{n \to \infty}\left(1-p_{n}\right)^{k}}=\frac{\lim _{n \to \infty}\left(1-p_{n}\right)^{n}}{1}=\lim _{n \to \infty}\left(1-p_{n}\right)^{\frac{1}{p_{n}} n p_{n}} \\
&=\lim _{n \to \infty}\left(\left(1-p_{n}\right)^{-\frac{1}{p_{n}}}\right)^{-n p_{n}}=\lim _{n \to \infty}\left(\left(1-p_{n}\right)^{-\frac{1}{p_{n}}}\right)^{a-n p_{n}-a} \\
&=\lim _{n \to \infty}\left(\left(1-p_{n}\right)^{-\frac{1}{p_{n}}}\right)^{-a}\left(\left(1-p_{n}\right)^{-\frac{1}{p_{n}}}\right)^{a-n p_{n}} \\
&=\lim _{n \to \infty}\left(\left(1-p_{n}\right)^{-\frac{1}{p_{n}}}\right)^{-a} \lim _{n \to \infty}\left(\left(1-p_{n}\right)^{-\frac{1}{p_{n}}}\right)^{a-n p_{n}}
\end{aligned}$$
Последовательность $a - np_n$ является бесконечно малой, обозначим её как $\gamma_n.$ Вторая скобка:
$$\lim _{n \to \infty}\left(\left(1-p_{n}\right)^{-\frac{1}{p_{n}}}\right)^{\gamma_{n}}=\lim _{n \to \infty} e^{\gamma_{n} \frac{\ln \left(1-p_{n}\right)}{p_{n}}}=e^{\lim\limits _{n \to \infty} \gamma_{n} \frac{\ln \left(1-p_{n}\right)}{p_{n}}}=e^{\lim\limits _{n \to \infty} \gamma_{n} \lim\limits_{n \to \infty} \frac{\ln \left(1-p_{n}\right)}{p_{n}}}=e^{0 \cdot 1}=1.$$
Первая скобка:
$$\lim _{n \to \infty}\left(\left(1-p_{n}\right)^{-\frac{1}{p_{n}}}\right)^{-a}=\left(\lim _{n \to \infty}\left(1-p_{n}\right)^{-\frac{1}{p_{n}}}\right)^{-a}=e^{-a}.$$
Собирая всё вышеполученное:
$$\lim _{n \to \infty} \mathrm{P}\left(S_{n}=k\right)=\lim _{n \to \infty} \frac{\left(n p_{n}\right)^{k}}{k !} \cdot \frac{n !}{(n-k) ! n^{k}} \cdot \frac{\left(1-p_{n}\right)^{n}}{\left(1-p_{n}\right)^{k}} = $$
$$=\lim _{n \to \infty} \frac{\left(n p_{n}\right)^{k}}{k !} \cdot \lim _{n \to \infty} \frac{n !}{(n-k) ! n^{k}} \cdot \lim _{n \to \infty} \frac{\left(1-p_{n}\right)^{n}}{\left(1-p_{n}\right)^{k}}=\frac{a^{k}}{k !} \cdot 1 \cdot e^{-a}=\frac{a^{k}}{k !} e^{-a} \cdot \square$$

\section {Испытания Бернулли. Геометрическое распределение. Теорема Реньи. Показательное распределение.}

\myth{} Вероятность того, что первый успех произойдёт в испытании с номером $k \in \mathbb{N} = \{1, 2, 3, ...\}$, равна $\myprob{\tau = k} = pq^{k-1}$.

\myqed{} Вероятность первым $k - 1$ испытаниям завершиться неудачей, а последнему - успехом, равна 
$$\mathrm{P}(\tau=k)=\mathrm{P}(\mu, \ldots, \mu, y)=p q^{k-1}. ~~~ \square$$

\mydef{} Набор вероятностей $\{p q^{k-1}\}$, где $k$ принимает любые значения из множества натуральных чисел, называется {\it геометрическим распределением} вероятностей. 

\mydef{} Случайная величина $\xi$ имеет {\it показательное (экспоненциальное) распределение} с параметром $\alpha > 0$, если $\xi$ имеет следующие плотность и функцию распределения:
$$f(x)=\left\{\begin{array}{cc}
0, & \text { если } x<0 \\
\alpha e^{-\alpha x}, & \text { если } x \geqslant 0 ;
\end{array} \quad F(x)=\left\{\begin{array}{cc}
0, & \text { если } x<0 \\
1-e^{-\alpha x}, & \text { если } x \geqslant 0
\end{array}\right.\right.$$

\myth{} \textbf{(Реньи)} (без доказательства). Пусть случайная величина $N$ имеет геометрическое распределение. $X_1, X_2, ...$ - независимые одинаково распределённые случайные величины, $X_i \geqslant 0$ и $0 < a = \mathbb{E}X < \infty$, $S_n = \sum\limits_{i=1}^N X_i.$

Тогда $$\sup _{x}\left|P\left(\frac{p}{a} S_{N}<x\right)-G(x)\right| \underset{p \rightarrow 0}{\longrightarrow} 0,$$
где $G(x)$ - функция стандартного показательного распределения.

Если $b^2 = \mathbb{E}X_i^2$, тогда $$\sup_{x}\left|P\left(\frac{p}{a} S_{N}<x\right)-G(x)\right| \leqslant \frac{p b^{2}}{(1-p) a^{2}}.$$
 
\section{Испытания Бернулли. Теорема Муавра—Лапласа. Нормальное распределение.}

\myth{} \textbf{(Локальная предельная теорема Муавра-Лапласа.)} (без доказательства) Пусть $S_n$ - число успехов в $n$ испытаниях Бернулли с вероятностью успеха $p$. Если $n p(1-p) \underset{n \to \infty}{\longrightarrow} \infty$, то
$$\forall m \in \mathbb{Z}: 0 \leqslant m \leqslant n \quad \mathrm{P}\left(S_{n}=m\right)=\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{x^{2}}{2}}\left(1+\underline{O}\left(\frac{1}{\sigma}\right)\right),$$
где $x = \frac{m - np}{\sigma},$ а $\sigma=\sqrt{\mathrm{D} S_{n}}=\sqrt{n p(1-p)}$.

\myth{} \textbf{(Интегральная теорема Муавра-Лапласа)} Если выполнено условие локальной теоремы и $C$ - произвольная положительная константа, то равномерно по $a$ и $b$ из отрезка $[-C,C]$ (пусть $b \geqslant a$)
$$\mathrm{P}\left(a \leqslant \frac{S_{n}-n p}{\sqrt{n p(1-p)}} \leqslant b\right) \underset{n \to \infty}{\longrightarrow} \frac{1}{\sqrt{2 \pi}} \int\limits_{a}^{b} e^{-\frac{x^{2}}{2}} d x$$

\mynote{} Утверждение верно $\forall a,b \in \mathbb{R}$. Ограничение требуется для простоты доказательства.

\myqed{} Обозначим за $q$ величину $1 - p$ и проведём небольшие выкладки с использованием локальной предельной теоремы в переходе, помеченном $\triangle$. В переходе $*$ - перенос знака.

$$\mathrm{P}\left(a \leqslant \frac{S_{n}-n p}{\sqrt{n p(1-p)}} \leqslant b\right)=\mathrm{P}\left(n p+a \sqrt{n p q} \leqslant S_{n} \leqslant n p+b \sqrt{n p q}\right) \stackrel{*}{=}$$

Пусть множество $M$ - это все целые числа $m$, такие что выполнено неравенство в последней вероятности. То есть,

$$\begin{array}{c}
M=\{m: n p+a \sqrt{n p q} \leqslant m \leqslant n p+b \sqrt{n p q}\} \Rightarrow \\
\stackrel{*}{=} \sum\limits_{m \in M} \mathrm{P}\left(S_{n}=m\right) \triangleq \sum_{m \in M} \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{x_{m}^{2}}{2}}\left(1+\underline{O}\left(\frac{1}{\sigma}\right)\right),
\end{array}$$
где $x_m$ понимается как $\frac{m - np}{\sigma}.$ Обозначим как $\Delta x_{m}$ разность $x_m - x_{m-1}.$ Рассмотрим эту разность:
$$\Delta x_{m}=x_{m}-x_{m-1}=\frac{m-n p}{\sigma}-\frac{m-1-n p}{\sigma}=\frac{m-n p-m+1+n p}{\sigma}=\frac{1}{\sigma}.$$
Таким образом после замены в последней сумме $\frac{1}{\sigma}$ на $\Delta x_m$ ясно, что это интегральная сумма Римана для интегрируемой на любом отрезке функции $e^{-\frac{x^2}{2}}$ плюс $\underline{O}\left(\frac{1}{\sigma}\right)$:
$$\Delta x_{m}=\frac{1}{\sigma}=\frac{1}{\sqrt{n p(1-p)}} \underset{n \to \infty}{\longrightarrow} 0$$ по условию локальной теоремы.

Таким образом, на отрезке $[-C,C]$ сумма сходится к интегралу:
$$\sum_{m \in M} \frac{1}{\sqrt{2 \pi}} \Delta x_{m} e^{-\frac{x_{m}^{2}}{2}}\left(1+\underline{O}\left(\Delta x_{m}\right)\right) \underset{n \to \infty}{\longrightarrow} \frac{1}{\sqrt{2 \pi}} \int\limits_{a}^{b} e^{-\frac{x^{2}}{2}} d x,$$

следовательно сойдется и на любом подотрезке. Причём в
силу свойств сумм Римана, разность между суммой и предельным значением будет
меньше $\varepsilon$ при $n$ не меньше некоторого $N_\varepsilon$, которое для всех допустимых $a$ и $b$ будет одним и тем же - это и есть равномерность (что верно в силу критерия Дарбу о сходимости). $~~~ \square$

\mydef{} Случайная величина $\xi$ имеет {\it нормальное (гауссовское) распределение} с параметрами $a$ и $\sigma^2$, где $a \in \mathbb{R}, \sigma > 0$, если $\xi$ имеет следующую плотность распределения: 
$$f(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-a)^{2}}{2 \sigma^{2}}}, \quad x \in \mathbb{R}.$$

\section{Совокупности случайных величин. Совместная функция распределения. Независимость случайных величин. Критерии независимости. Ковариация, коэффициент корреляции.}

\subsubsection{Совместное распределение, его свойства}

Пусть случайные величины $\xi_1, ..., \xi_n$ заданы на одном вероятностном пространстве $(\Omega, \mathcal{F}, \mathbb{P})$.

\mydef{} Функция $$F\left(x_{1}, \ldots, x_{n}\right)=\mathrm{P}\left(\xi_{1}<x_{1}, \ldots, \xi_{n}<x_{n}\right)$$
называется функцией распределения вектора $(\xi_1, ..., \xi_n)$ или функцией {\it совместного} распределения случайных величин $\xi_1, ... \xi_n$.

Свойства для двумерного случая $(\xi, \eta)$:

\begin{enumerate}
    \item Функция распределения $F(x,y) = \myprob{\xi < x, \eta < y}$ не убывает по $x$ и по $y$.
    \item $F(x,y)$ стремится к нулю, если любую из переменных устремить к $-\infty$.
    \item $F(x,y)$ стремится к единице, если {\it обе} переменные устремить к $+\infty$.
    \item $F(x,y)$ непрерывна слева по каждой переменной
    \item Чтобы по функции совместного распределения восстановить функцию распределения $\xi$ и $\eta$ в отдельности, следует устремить <<лишнюю>> переменную к $+\infty$:
    $$\lim _{x \rightarrow+\infty} F(x, y)=F_{\eta}(y), \quad \lim _{y \rightarrow+\infty} F(x, y)=F_{\xi}(x)$$
\end{enumerate}

\subsubsection{Виды многомерных распределений}

\mydef{} Совместное распределение величин $\xi$ и $\eta$ {\it дискретно}, если каждая из них имеет дискретное распределение. Если $\xi$ принимает значения $a_1, a_2, ...$, а $\eta$ принимает значения $b_1, b_2, ...$, то пара $(\xi, \eta)$ принимает всевозможные значения $(a_i, b_j)$. Таблицу, на пересечении $i$-ой строки и $j$-го столбца которой стоит вероятность $p_{i,j} = \myprob{\xi = a_i, \eta = b_j}$, называют {\it таблицей совместного распределения} случайных величин $\xi$ и $\eta$. Вероятности $p_{i,j}$ в сумме дают единицу:
$$\sum_{i} \sum_{j} p_{i, j}=1$$

\mydef{} Говорят, что случайные величины $\xi$ и $\eta$ имеют {\it абсолютно непрерывное совместное распределение}, если существует неотрицательная функция $f(x,y)$ такая, что для любых $a < b, c < d$ имеет место равенство
$$\mathrm{P}(a<\xi<b, c<\eta<d)=\int\limits_{a}^{b} d x \int\limits_{c}^{d} f(x, y) d y.$$
Если такая функция $f(x,y)$ существует, она называется {\it плотностью совместного распределения} случайных величин $\xi$ и $\eta$.

Если случайные величины $\xi$ и $\eta$ имеют абсолютное непрерывное совместное распределение, то для любых $x, y$ имеет место равенство:
\begin{equation}F(x, y)=\mathrm{P}(\xi<x, \eta<y)=\int\limits_{-\infty}^{x} d u \int\limits_{-\infty}^{y} f(u, v) d v\end{equation}

Плотность совместного распределения имеет те же свойства, что и плотность распределения одной случайной величины: неотрицательность и нормированность:
$$f(x, y) \geqslant 0, \quad \int\limits_{-\infty}^{\infty} d x \int\limits_{-\infty}^{\infty} f(x, y) d y=1.$$

По функции совместного распределения его плотность находится как смешанная частная производная (в точках, где она существует):
$$f(x, y)=\frac{\partial^{2}}{\partial x \partial y} F(x, y).$$

\myth{} Если случайные величины $\xi$ и $\eta$ имеют абсолютное непрерывное совместное распределение с плотностью $f(x,y)$, то $\xi$ и $\eta$ в отдельности тоже имеют абсолютно непрерывные распределения с плотностями:
$$f_{\xi}(x)=\int\limits_{-\infty}^{\infty} f(x, y) d y ; \quad f_{\eta}(y)=\int\limits_{-\infty}^{\infty} f(x, y) d x.$$

\myqed{} TODO

\subsubsection{Независимость случайных величин}

\mydef{} Случайные величины $\xi_1, ..., \xi_n$ называют независимыми, если для любых множеств $B_1, ..., B_n$ имеет место равенство:

$$\mathrm{P}\left(\xi_{1} \in B_{1}, \ldots, \xi_{n} \in B_{n}\right)=\mathrm{P}\left(\xi_{1} \in B_{1}\right) \cdot \ldots \cdot \mathrm{P}\left(\xi_{n} \in B_{n}\right).$$

Можно сформулировать и другое, равносильное определение.

\mydef{} Случайные величины $\xi_1, ..., \xi_n$ независимы, если функция совместного распределения распадается в произведение частных функций распределения, т.е. для любых $x_1, ..., x_n$ имеет место равенство:
$$F\left(x_{1}, \ldots, x_{n}\right)=F_{\xi_{1}}\left(x_{1}\right) \cdot \ldots \cdot F_{\xi_{n}}\left(x_{n}\right).$$

Для случайных величин с дискретными или с абсолютно непрерывными
распределениями эквивалентные определения независимости выглядят так.

\mydef{} Случайные величины $\xi_1, ..., \xi_n$ с дискретным распределением независимы, если для любых чисел $a_1, ..., a_n$ имеет место равенство: $\mathrm{P}\left(\xi_{1}=a_{1}, \ldots, \xi_{n}=a_{n}\right)=\mathrm{P}\left(\xi_{1}=a_{1}\right) \cdot \ldots \cdot \mathrm{P}\left(\xi_{n}=a_{n}\right)$.

\mydef{} Случайные величины $\xi_1, ..., \xi_n$ с абсолютно непрерывным совместным распределением независимы, если плотность совместного распределения распадается в произведение плотностей, т.е. для любых $x_1, ..., x_n$ имеет место равенство: $f\left(x_{1}, \ldots, x_{n}\right)=f_{\xi_{1}}\left(x_{1}\right) \cdot \ldots \cdot f_{\xi_{n}}\left(x_{n}\right)$.

TODO Критерии независимости

\subsubsection{Ковариация, коэффициент корреляции, их свойства}

\mydef{} {\it Ковариацией} $\operatorname{cov}(\xi,\eta)$ случайных величин $\xi$ и $\eta$ называется число $$\operatorname{cov}(\xi,\eta)=\mathbb{E}((\xi-\mathbb{E} \xi)(\eta-\mathbb{E} \eta)).$$

\mynote{} $$\begin{aligned} \operatorname{cov}(\xi, \eta) &=\mathbb{E}((\xi-\mathbb{E} \xi)(\eta-\mathbb{E} \eta))=\mathbb{E}(\xi \eta-\eta \mathbb{E} \xi-\xi \mathbb{E} \eta+\mathbb{E} \xi \mathbb{E} \eta) \\ &=\mathbb{E} \xi \eta-\mathbb{E} \eta \mathbb{E} \xi-\mathbb{E} \xi \mathbb{E} \eta+\mathbb{E} \xi \mathbb{E} \eta=\mathbb{E} \xi \eta-\mathbb{E} \xi \mathbb{E} \eta \end{aligned}$$

Свойства:

\begin{enumerate}
    \item $\operatorname{cov}(\xi, \xi)=\mathrm{D} \xi.$
    \item $\operatorname{cov}(\xi, \eta)=\operatorname{cov}(\eta, \xi).$
    \item $\operatorname{cov}(c \xi, \eta)=c \operatorname{cov}(\xi, \eta).$
    \item Дисперсия суммы нескольких случайных величин вычисляется по любой из следующих формул:
    $$\mathrm{D}\left(\xi_{1}+\ldots+\xi_{n}\right)=\sum_{i=1}^{n} \mathrm{D} \xi_{i}+\sum_{i \neq j} \operatorname{cov}\left(\xi_{i}, \xi_{j}\right)=\sum_{i=1}^{n} \mathrm{D} \xi_{i}+2 \sum_{i<j} \operatorname{cov}\left(\xi_{i}, \xi_{j}\right).$$
\end{enumerate}

\mydef{} {\it Коэффициентом корреляции} $\rho(\xi,\eta)$ случайных величин $\xi$ и $\eta$, дисперсии которых существуют и отличны от нуля, называется число
$$\rho(\xi, \eta)=\frac{\operatorname{cov}(\xi, \eta)}{\sqrt{D \xi} \sqrt{D \eta}}.$$

Свойства:

\begin{enumerate}
    \item Коэффициент корреляции независимых случайных величин равен нулю.
    
    \myqed{} В числителе дроби, которой равен коэффициент корреляции,
окажется ноль. В знаменателе нуля быть не должно, это обеспечивается определением.
$ ~~~ \square$

    \item Для любых двух случайных величин (для которых выполнены условия определения) их коэффициент корреляции по модулю не превосходит единицы.
    
    \myqed{} Обозначим эти две случайные величины как $X$ и $Y$ и центрируем: $X_c = X - \mathbb{E}X$ и $Y_c = Y - \mathbb{E}Y$. Так как $\operatorname{cov}(X, Y)=\operatorname{cov}\left(X_{c}, Y_{c}\right)$, а дисперсия случайной величины не меняется от смещения случайной величины на константу, коэффициент корреляции не изменится.
    
    Далее, т.к. $\mathbb{E} X_{c}=\mathbb{E} Y_{c}=0$:
    $$\mathrm{D} X_{c}=\mathbb{E} X_{c}^{2}-\left(\mathbb{E} X_{c}\right)^{2}=\mathbb{E} X_{c}^{2}, \mathrm{D} Y_{c}=\mathbb{E} Y_{c}^{2},$$$$ \operatorname{cov}\left(X_{c}, Y_{c}\right)=\mathbb{E}\left(X_{c} Y_{c}\right)-\mathbb{E} X_{c} \mathbb{E} Y_{c}=\mathbb{E}\left(X_{c} Y_{c}\right)$$
    
    Далее идут те же рассуждения, что часто используются при доказательстве неравенства Коши-Буняковского:
    
    $$\forall a \in \mathbb{R} \quad 0 \leqslant \mathrm{D}\left(X_{c}-a Y_{c}\right)=\mathbb{E}\left(X_{c}-a Y_{c}\right)^{2}-\left(\mathbb{E}\left(X_{c}-a Y_{c}\right)\right)^{2}=\mathbb{E}\left(X_{c}-a Y_{c}\right)^{2}.$$
    
    Полученное неравенство можно рассматривать как квадратное неравенство относительно $a$, а именно
    
    $$\mathbb{E}\left(X_{c}-a Y_{c}\right)^{2}=\mathbb{E} X_{c}^{2}-2 a \mathbb{E}\left(X_{c} Y_{c}\right)+a^{2} \mathbb{E} Y_{c}^{2} \geqslant 0$$
    
    Поскольку верно это для любого $a$, то дискриминанту нельзя ни в коем случае быть больше нуля. То есть:
    
    $$\left(\mathbb{E}\left(X_{c} Y_{c}\right)\right)^{2}-\mathbb{E} X_{c}^{2} \mathbb{E} Y_{c}^{2} \leqslant 0 \Longleftrightarrow\left|\mathbb{E}\left(X_{c} Y_{c}\right)\right| \leqslant \sqrt{\mathbb{E} X_{c}^{2} \mathbb{E} Y_{c}^{2}} $$$$\Rightarrow\left|\operatorname{cov}\left(X_{c}, Y_{c}\right)\right| \leqslant \sqrt{\mathrm{D} X_{c} \mathrm{D} Y_{c}}.$$
    
    По доказанному выше <<стирание>> индексов не изменит коэффициентов. $\square$

    \item Если $|\rho(X,Y)| = 1$, то с вероятностью один $X$ и $Y$ линейно выражаются друг через друга. То есть,
    $$|\rho(X, Y)|=1 \Longrightarrow \exists b \neq 0, c \in \mathbb{R}: \mathrm{P}(X-b Y=c)=1.$$
    
    \myqed{} Доказательство этого свойства целиком опирается на доказательство предыдущего: если выполнилось равенство $|\operatorname{cov}(X, Y)|=\sqrt{\mathrm{D} X \mathrm{D} Y}$, то квадратное неравенство относительно $a$ может обращаться в равенство при некотором $a = b$. Но это равенство означает, что равна нулю $\mathrm{D}(X-b Y)$, а это сразу говорит о том, что с вероятностью один $X - bY$ равна константе. Обозначим эту константу за $c$ и получим то, что нужно было доказать. $~~~\square$

\end{enumerate}

\section{Виды сходимости последовательностей случайных величин.}

Везде далее каждая случайная величина $\xi_n(\omega)$ задана на вероятностном пространстве $(\Omega_n,\mathcal{F}_n,\mathbb{P})$.

\mydef{} Последовательность случайных величин $\{ \xi_n \}$ сходится к случайной величине $\xi$ {\it по вероятности}: $\lim\limits_{n \to \infty} \xi_{n} \overset{\mathbb{P}}{=} \xi$, если для $\forall \varepsilon > 0 \Rightarrow \lim _{n \to \infty} \mathbb{P}\left(\left|\xi_{n}-\xi\right|>\varepsilon\right)=0.$

\mydef{} Последовательность случайных величин $\{ \xi_n \}$ сходится к случайной величине $\xi$ {\it с вероятностью 1 (почти всюду)}: $\xi_n \xrightarrow[n \rightarrow \infty]{\text{с вероятностью}~1} \xi$, если $\mathbb{P}\left(\omega: \xi_{n}(\omega) \rightarrow \xi(\omega)\right)=1$. Рассматривается на вероятностном пространстве $(\Omega, \mathcal{F}, \mathbb{P})$, $\{ \xi_n(\omega) \}$ - последовательность случайных величин (по определению измеримых), следовательно, определение корректно.

Введём множество $A=\left\{\left(\omega: \xi_{n}(\omega) \rightarrow \xi(\omega)\right\}\right.$. Его можно представить в виде 
$$A=\bigcap_{k=1}^{\infty} \bigcup_{N=1}^{\infty} \bigcap_{n=N}^{\infty}\left(\omega:\left|\xi_{n}(\omega)-\xi(\omega)\right|<\frac{1}{k}\right)=\bigcap_{k=1}^{\infty} \varliminf_{n \to \infty}\left(\omega:\left|\xi_{n}(\omega)-\xi(\omega)\right|<\frac{1}{k}\right),$$ $A \in \mathcal{F}.$

или, что то же самое, $A=\left(\omega: \forall k \in \mathbf{N}, \exists N: \forall n \geq N \Rightarrow\left|\xi_{n}(\omega)-\xi(\omega)\right|<\frac{1}{k}\right).$

Тогда определение сходимости с вероятностью 1 можно переписать как $\myprob{A} = 1.$

\mydef{} Последовательность случайных величин $\{ \xi_n \}$ сходится к случайной величине $\xi$ {\it в среднем порядка} $\alpha > 0: \xi_n \rightarrow \xi$, если $\lim _{n \to \infty} \mathbb{E}\left|\xi_{n}-\xi\right|^{\alpha}=0.$

\mydef{} Последовательность случайных величин $\{ \xi_n \}$ сходится к случайной величине $\xi$ {\it по распределению}: $\xi_n \rightarrow \xi$, если $F_{\xi_n}(x) \rightarrow F_\xi(x), \forall x_0 \Rightarrow F_\xi(x_0)$ непрерывна.

\mydef{} Последовательность случайных величин $\{ \xi_n \}$ сходится к случайной величине $\xi$ {\it слабо}: $\xi_n \Rightarrow \xi$, если для любой непрерывной ограниченной функции $\varphi$ верно:
$$\lim _{n \to \infty} \mathbb{E} \varphi\left(\xi_{n}\right)=\mathbb{E} \varphi(\xi).$$

\section{Неравенства Маркова, Чебышева и Гаусса. Правило «трех сигм». Закон больших чисел в форме Чебышева.}

\myth{} \textbf{(неравенство Маркова)} Если $\mathbb{E}|\xi| < \infty$, то для любого $x > 0$
$$\myprob{|\xi| \geqslant x} \leqslant \frac{\mathbb{E}|\xi|}{x}.$$

\mydef{}  Назовём индикатором события $A$ случайную величину $I(A)$, равную единице, если событие $A$ произошло, и нулю, если $A$ не произошло.

\myqed{} По определению, величина $I(A)$ имеет распределение Бернулли с параметром $p = \myprob{I(A) = 1} = \myprob{A}$ и её математическое ожидание равно вероятности успеха $p = \myprob{A}.$ Индикаторы прямого и противоположного событий связаны равенством $I(A) + I(\overline{A}) = 1.$ Поэтому
$$|\xi|=|\xi| \cdot I(|\xi|<x)+|\xi| \cdot I(|\xi| \geqslant x) \geqslant|\xi| \cdot I(|\xi| \geqslant x) \geqslant x \cdot I(|\xi| \geqslant x).$$
Тогда $\mathbb{E}|\xi| \geqslant \mathbb{E}(x \cdot I(|\xi| \geqslant x))=x \cdot \mathrm{P}(|\xi| \geqslant x)$. Осталось разделить обе части этого неравенства на положительное число $x$. $~~~\square$

\mycon{} \textbf{(обобщённое неравенство Чебышёва)}. Пусть функция $g$ не убывает и неотрицательна на $\mathbb{R}.$ Если $\mathbb{E}g(\xi) < \infty$, то для любого $x \in \mathbb{R}$
$$P(\xi \geqslant x) \leqslant \frac{E g(\xi)}{g(x)}.$$

\myqed{} Заметим, что $\myprob{\xi \geqslant x} \leqslant \myprob{g(\xi) \geqslant g(x)}$, поскольку функция $g$ не убывает. Оценим последнюю вероятность по неравенству Маркова, которое можно применять в силу неотрицательности $g$:
$$\mathrm{P}(g(\xi) \geqslant g(x)) \leqslant \frac{\mathbb{E} g(\xi)}{g(x)}. ~~~ \square$$

\mycon{} \textbf{(неравенство Чебышёва)} Если $\mathrm{D}\xi$ существует, то для любого $\varepsilon > 0$
$$\mathrm{P}(|\xi-\mathbb{E} \xi| \geqslant \varepsilon) \leqslant \frac{\mathrm{D} \xi}{x^{2}}.$$

\myqed{} Для $x > 0$ неравенство $|\xi - \mathbb{E}\xi| \geqslant x$ равносильно неравенству $(\xi - \mathbb{E}\xi)^2 \geqslant x^2$, поэтому
$$\mathrm{P}(|\xi-\mathbb{E} \xi| \geqslant x)=\mathrm{P}\left((\xi-\mathbb{E} \xi)^{2} \geqslant x^{2}\right) \leqslant \frac{\mathbb{E}(\xi-\mathbb{E} \xi)^{2}}{x^{2}}=\frac{\mathrm{D} \xi}{x^{2}}. ~~~\square$$

\myth{} \textbf{(неравенство Гаусса)} (без доказательства) Пусть $X$ - случайная величина с модой $m$ и пусть $\tau^2$ - математическое ожидание $(X - m)^2.$ Тогда верно следующее:

$$\mathbb{P}(|X-m|>k) \leq\left\{\begin{array}{ll}
\left(\frac{2 \tau}{3 k}\right)^{2}, & \text { если } k \geq \frac{2 \tau}{\sqrt{3}} \\
1-\frac{k}{\tau \sqrt{3}}, & \text { если } 0 \leqslant k \leqslant \frac{2 \tau}{\sqrt{3}}
\end{array}\right.$$

В неравенстве Чебышева в качестве $\varepsilon$ можно брать любое положительное число. Если взять в качестве $\varepsilon$ величину $3\sigma$, где $\sigma$ — стандартное отклонение (то есть именно корень из дисперсии), то получится
$$\mathrm{P}(|X-\mathbb{E} X|>3 \sigma) \leqslant \frac{\mathrm{D} X}{9 \mathrm{D} X}=\frac{1}{9}$$
или то же самое $$\mathrm{P}(|X-\mathbb{E} X| \leqslant 3 \sigma) \geqslant 1-\frac{1}{9}=\frac{8}{9}.$$ 
Это соотношение и зовётся {\it правилом трёх сигм}.

\mydef{} Говорят, что последовательность случайных величин $\xi_1, \xi_2, ...$ с конечными первыми моментами {\it удовлетворяет закону Больших чисел (ЗБЧ)}, если
$$\frac{\xi_{1}+\ldots+\xi_{n}}{n}-\frac{\mathbb{E} \xi_{1}+\ldots+\mathbb{E} \xi_{n}}{n} \stackrel{\mathrm{p}}{\longrightarrow} 0 \text { при } n \rightarrow \infty.$$

\myth{} \textbf{(ЗБЧ Чебышёва)} Для любой последовательности $\xi_1, \xi_2, ...$ попарно независимых и одинаково распределённых случайных величин с конечным вторым моментом $\mathbb{E}\xi_1^2 < \infty$ имеет место сходимость
$$\frac{\xi_{1}+\ldots+\xi_{n}}{n} \stackrel{\mathrm{p}}{\longrightarrow} \mathbb{E} \xi_{1}.$$

\myqed{} Обозначим через $S_n = \xi_1 + ... + \xi_n$ сумму первых $n$ случайных величин. Из линейности матожидания получим
$$\mathbb{E}\left(\frac{S_{n}}{n}\right)=\frac{\mathbb{E} \xi_{1}+\ldots+\mathbb{E} \xi_{n}}{n}=\frac{n \mathbb{E} \xi_{1}}{n}=\mathbb{E} \xi_{1}.$$
Пусть $\varepsilon > 0.$ Воспользуемся неравенством Чебышёва:
$$\mathrm{P}\left(\left|\frac{S_{n}}{n}-\mathbb{E}\left(\frac{S_{n}}{n}\right)\right| \geqslant \varepsilon\right) \leqslant \frac{\mathrm{D}\left(\frac{S_{n}}{n}\right)}{\varepsilon^{2}}=\frac{\mathrm{D} S_{n}}{n^{2} \varepsilon^{2}}=\frac{\mathrm{D} \xi_{1}+\ldots+\mathrm{D} \xi_{n}}{n^{2} \varepsilon^{2}}=$$$$=\frac{n \mathrm{D} \xi_{1}}{n^{2} \varepsilon^{2}}=\frac{\mathrm{D} \xi_{1}}{n \varepsilon^{2}} \rightarrow 0 \text { при } n \rightarrow \infty,$$
так как $\mathrm{D}\xi_1 < \infty$. Дисперсия суммы превратилась в сумму дисперсий в силу попарной независимости слагаемых, из-за которой все ковариации $\operatorname{cov}(\xi_i, \xi_j)$ по свойству ковариации обратились в нуль при $i \neq j. ~~~ \square$

\section{Характеристические функции и их свойства.}
\begin{defn}
    {\it Характеристическая функция случайной величины} $\xi$~--- функция $\varphi_{\xi}: \mathbb{R} \rightarrow \mathbb{C}$:
    \begin{equation*}
        \varphi_{\xi}(t)
        = \mathbb{E} e^{i t \xi}
        = \mathbb{E} \cos (t \xi)+i \mathbb{E} \sin (t \xi) = \int\limits_{\mathbb{R}}^{} \mathrm{e}^{\mathrm{i} \xi x} d F_{\xi}(x),
    \end{equation*}
    где интеграл справа называется {\it интегралом Фурье-Стильтьеса}.
    
    Для абсолютно непрерывного распределения характеристическая функция имеет вид
    \begin{equation*}
        \varphi_{\xi}(t)=\int\limits_{\mathbb{R}} e^{i t x} f(x) d x
    \end{equation*}
    Для дискретного, соответственно
    \begin{equation*}
        \varphi_{\xi}(t)=\sum_{i} e^{i t x_{i}} \mathbb{P}\left\{\xi=x_{i}\right\}
    \end{equation*}
\end{defn}

\begin{exmp}
    Характеристическая функция случайной величины $\xi \sim \mathbf{N}(0,1)$:
    \begin{multline*}
        \varphi_{\xi}(t) 
        = \frac{1}{\sqrt{2 \pi}} \int\limits_{-\infty}^{\infty} e^{i t x} e^{-x^{2} / 2} d x
        = \frac{1}{\sqrt{2 \pi}} \int\limits_{-\infty}^{\infty} e^{-t^{2} / 2} e^{-(x-i t)^{2} / 2} d x = \\ 
        = e^{-t^{2} / 2} \frac{1}{\sqrt{2 \pi}} \int\limits_{-\infty}^{\infty} e^{-(x-i t)^{2} / 2} d(x-i t)
        = e^{-t^{2} / 2}
    \end{multline*}
\end{exmp}

\begin{thm}
    Свойства характеристической функции:
    \begin{enumerate}
        \item Характеристическая функция существует для любой случайной величины $\xi$.
        \item Характеристическая функция линейна, т.е. $\forall~ \xi,~ \forall~ a, b \in \mathbb{R}$:
        \begin{equation*}
            \varphi_{a \xi + b}(t) = e^{itb} \varphi_{\xi}(at)
        \end{equation*}
        \item $|\phi_{\xi}(t)|=|\mathbb{E} e^{i t \xi}| \leqslant 1,~ \varphi_{\xi}(0) = 1, ~\overline{\varphi_{\xi}(t)} = \varphi_{\xi}(-t) = \varphi_{-\xi}(t) ~\forall t \in \mathbb{R}$
        \item Если случайные величины $\xi$ и $\eta$ независимы, то
        \begin{equation*}
            \varphi_{\xi + \eta}(t) = \varphi_{\xi}(t) \varphi_{\eta}(t)
        \end{equation*}
        \item Характеристическая функция равномерно непрерывна.
        \item Если существует абсолютный момент $k$-го порядка $\mathbb{E}|\xi|^{k} < \infty,~ k \geqslant 1$, то существует непрерывная $k$-я производная характеристической функции:
        \begin{equation*}
            \left.\cfrac{\partial^{k} \varphi_{\xi}}{\partial t^{k}}\right|_{t=0}= i^{k} \mathbb{E} \xi^{k}
        \end{equation*}
        \item Характеристическая функция случайно величины $\xi$ однозначно определяет её функцию распределения $F_{\xi}$. Функция распределения восстанавливаются по характеристической функции с помощью обратных преобразований Фурье.
        
        Дискретное распределение:
        \begin{equation*}
            \mathbb{P}(\xi=k)=\frac{1}{2 \pi} \int_{-\pi}^{\pi} e^{-i t k} \phi_{X}(t) d t, k \in \mathbb{Z}
        \end{equation*}
        
        Абсолютно непрерывное распределение:
        \begin{equation*}
            f_{\xi}(x)=\frac{1}{2 \pi} \int_{-\infty}^{\infty} e^{-i t x} \phi_{X}(t) d t, x \in \mathbb{R}
        \end{equation*}
    \end{enumerate}
\end{thm}

\begin{proof}
    \begin{enumerate}
        \item Существование характеристической функции равносильно равномерной сходимости соответствующего интеграла. Докажем её по признаку Вейерштрасса:
        \begin{equation*}
            \left|\varphi_{\xi}(t)\right|=\left|\int\limits_{\mathbb{R}} e^{i t x} d F(x)\right| \leqslant \int\limits_{\mathbb{R}}\left|e^{i t x}\right| d F(x)=\int\limits_{\mathbb{R}} d F(x)=1
        \end{equation*}
        \item $\varphi_{a \xi+b}(t) 
        = \mathbb{E}e^{i t(a \xi+b)}
        = e^{i t b} \mathbb{E}e^{i t a \xi}
        = e^{i t b} \varphi_{\xi}(t a)$
        \item Первое неравенство доказано в пункте 1, второе равенство очевидно.
        \begin{equation*}
            \varphi_{\xi}(-t) = \mathbb{E}cos(-t \xi) + i\mathbb{E}sin(-t \xi) = \mathbb{E}cos(t \xi) - i\mathbb{E}sin(t \xi) = \overline{\varphi_{\xi}(t)}
        \end{equation*}
        Оставшиеся равенства следуют из свойства линейности.
        \item $\varphi_{\xi + \eta}(t) 
        = \mathbb{E}e^{it(\xi + \eta)} 
        = \mathbb{E}e^{it\xi}\mathbb{E}e^{it\eta}
        = \varphi_{\xi}(t)\varphi_{\eta}(t)$
        \item Выберем сколь угодно малое $\varepsilon > 0$ и оценим разность значений характеристической функции в точках $t$ и $t + h$:
        \begin{multline*}
            |\varphi(t+h)-\varphi(t)| 
            = \left|\int_{\mathbb{R}} \left(\mathrm{e}^{\mathrm{i}(t+h) x}-\mathrm{e}^{\mathrm{i} t}\right) d F(x)\right|
            = \left|\int_{\mathbb{R}} \mathrm{e}^{\mathrm{i} t x}\left(\mathrm{e}^{\mathrm{i} h x}-1\right) d F(x)\right| \leqslant \\
            \leqslant \int\limits_{\mathbb{R}} \left|\mathrm{e}^{\mathrm{i} h x}-1\right| d F(x)=\int\limits_{|x| \leqslant R}\left|\mathrm{e}^{\mathrm{i} h x}-1\right| d F(x)+\int\limits_{|x|>R}\left|\mathrm{e}^{\mathrm{i} h x}-1\right| d F(x)
        \end{multline*}
        Теперь выберем $R$ настолько большим, чтобы $\mathrm{P}(|X|>R) < \frac{\varepsilon}{4}$. Поскольку $\left|e^{i h x}-1\right| \leqslant 2$, второй интеграл при этом не превосходит по величине $\frac{\varepsilon}{2}$. После этого выберем $h$ столь малым, чтобы $\left|\mathrm{e}^{\mathrm{i} h x}-1\right|<\frac{\varepsilon}{2}~$ при всех $|x| \leqslant R$. Тогда и первый интеграл не превосходит $\frac{\varepsilon}{2}$ и, таким образом, по заданному $\varepsilon > 0$ подобрано столь малое $h >0$, что $|\varphi(s+h)-\varphi(s)|<\varepsilon~ \forall t \in \mathbb{R}$.
        \item Если существует $\mathbb{E}\xi^{k}<\infty,~ k \geqslant 1$, то для всех $j = \overline{1, k}$ существуют $\mathbb{E}\xi^{j}<\infty$. Следовательно,
        \begin{equation*}
            \left|\int_{\mathbb{R}}(i x)^{j} e^{i t x} d F(x)\right| \leqslant \int_{\mathbb{R}}|x|^{k} d F(x)=\mathbb{E}\left[|\xi|^{j}\right]<\infty~ \forall j = \overline{1, k}
        \end{equation*}
        Т.е. интегралы $\int_{\mathbb{R}}(i x)^{k} e^{i t x} d F(x)$ сходятся равномерно по $t$
    \end{enumerate}
\end{proof}

\section{Закон больших чисел в форме Хинчина}
\begin{thm}
    Для любой последовательности $\xi_{1}, \xi_{2}, \ldots$ независимых и одинаково распределённых случайных величин с конечным первым моментом $E\left|\xi_{1}\right|<\infty$ имеет место сходимость:
    \begin{equation*}
        \frac{S_{n}}{n} = \frac{\xi_{1}+\ldots+\xi_{n}}{n} \xrightarrow[]{\text{p}} \mathbb{E} \xi_{1}
    \end{equation*}
\end{thm}
Для доказательства теоремы нам потребуется следующая лемма.
\begin{lem}
    Если $\xi_{n} \Rightarrow c=const$, то $\xi_{n} \stackrel{\mathrm{p}}{\longrightarrow} c$
\end{lem}
\begin{proof}
    Пусть $\xi_{n} \Rightarrow c$, т.е.
    \begin{equation*}
        F_{\xi_{n}}(x) \rightarrow F_{c}(x)=\left\{\begin{array}{ll}
        0, & x \leqslant c \\
        1, & x>c
        \end{array}\right.
    \end{equation*}
    при любом $x$, являющемся точкой непрерывности предельной функции $F_{c}(x)$, т. е. $\forall~ x \neq c$.
    
    Возьмём произвольное $\varepsilon>0$ и докажем, что $\mathrm{P}\left(\left|\xi_{n}-c\right|<\varepsilon\right) \rightarrow 1$:
    \begin{multline*}
        \mathrm{P}\left(-\varepsilon<\xi_{n}-c<\varepsilon\right)=\mathrm{P}\left(c-\varepsilon<\xi_{n}<c+\varepsilon\right) \geqslant \mathrm{P}\left(c-\varepsilon / 2 \leqslant \xi_{n}<c+\varepsilon\right)= \\
        \quad=F_{\xi_{n}}(c+\varepsilon)-F_{\xi_{n}}(c-\varepsilon / 2) \rightarrow F_{c}(c+\varepsilon)-F_{c}(c-\varepsilon / 2)=1-0=1
    \end{multline*}
    поскольку в точках $c+\varepsilon$ и $c-\varepsilon / 2$ функция $F_{c}$ непрерывна, и, следовательно, имеет место сходимость последовательностей $F_{\xi_{n}}(c+\varepsilon)$ к $F_{c}(c+\varepsilon)=1$ и $F_{\xi_{n}}(c-\varepsilon / 2)$ к $F_{c}(c-\varepsilon / 2)=0$.
    
    Осталось заметить, что $\mathrm{P}\left(\left|\xi_{n}-c\right|<\varepsilon\right)$ не бывает больше $1$, так что по свойству предела зажатой последовательности $\mathrm{P}\left(\left|\xi_{n}-c\right|<\varepsilon\right) \rightarrow 1$.
\end{proof}

Перейдём к доказательству теоремы.

\begin{proof}
    По вышеприведённому свойству сходимость по вероятности к постоянной эквивалентна слабой сходимости. Так как $a$~--- постоянная, достаточно доказать слабую сходимость $\frac{S_{n}}{n}$ к $a$. По теореме о непрерывном соответствии, эта сходимость имеет место тогда и только тогда, когда для любого $t \in \mathbb{R}$ сходятся характеристические функции
    \begin{equation*}
        \varphi_{S_{n} / n}(t) \rightarrow \varphi_{a}(t)=\mathbb{E} e^{i t a}=e^{i t a}
    \end{equation*}
    
    Найдём характеристическую функцию случайной величины $\frac{S_{n}}{n}$. Пользуясь свойствами характеристической функции, получаем
    \begin{equation*}
        \varphi_{S_{n} / n}(t)=\varphi_{S_{n}}\left(\frac{t}{n}\right)=\left(\varphi_{\xi_{1}}\left(\frac{t}{n}\right)\right)^{n}
    \end{equation*}
    
    Вспомним, что первый момент $\xi_{1}$ существует, поэтому мы можем разложить $\varphi_{\xi_{1}}(t)$ в ряд Тейлора в окрестности нуля:
    \begin{equation*}
        \varphi_{\xi_{1}}(t)=1+i t \mathbb{E} \xi_{1}+o(|t|)=1+i t a+o(|t|)
    \end{equation*}
    
    В точке $\frac{t}{n}$ соответственно:
    \begin{gather*}
        \varphi_{\xi_{1}}\left(\frac{t}{n}\right)=1+\frac{i t a}{n}+o\left(\left|\frac{t}{n}\right|\right) \\
        \varphi_{S_{n} / n}(t)=\left(\varphi_{\xi_{1}}\left(\frac{t}{n}\right)\right)^{n}=\left(1+\frac{i t a}{n}+o\left(\left|\frac{t}{n}\right|\right)\right)^{n}
    \end{gather*}
    
    При $n \rightarrow \infty$ воспользуемся <<замечательным пределом>> $\left(1+\frac{x}{n}\right)^{n} \rightarrow e^{x}$ и получим:
    \begin{equation*}
        \varphi_{S_{n} / n}(t)=\left(1+\frac{i t a}{n}+o\left(\left|\frac{t}{n}\right|\right)\right)^{n} \rightarrow e^{i t a}
    \end{equation*}
\end{proof}

\section{Центральная предельная теорема}
\begin{thm}[Центральная предельная теорема]
    Пусть $\xi_{1}, \xi_{2}, \ldots$~--- последовательность независимых одинаково распределенных (невырожденных) случайных величин с $\mathbb{E} \xi_{1}^{2}<\infty$ и $S_{n}=\xi_{1}+\ldots+\xi_{n}$. Тогда
    \begin{equation*}
        \mathbb{P}\left(\frac{S_{n}-\mathbb{E} S_{n}}{\sqrt{\mathrm{D} S_{n}}} \leqslant x\right)
        \xrightarrow[n \rightarrow \infty]{}
        \varphi(x) = \frac{1}{\sqrt{2 \pi}} \int\limits_{-\infty}^{x} e^{-\frac{z^{2}}{2}} dz~ \forall x \in \mathbb{R}
    \end{equation*}
\end{thm}
\begin{proof}
Пусть $\mathbb{E} \xi_{1}=m, \mathrm{D} \xi_{1}=\sigma^{2}$ и $\varphi(t)=\mathbb{E} e^{i t\left(\xi_{1}-m\right)}$. Введём
\begin{equation*}
    \varphi_{n}(t)=\mathbb{E} e^{i t \frac{S_{n}-\mathbb{E} S_{n}}{\sqrt{\mathbb{D} S_{n}}}} = 
    \left[\varphi\left(\frac{t}{\sigma \sqrt{n}}\right)\right]^{n}
\end{equation*}

В силу разложения характеристической функции
\begin{equation*}
    \varphi_{X}(t)=1+i t \mathbb{E} X+\ldots+\frac{(i t)^{n}}{n !} \mathbb{E} X^{n}+R_{n}(t)
\end{equation*}

При $n=2$ получим 
\begin{equation*}
    \varphi(t)=1-\frac{\sigma^{2} t^{2}}{2}+\bar{o}\left(t^{2}\right), \quad t \rightarrow 0
\end{equation*}

Следовательно, для любого $t \in \mathbb{R}$ при $n \rightarrow \infty$
\begin{equation*}
    \varphi_{n}(t)=\left[1-\frac{\sigma^{2} t^{2}}{2 \sigma^{2} n}+\bar{o}\left(\frac{1}{n}\right)\right] \rightarrow e^{-\frac{t^{2}}{2}}
\end{equation*}

Функция $e^{-\frac{t^{2}}{2}}$ является характеристической функцией $\mathbf{N}(0,1)$. В силу теорем о непрерывном соответсвии между функциями распределения и характеристическими функциями центральная предельная теорема доказана.
\end{proof}

\section{Условное математическое ожидание.}

Пусть $\xi$ и $\eta$~--- две случайные величины на некотором вероятностном пространстве, причём $E|\xi|<\infty$; $L=L(\eta)$~--- множество, в котором собраны все случайные величины, имеющие вид $\zeta=g(\eta)$, где $g(x)$~--- произвольная борелевская функция. Скалярным произведением двух случайных величин $\varphi$ и $\zeta$ назовём $(\varphi, \zeta)=\mathbb{E}(\varphi \cdot \zeta)$, если это математическое ожидание существует.

Условное математическое ожидание $\mathbb{E}(\xi | \eta)$ случайной величины $\xi$ относительно $\eta$ можно представлять себе как результат ортогонального проектирования случайной величины $\xi$ на пространство $L$.

Результат проектирования — такая случайная величина $\mathbb{E}(\xi | \eta)=\widehat{\xi}$, для которой выполнено основное и единственное свойство ортопроекции: её разность с $\xi$ ортогональна всем элементам $L$. Ортогональность означает, что для любой $g(\eta) \in L$ обращается в нуль (если вообще существует) скалярное произведение $(\xi-\widehat{\xi}, g(\eta))$, т. е.
\begin{equation*}
    \mathbb{E}((\xi-\widehat{\xi}) g(\eta))=0~ \text{ или }~ \mathbb{E}(\xi g(\eta))=\mathbb{E}(\widehat{\xi} g(\eta))
\end{equation*}
Это свойство называют {\it тождеством ортопроекции}. Чтобы матожидание существовало всегда, достаточно брать лишь ограниченные функции $g(y)$.

\begin{defn}
    Пусть $\mathbb{E}|\xi|<\infty$, $L=L(\eta)$~--- множество всех борелевских функций от случайной величины $\eta$. Условным математическим ожиданием $E(\xi | \eta)$ называется случайная величина $\widehat{\xi} \in L$, удовлетворяющая тождеству ортопроекции.
\end{defn}

\begin{thm}
    Свойства условного математического ожидания:
    \begin{enumerate}
        \item Все свойства математического ожидания, как, например, линейность, однако борелевские функции от случайной величины $\eta$ выносятся из-под знака математического ожидания как постоянные.
        \item Пусть $\mathbb{E} \xi^{2}<\infty$.Тогда расстояние от $\xi$ до её ортопроекции $\widehat{\xi}=\mathbb{E}(\xi | \eta)$ является наименьшим из расстояний от $\xi$ до всех «точек» множества $L$:
        \begin{equation*}
            \min_{g(\eta) \in L} \mathbb{E}(\xi-g(\eta))^{2}=\mathbb{E}(\xi-\widehat{\xi})^{2}
        \end{equation*}
        \item Если $f(\eta) \in L$ такова, что $\mathbb{E}|f(\eta) \cdot \xi|<\infty$, то
        \begin{equation*}
            \mathbb{E}(f(\eta) \cdot \xi | \eta)
            \stackrel{\text{п.н.}}{=}
            f(\eta) \cdot \mathbb{E}(\xi | \eta)
        \end{equation*}
        \begin{proof}
            Рассмотрим только случай, когда $f(\eta)$ ограничена. Проверим, что $\zeta=f(\eta) \cdot E(\xi | \eta)$ удовлетворяет тождеству ортопроекции: для любой ограниченной $g(\eta) \in L$
            \begin{equation*}
                \mathbb{E}(f(\eta) \xi \cdot g(\eta))=\mathbb{E}(\zeta \cdot g(\eta))
            \end{equation*}
            Обозначим $h(\eta)=f(\eta) g(\eta) \in L$. Эта функция ограничена, поэтому
            \begin{equation*}
                \mathbb{E}(\xi f(\eta) \cdot g(\eta))=\mathbb{E}(\xi h(\eta))=\mathbb{E}(\widehat{\xi} h(\eta))=\mathbb{E}(\zeta \cdot g(\eta))
            \end{equation*}
        \end{proof}
        \item Если $\mathbb{E}|g(\xi, \eta)|<\infty$, то
        \begin{equation*}
            \mathbb{E} g(\xi, \eta)=\mathbb{E}\left[\left.\mathbb{E}(g(\xi, y) | \eta)\right|_{y=\eta}\right]
        \end{equation*}
        \item Если $\xi$ и $\eta$ независимы, то $\mathbb{E}(\xi | \eta)=\mathbb{E} \xi$.
    \end{enumerate}
\end{thm}
\subsubsection{Вычисление условного матожидания}
Возьмём функцию $h(y)=\mathbb{E}(\xi | \eta=y)$, для которой $\mathbb{E}(\xi | \eta)=h(\eta)$ и рассмотрим, что такое условное математическое ожидание относительно события $\{\eta=y\}$ для двух случаев: когда обе случайные величины имеют дискретное распределение и когда их совместное распределение абсолютно непрерывно.
\begin{enumerate}
    \item Пусть $\xi$ принимает значения $a_{1}, a_{2}, \ldots$, а $\eta$~--- значения $b_{1}, b_{2}, \ldots$ Тогда $h(\eta)$ может принимать только значения $h\left(b_{1}\right), h\left(b_{2}\right), \ldots$, где
    \begin{equation*}
        h(y)=\sum_{i} a_{i} \mathrm{P}\left(\xi=a_{i} | \eta=y\right)
    \end{equation*}
    Иначе говоря, при каждом фиксированном $y$ начение $h(y)$ определяется как математическое ожидание дискретного распределения со значениями $a_{i}$ и вероятностями $\mathrm{P}\left(\xi=a_{i} | \eta=y\right)$. Такое распределение называется {\it условным распределением случайной величины $\xi$ при условии $\eta = y$}.
    
    \item Во втором случае пусть $f_{\xi, \eta}(x, y)$~--- плотность совместного распределения, $f_{\eta}(y)$~--- плотность распределения величины $\eta$.Тогда положим
    \begin{equation*}
        h(y)=\int\limits_{\mathbb{R}} x \frac{f_{\xi, \eta}(x, y)}{f_{\eta}(y)} d x
    \end{equation*}
    При фиксированном $y$ число $h(y)$ есть математическое ожидание абсолютно непрерывного распределения с плотностью распределения
    \begin{equation*}
        f(x | y)=\frac{f_{\xi, \eta}(x, y)}{f_{\eta}(y)}=\frac{f_{\xi, \eta}(x, y)}{\int\limits_{\mathbb{R}} f_{\xi, \eta}(x, y) d x}
    \end{equation*}
    Такое распределение называется {\it условным распределением величины $\xi$ при условии $\eta = y$}, а функция $f(x | y)$~--- {\it условной плотностью}.
\end{enumerate}

    Убедимся формально (скажем, в абсолютно непрерывном случае), что определённая выше $h(\eta)$ удовлетворяет тождеству ортопроекции и, следовательно, является условным матожиданием $\mathbb{E}(\xi | \eta)$. Для любой $g(\eta) \in L$ (такой, что соответствующее математическое ожидание существует) левая часть тождества ортопроекции равна
    \begin{equation*}
        \mathbb{E}(\xi \cdot g(\eta))=\iint_{\mathbb{R}^{2}} x g(y) f_{\xi, \eta}(x, y) d x d y
    \end{equation*}
    
    Правая часть равна
    \begin{equation*}
        \mathbb{E}(h(\eta) g(\eta))=\int\limits_{\mathbb{R}} h(y) g(y) f_{\eta}(y) d y=\iint_{\mathbb{R} \mathbb{R}} x \frac{f_{\xi, \eta}(x, y)}{f_{\eta}(y)} d x \cdot g(y) f_{\eta}(y) d y
    \end{equation*}
    
    Сокращая $f_{\eta}(y)$, получаем равенство левой и правой частей.
    
\chapter{Математическая статистика}

\section{Статистическая структура. Выборка. Статистика. Порядковые статистики. Вариационный ряд. Эмпирическая функция распределения}

\begin{defn}
{\it Статистическая структура}~--- совокупность $(\Omega, \mathcal{A}, \mathcal{F})$, где $\Omega$~---множество элементарных исходов, $\mathcal{A}$~--- $\sigma$-алгебра событий, $\mathcal{F}$~--- семейство вероятностных мер, определённых на $\mathcal{A}$, параметризованное одно- или многомерным числовым параметром: $\mathcal{F} = (\mathcal{F}_{\theta}\,|\,\theta \in \Theta \subset R^{m})$.
\end{defn}

\begin{defn}
{\it Выборка} $\mathbf{X} = (X_{1}, \ldots, X_{n})$ объёма $n$~--- набор из $n$ независимых и одинаково распределённых случайных величин, имеющих такое же распределение, как и наблюдаемая случайная величина $\xi$.
\end{defn}

До того, как эксперимент проведён, выборка~--- набор случайных величин, после~--- набор чисел из множества возможных значений случайной величины. Числовой набор $\mathbf{X}(\omega_0) = (X_{1}(\omega_0), \ldots, X_{n}(\omega_0)) = (x_1, \ldots, x_n)$~--- {\it реализация выборки} на элементарном исходе $\omega_0$.

\begin{defn}
{\it Статистика}~--- измеримая функция от выборки $T(\mathbf{X})$.
\end{defn}

\begin{defn}
{\it Вариационный ряд}~--- выборка $X_{1}, \ldots, X_{n}$, упорядоченная по возрастанию на каждом элементарном исходе.
\end{defn}
Вариационный ряд строится следующим образом:
\begin{gather*}
    X_{(1)}(\omega)=\min (X_{1}(\omega), \ldots, X_{n}(\omega)) \\
    X_{(k)}(\omega)=\{\forall \omega \in \Omega \Rightarrow \exists m \leqslant i_{1}, \ldots, i_{k-1}, i_{k}, i_{k+1}, \ldots, i_{n} \leqslant n, i_{j} \neq i_{m}(j \neq m): \\ 
    X_{(k)}(\omega)=X_{i_{k}}(\omega) \\
    X_{i_{1}}(\omega), \ldots, X_{i_{k-1}}(\omega) \leqslant X_{i_{k}}(\omega); X_{i_{k+1}}(\omega), \ldots, X_{i_{n}}(\omega)>X_{i_{k}}(\omega)\}, 2 \leqslant k \leqslant n-1 \\
    X_{(n)}(\omega)=\max (X_{1}(\omega), \ldots, X_{n}(\omega))
\end{gather*}

Элемент $X_{(k)}$~--- {\it $k$-я порядковая статистика}.

\begin{defn}
{\it Эмпирическая функция распределения}, построенная по выборке $X_{1}, \ldots, X_{n}$ объёма $n$~--- случайная функция $F_{n}^{*}: \mathbb{R} \times \Omega \rightarrow[0,1]$, при каждом $y \in \mathbb{R}$ равная:
$F_{n}^{*}(y) =\frac{1}{n} \sum_{i=1}^{n} \mathrm{I}\left(X_{i}<y\right)$
\end{defn}

Эмпирическая функция распределенния строится по вариационному ряду следующим образом:

\begin{equation*}
    F_{n}^{*}(y)=\left\{\begin{array}{ll}
    0, & \text { если } y \leqslant X_{(1)} \\
    \frac{k}{n}, & \text { если } X_{(k)}<y \leqslant X_{(k+1)} \\
    1 & \text { при } y>X_{(n)}
    \end{array}\right.
\end{equation*}

\begin{exmp}
Найдём эмпирические функции распределения для крайних порядковых статистик.

\begin{gather*}
    \begin{aligned}
        F_{(1)}(x)=\mathbb{P}(X_{(1)} < x) 
    = 1 - \mathbb{P} (\mathrm{X}_{(1)} \geq x) 
    = 1 - \mathbb{P}(x_{1} \geq x, \ldots, x_{n} \geq x) = \\
    = 1 - \prod_{i=1}^{n} \mathbb{P}(x_{i} \geq x) 
    = 1 - (\mathbb{P}({x}_{1} \geq x))^{n} 
    = 1 - (1 - F(x))^{n} 
    \end{aligned} \\
    \begin{aligned}
        F_{(n)}(x) 
        = \mathbb{P}(X_{(n)} < x) 
        = \mathbb{P}(x_{1} < x, \ldots, x_{n} < x) = \\
        = \prod_{i=1}^{n} \mathbb{P}(x_{i} < x) 
        = (\mathbb{P}({x}_{1} < x))^{n} 
        = F^{n}(x)
    \end{aligned}
\end{gather*}
\end{exmp}

\begin{thm} Свойства эмпирической функции распределения:
\begin{enumerate}
    \item Пусть $X_{1}, \ldots, X_{n}$~--- выборка из распределения $\mathcal{F}$ с функцией распределения $F$ и пусть $F_{n}^{*}$ — эмпирическая функция распределения, построенная по этой выборке. Тогда $F_{n}^{*}(y) \xrightarrow[n \to \infty]{\mathrm{p}} F(y)$ для любого $y \in \mathbb{R}.$
    \item Для любого y $\in \mathbb{R}$:
    \begin{enumerate}[label={\arabic*)}]
        \item $\mathbb{E} F_{n}^{*}(y)=F(y)$, т.е. $F_{n}^{*}(y)$~--- несмещённая оценка для $F(y)$.
        \item $\mathbb{D} F_{n}^{*}(y)=\cfrac{F(y)(1-F(y))}{n}$
        \item $\sqrt{n}(F_{n}^{*}(y)-F(y)) \Rightarrow \mathbf{N}(0, (1-F(y))F(y))$, т.е. $F_{n}^{*}(y)$~--- асимптотически нормальная оценка для $F(y)$.
        \item $n F_{n}^{*}(y) \sim \mathbf{B}(n, F(y))$
    \end{enumerate}
\end{enumerate}
\end{thm}
\pagebreak
\begin{proof}\leavevmode
\begin{enumerate}
    \item $F_{n}^{*}(y)=\frac{1}{n} \sum_{i=1}^{n} \mathrm{I}(X_{i}<y)$, при этом случайные величины $\mathrm{I}(X_{1}<y),~ \mathrm{I}(X_{2}<y), \ldots$ независимы и одинаково распределены, их математическое ожидание конечно:
    \begin{equation*}
        \mathbb{E}\mathrm{I}(X_{1}<y)=1 \cdot \mathrm{P}(X_{1}<y)+0 \cdot \mathrm{P}(X_{1} \geqslant y)=\mathrm{P}(X_{1}<y)=F(y)<\infty
    \end{equation*}
    Следовательно, применим ЗБЧ в форме Хинчина:
    \begin{equation*}
        F_{n}^{*}(y)=\cfrac{\sum_{i=1}^{n} \mathrm{I}(X_{i}<y)}{n} \xrightarrow[]{\mathrm{p}} \mathbb{E}\mathrm{I}(X_{1}<y)=F(y) 
    \end{equation*}
    \item Заметим:
    \begin{gather*}
        \mathrm{I}(X_{1}<y) \sim  \mathbf{Bi}(F(y)) \Rightarrow \mathbb{E}\mathrm{I}(X_{1}<y) = F(y) \\
        \mathbb{D}\mathrm{I}(X_{1}<y) = F(y)(1-F(y))
    \end{gather*}
    \begin{enumerate}[label={\arabic*)}]
        \item Случайные величины $\mathrm{I}(X_{i}<y)$ одинаково распределены, поэтому:
        \begin{equation*}
            \mathbb{E} F_{n}^{*}(y)=\mathbb{E} \cfrac{\sum_{i=1}^{n} \mathrm{I}(X_{i}<y)}{n}=\cfrac{\sum_{i=1}^{n} \mathbb{E}\mathrm{I}(X_{i}<y)}{n}=\cfrac{n \mathbb{E}\mathrm{I}(X_{1}<y)}{n}=F(y)  
        \end{equation*}
        
        \item Случайные величины $\mathrm{I}(X_{i}<y)$ независимы и одинаково распределены, поэтому:
        \begin{multline*}
            \mathbb{D}\mathrm{I}_{n}^{*}(y)
            = \mathbb{D} \cfrac{\sum_{i=1}^{n} \mathrm{I}(X_{i}<y)}{n}
            = \cfrac{\sum_{i=1}^{n} \mathbb{D}\mathrm{I}(X_{i}<y)}{n^{2}}
            = \\
            = \cfrac{n\mathbb{D}\mathrm{I}(X_{1}<y)}{n^{2}}
            = \cfrac{F(y)(1-F(y))}{n}
        \end{multline*}
    
        \item Применим ЦПТ:
        \begin{multline*}
            \sqrt{n}\left(F_{n}^{*}(y)-F(y)\right)
            = \sqrt{n}\left(\cfrac{\sum \mathrm{I}(X_{i}<y)}{n}-F(y)\right) 
            = \\
            = \cfrac{\sum_{i=1}^{n} \mathrm{I}(X_{i}<y)-n F(y)}{\sqrt{n}} 
            = \cfrac{\sum_{i=1}^{n} \mathrm{I}(X_{i}<y)-n \mathbb{E}\mathrm{I}(X_{1}<y)}{\sqrt{n}} 
            \Rightarrow \\
            \Rightarrow \mathbf{N}(0, \mathbb{D}\mathrm{I}(X_{1}<y))
            = \mathbf{N}(0, (1-F(y))F(y))
        \end{multline*}
        \item Следует из устойчивости по суммированию биномиального распределения. Поскольку $\mathrm{I}\left(X_{i}<y\right)$ независимы и имеют распределение Бернулли $\mathbf{Bi}(F(y))$, то их сумма
        \begin{equation*}
            n F_{n}^{*}(y)=\mathrm{I}\left(X_{1}<y\right)+\ldots+\mathrm{I}\left(X_{n}<y\right)
        \end{equation*}
        имеет биномиальное распределение $\mathbf{B}(n, F(y))$.
    \end{enumerate}
\end{enumerate}  
\end{proof}

\section{Выборочные моменты. Их свойства}

Рассмотрим случайную величину $\xi^{*}$ с эмпирическим распределением, введём для последнего числовые характеристики.

\begin{defn}
{\it Выборочное математическое ожидание:} 
\begin{equation*}
    \tilde{\mathbb{E}} \xi^{*}=\sum_{i=1}^{n} \frac{1}{n} X_{i}=\frac{1}{n} \sum_{i=1}^{n} X_{i}=\overline{X}
\end{equation*}

Выборочное матожидание функции $g(\xi^{*})$:
\begin{equation*}
    \tilde{\mathbb{E}} g\left(\xi^{*}\right)=\frac{1}{n} \sum_{i=1}^{n} g\left(X_{i}\right)=\overline{g(\mathbf{X})}
\end{equation*}
\end{defn}

\begin{defn}
{\it Выборочная дисперсия:}
\begin{equation*}
    \tilde{\mathbb{D}} \xi^{*}=\sum_{i=1}^{n} \frac{1}{n}(X_{i}-\tilde{\mathbb{E}} \xi^{*})^{2}=\frac{1}{n} \sum_{i=1}^{n}(X_{i}-\overline{X})^{2}=S^{2}
\end{equation*}
\end{defn}

\begin{defn}
{\it Несмещённая выборочная дисперсия:} 
\begin{equation*}
    S_{0}^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}
\end{equation*}
\end{defn}

\begin{defn}
{\it Выборочный момент $k$-го порядка:}
\begin{equation*}
    \tilde{\mathbb{E}}(\xi^{*})^{k}=\sum_{i=1}^{n} \frac{1}{n} X_{i}^{k}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{k}=\overline{X^{k}}
\end{equation*}
\end{defn}

Все вышеперечисленные характеристики являются случайными величинами как функции от выборки $X_{1}, \ldots, X_{n}$ и оценками для истинных моментов искомого распределения.

\begin{thm}
Выборочное среднее $\overline{X}$ является несмещённой, состоятельной и асимптотически нормальной оценкой для теоретического среднего (математического ожидания):

\begin{enumerate}[label={\arabic*.}]
    \item Если $\mathbb{E}|X_{1}|<\infty$, то $\mathbb{E}\overline{X}=\mathbb{E} X_{1}=a$
    \item Если $\mathbb{E}|X_{1}|<\infty$, то $\overline{X} \xrightarrow[]{\mathrm{p}} \mathbb{E} X_{1}=a$ при $n \rightarrow \infty$.
    \item Если $\mathbb{D} X_{1}<\infty, \quad \mathbb{D} X_{1} \neq 0$, то $\sqrt{n}(\overline{X}-\mathbb{E} X_{1}) \Rightarrow \mathbf{N}(0, \mathbb{D} X_{1})$.
\end{enumerate}
\end{thm}

\begin{proof}
\begin{enumerate}[label={\arabic*.}]
    \item $\mathbb{E} \overline{X}=\frac{1}{n}(\mathbb{E} X_{1}+\ldots+\mathbb{E} X_{n})=\frac{1}{n} \cdot n \mathbb{E} X_{1}=\mathbb{E} X_{1}=a$
    \item Из ЗБЧ в форме Хинчина:
    \begin{equation*}
        \overline{X}
        = \cfrac{X_{1}+\ldots+X_{n}}{n} \xrightarrow[]{\mathrm{p}} \mathbb{E} X_{1} 
        = a
    \end{equation*}

    \item Из ЦПТ:
    \begin{equation*}
        \sqrt{n}\left(\overline{X}-\mathbb{E} X_{1}\right) 
        = \cfrac{\sum_{i=1}^{n} X_{i}-n \mathbb{E} X_{1}}{\sqrt{n}} 
        \Rightarrow \mathbf{N}(0, \mathbb{D} X_{1})
    \end{equation*}
\end{enumerate}
\end{proof}

\begin{rmrk}
    Аналогичными свойствами обладает выборочный $k$-й момент, являющийся несмещённой, состоятельной и асимптотически нормальной оценкой для теоретического $k$-го момента.
\end{rmrk}

\begin{thm}
Пусть $\mathbb{D} X_{1}<\infty$.
\begin{enumerate}
    \item Выборочные дисперсии $S^{2}$ и $S^{2}_0$ являются состоятельными оценками для истинной дисперсии:
    \begin{equation*}
        S^{2} \xrightarrow[]{\mathrm{p}} \mathbb{D} X_{1}=\sigma^{2}, \quad S_{0}^{2} \xrightarrow[]{\mathrm{p}} \mathbb{D} X_{1}=\sigma^{2}
    \end{equation*}
    \item Величина $S^{2}$~--- смещённая оценка дисперсии, а $S^{2}_0$~--— несмещённая:
    \begin{equation*}
        \mathbb{E} S^{2}=\frac{n-1}{n} \mathbb{D} X_{1}=\frac{n-1}{n} \sigma^{2} \neq \sigma^{2}, \quad \mathbb{E} S_{0}^{2}=\mathbb{D} X_{1}=\sigma^{2}
    \end{equation*}
    
    \item Если $0 \neq \mathbb{D}(X_{1}-\mathbb{E}X_{1})^{2}<\infty$, то $S^{2}$ и $S^{2}_0$ являются асимптотически нормальными оценками истинной дисперсии:
    \begin{equation*}
        \sqrt{n}\left(S^{2}-\mathbb{D} X_{1}\right) \Rightarrow \mathbf{N}(0, \mathbb{D}(X_{1}-\mathbb{E} X_{1})^{2})
    \end{equation*}
\end{enumerate}
\end{thm}

\begin{proof}
\begin{enumerate}
    \item $S^{2}=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}=\overline{X^{2}}-(\overline{X})^{2}$

    Используя состоятельность первого и второго выборочных моментов и свойства сходимости по вероятности, получаем:
    \begin{gather*}
        S^{2}=\overline{X^{2}}-(\overline{X})^{2} \xrightarrow[]{\mathrm{p}} \mathbb{E} X_{1}^{2}-\left(\mathbb{E} X_{1}\right)^{2}=\sigma^{2} \\
        \cfrac{n}{n-1} \rightarrow 1 \Rightarrow S_{0}^{2}=\frac{n}{n-1} S^{2} \xrightarrow[]{\mathrm{p}} \sigma^{2}
    \end{gather*}
    
    \item Используя несмещённость первого и второго выборочных моментов:
    \begin{multline*}
        \mathbb{E} S^{2} = \mathbb{E}\left(\overline{X^{2}}-(\overline{X})^{2}\right)
        = \mathbb{E} \overline{X^{2}}-\mathbb{E}(\overline{X})^{2}
        = \mathbb{E} X_{1}^{2}-\mathbb{E}(\overline{X})^{2} = \\
        = \mathbb{E} X_{1}^{2}-\left((\mathbb{E} \overline{X})^{2}+\mathbb{D} \overline{X}\right)
        = \mathbb{E} X_{1}^{2}-\left(\mathbb{E} X_{1}\right)^{2}-\mathbb{D}\left(\frac{1}{n} \sum_{i=1}^{n} X_{i}\right) = \\
        = \sigma^{2}-\frac{1}{n^{2}} n \mathbb{D} X_{1}
        = \sigma^{2}-\frac{\sigma^{2}}{n}
        = \frac{n-1}{n} \sigma^{2}
    \end{multline*}
    
    Откуда следует:
    \begin{equation*}
        \mathbb{E} S_{0}^{2}=\frac{n}{n-1} \mathbb{E} S^{2}=\sigma^{2}
    \end{equation*}
    
    \item Введём случайные величины $Y_{i}=X_{i}-a$; $\mathbb{E}Y_{i} = 0, \mathbb{D} Y_{1}=\mathbb{D} X_{1}=\sigma^{2}$.
    \begin{gather*}
        S^{2}=\frac{1}{n} \sum_{i=1}^{n}(X_{i}-\overline{X})^{2}=\frac{1}{n} \sum_{i=1}^{n}(X_{i}-a-(\overline{X}-a))^{2}=\overline{Y^{2}}-(\overline{Y})^{2} \\
        \begin{aligned}
            \sqrt{n}(S^{2}-\sigma^{2}) = \sqrt{n}(\overline{Y^{2}}-(\overline{Y})^{2}-\sigma^{2})
            = \sqrt{n}t(\overline{Y^{2}}-\mathbb{E} Y_{1}^{2})-\sqrt{n}(\overline{Y})^{2} = \\
            =\frac{\sum_{i=1}^{n} Y_{i}^{2}-n \mathbb{E} Y_{1}^{2}}{\sqrt{n}}-\overline{Y} \cdot \sqrt{n} \overline{Y} \Rightarrow \mathbf{N}(0, \mathbb{D}(X_{1}-a)^{2})
    \end{aligned}
    \end{gather*}
    поскольку $\cfrac{\sum_{i=1}^{n} Y_{i}^{2}-n \mathbb{E} Y_{1}^{2}}{\sqrt{n}} \Rightarrow \mathbf{N}(0, \mathbb{D} Y_{1}^{2})$ по ЦПТ, а $\overline{Y} \cdot \sqrt{n} \overline{Y} \Rightarrow 0$ как произведение последовательностей $\overline{Y} \xrightarrow[n \rightarrow \infty]{p} 0$ и $\sqrt{n} \overline{Y} \Rightarrow \mathbf{N}(0, \mathbb{D} X_{1})$.
\end{enumerate}
\end{proof}

\section{Точечная оценка. несмещённость, состоятельность, оптимальность. Теорема о единственности оптимальной оценки}
\begin{defn}
{\it Статистика} или {\it оценка}~--- измеримая функция от выборки $T(\mathbf{X})$.
\end{defn}

\begin{defn}
{\it Несмещённая оценка} парамаетра $\theta$~--- статистика $T(\mathbf{X})$, т.ч. $\forall \theta \in \Theta: \mathbb{E}T(\theta) = \theta$.
\end{defn}

\begin{defn}
{\it Асимптотически несмещённая оценка} параметра $\theta$~--- статистика $T(\mathbf{X})$, т.ч. $\forall \theta \in \Theta: \mathbb{E}T(\theta) \xrightarrow[n \rightarrow \infty]{} \theta$.
\end{defn}

\begin{defn}
{\it Состоятельная оценка} параметра $\theta$~--- статистика $T(\mathbf{X})$, т.ч. $\forall \theta \in \Theta: T(\theta) \xrightarrow[n \rightarrow \infty]{p} \theta$.
\end{defn}

Оценки также могут вводиться и для функций $\tau(\theta)$ параметра $\theta$; они обладают всеми аналогичными свойствами.

Несмещённость означает отсутствие ошибки «в среднем», т. е. при систематическом использовании данной оценки. Несмещённость является желательным, но не обязательным свойством оценок. Достаточно, чтобы смещение оценки (разница между её средним значением и истинным параметром) уменьшалось с ростом объёма выборки. Поэтому асимптотическая несмещённость является весьма желательным свойством оценок. Свойство состоятельности означает, что последовательность оценок приближается к неизвестному параметру при увеличении количества наблюдений. В отсутствие этого свойства оценка совершенно «несостоятельна» как оценка.

\begin{rmrk}
    Отметим некоторые свойства несмещённых и состоятельных оценок.
    \begin{enumerate}
        \item Несмещённые оценки не единственны.
        
        К примеру в качестве несмещённой оценки для математического ожидания $\mathbb{E} X$ могут выступать $\mathbb{E} X_{1}$ или$\mathbb{E} \overline(\mathbf{X})$.
        
        \item Несмещённые оценки могут не существовать.
        \begin{exmp}
            Дано распределение $\mathbf{Pois}(\theta)$, над которым произведено одно наблюдение. Найти несмещённую оценку для функции $\tau(\theta) = \cfrac{1}{\theta}$.
                \begin{equation*}
                    \mathbb{E}T(\theta) 
                    = \mathlarger{\mathlarger{\sum}}_{x=0}^{\infty}T(x)e^{-\theta}\cfrac{\theta^{x}}{x!} 
                    = \cfrac{1}{\theta}
                    \Rightarrow \mathlarger{\mathlarger{\sum}}_{x=0}^{\infty}T(x)\cfrac{\theta^{x+1}}{x!}
                    = \mathlarger{\mathlarger{\sum}}_{r=0}^{\infty}\cfrac{\theta^{r}}{r!}
                    \Rightarrow T(x) \equiv \cfrac{1}{\theta}
                \end{equation*}
            Т.к. полученная статистика зависит от $\theta$, искомой несмещённой оценки для $\tau(\theta)$ не существует.
        \end{exmp}
        
    \item Несмещённые оценки могут существовать, но быть бессмысленными.
    \begin{exmp}
        Дано отрицательное биноминальное распределение $\operatorname{B}(1, \theta)$, над которым произведено одно наблюдение. Найти несмещённую оценку для параметра $\theta$.
        \begin{gather*}
            \mathbb{E}T(\theta) 
            = \sum_{x=0}^{\infty}T(x)\theta^{x} 
            = \cfrac{\theta}{1-\theta} 
            = \sum_{r=1}^{\infty}\theta^{r} \\
            T(x) = 
            \left\{\begin{array}{ll}
                0, & \text { если } x = 0 \\
                1, & \text { если } x \geq 1
            \end{array}\right.
        \end{gather*}
    Значения этой статистики не принадлежат параметрическому множеству $\Theta = (0, 1)$, следовательно, эта оценка бессмысленна.
    \end{exmp}
    
    \item Состоятельные оценки не единственны.
    
    К примеру, выборочная дисперсия $S^{2}$ и несмещённая выборочная дисперсия $S_0^{2}$ являются состоятельными оценками теоретической дисперсии.
    
    \item Состоятельные оценки могут быть смещёнными.
    
    Как было показано ранее, выборочная дисперсия является состоятельной, но смещённой оценкой теоретической дисперсии
    
    \end{enumerate}
\end{rmrk}

Для несмещённой оценки $T(\mathbf{X})$ параметра $\theta$: $\mathbb{E}_{\theta}(T(\mathbf{X})-\theta)^{2}=\mathbb{D}_{\theta} T(\mathbf{X})$. Т.к. для двух разных оценок $T_1(\mathbf{X})$, $T_2(\mathbf{X})$ соответствующие дисперсии могут быть несравнимыми, введём понятие оптимальной оценки.

\begin{defn}
{\it Оптимальная оценка} параметра $\theta$~--- статистика $T(\mathbf{X})$, т.ч.:
\begin{enumerate}
    \item $T(\mathbf{X})$~--- несмещённая.
    \item $T(\mathbf{X})$ имеет равномерно минимальную дисперсию, т.е. для любой другой несмещённой оценки $T_{1}(\mathbf{X})$ параметра $\theta$: $\mathbb{D}_{\theta} T(\mathbf{X}) \leqslant \mathbb{D}_{\theta} T_{1}(\mathbf{X})~ \forall X$.
\end{enumerate}
\end{defn}
\pagebreak
\begin{thm}
Если существует оптимальная оценка параметра $\theta$, то она единственна.
\end{thm}

\begin{proof}
Предположим обратное: пусть существуют две оптимальные оценки $T_1(\mathbf{X})$ и $T_2(\mathbf{X})$ параметра $\theta$. Тогда в силу их несмещённости: $\mathbb{E}_{\theta} T_{1}(\mathbf{X})=\mathbb{E}_{\theta} T_{2}(\mathbf{X})=T(\theta)$, а а в силу того, что они имеют равномерно минимальную дисперсию: $\mathbb{D}_{\theta} T_{1}(\mathbf{X})=\mathbb{D}_{\theta} T_{2}(\mathbf{X})~ \forall \theta$.

Введём новую статистику: 
\begin{equation}
    T_{3}(\mathbf{X})=\cfrac{T_{1}(\mathbf{X})+T_{2}(\mathbf{X})}{2}
\end{equation}

Так как $\mathbb{E}_{\theta} T_{3}(\mathbf{X})=\cfrac{\mathbb{E}_{\theta} T_{1}(\mathbf{X})+\mathbb{E}_{\theta} T_{2}(\mathbf{X})}{2}=\theta$, то $T_{3}(\mathbf{X})$~--- несмещённая оценка параметра $\theta$.

Имеем также:
\begin{equation*}
    \mathbb{D}_{\theta} T_{3}(\mathbf{X})=\cfrac{\mathbb{D}_{\theta}\left(T_{1}(\mathbf{X})+T_{2}(\mathbf{X})\right)}{4} =
    \cfrac{\mathbb{D}_{\theta} T_{1}(\mathbf{X})+\mathbb{D}_{\theta} T_{2}(\mathbf{X})+2 \operatorname{cov}\left(T_{1}(\mathbf{X}) T_{2}(\mathbf{X})\right)}{4}
\end{equation*}

В силу свойства
\begin{equation*}
    \mathbb{E}_{\theta} \xi^{2}<\infty, \mathbb{E}_{\theta} \eta^{2}<\infty \Rightarrow|\operatorname{cov}(\xi, \eta)| = | \mathbb{E}(\xi-\mathbb{E} \xi)(\eta-\mathbb{E} \eta)| \leqslant \sqrt{\mathbb{D} \xi} \sqrt{\mathbb{D} \eta},
\end{equation*}
где равенство достигается тогда и только тогда, когда $\xi=a \eta+b$, получаем:
\begin{equation*}
    \mathbb{D}_{\theta} T_{3}(\mathbf{X}) \leqslant \cfrac{\mathbb{D}_{\theta} T_{1}(\mathbf{X})+\mathbb{D}_{\theta} T_{2}(\mathbf{X})+2 \sqrt{\mathbb{D}_{\theta} T_{1}(\mathbf{X})} \sqrt{\mathbb{D}_{\theta} T_{2}(\mathbf{X})})}{4} =\mathbb{D}_{\theta} T_{1}(\mathbf{X})
\end{equation*}

В силу того, что $T_1(\mathbf{X})$ и $T_2(\mathbf{X})$ — оптимальные, дисперсия $T_3(\mathbf{X})$ не может быть меньше дисперсии $T_1(\mathbf{X})$, следовательно, справедливо равенство, достигаемое при следующих условиях:
\begin{equation*}
\begin{aligned}
    T_{1}(\mathbf{X})=a T_{2}(\mathbf{X})+b \Rightarrow \mathbb{E} T_{1}(\mathbf{X})
    = a \mathbb{E} T_{2}(\mathbf{X})+b 
    \Leftrightarrow \\
    \Leftrightarrow T(\theta) = a T(\theta)+b~ \forall \theta \Rightarrow a = 1, b = 0
\end{aligned}
\end{equation*}

\end{proof}

\section{Функция правдоподобия. Достаточные статистики, полные статистики. Теорема факторизации}

В зависимости от типа распределения $\mathcal{F}_\theta$ обозначим через $f_{\theta}(y)$ одну из следующих функций:
\begin{equation*}
    f_{\theta}(y) =
    \left\{\begin{array}{ll}
    \text { плотность } f_{\theta}(y), & \text { если } \mathcal{F}_{\theta} \text { абсолютно непрерывно, } \\
    P_{\theta}\left(X_{1}=y\right), & \text { если } \mathcal{F}_{\theta} \text { дискретно. }
    \end{array}\right.
\end{equation*}

\begin{defn}
{\it Функция правдоподобия} выборки $\mathbf{X}$:
\begin{equation*}
    L(\mathbf{X} , \theta)=f_{\theta}\left(X_{1}\right) \cdot f_{\theta}\left(X_{2}\right) \cdot \ldots \cdot f_{\theta}\left(X_{n}\right)=\prod_{i=1}^{n} f_{\theta}\left(X_{i}\right)
\end{equation*}
\end{defn}

В дискретном случае функция правдоподобия принимает вид:
\begin{equation*}
\begin{aligned}
    L(\mathbf{x} , \theta)=\prod_{i=1}^{n} f_{\theta}(x_{i}) 
    = \mathrm{P}_{\theta}(X_{1}=x_{1}) \cdot \ldots \cdot \mathrm{P}_{\theta}(X_{n}=x_{n}) = \\
    = \mathrm{P}_{\theta}(X_{1}=x_{1}, \ldots, X_{n}=x_{n})
\end{aligned}
\end{equation*}

Таким образом, смысл функции правдоподобия~--- вероятность попасть в заданную точку при соответствующем параметре $\theta$ в дискретном случае; для абсолютно непрерывного аналогично~--- вероятность попасть в куб с центром в $x_1, \ldots, x_n$ и сторонами $dx_1, \ldots, dx_n$.

\begin{defn}
{\it Достаточная статистика} для параметра $\theta$~--- статистика $T(\mathbf{X})$, т.ч. $\forall t,~ \forall B \in \mathfrak{B}(\mathbb{R}^{n})$ условное распределение $\mathbb{P}(X_1, \ldots, X_n \in B\,|\,T=t)$ не зависит от параметра $\theta$.
\end{defn}

Иными словами, если значение статистики $T$ известно и фиксировано, то даже знание её распределения больше не даёт никакой информации о параметре; достаточно лишь вычислить $T$ по выборке.

\begin{thm}[Критерий факторизации]
$T(\mathbf{X})$~--- достаточная статистика $\Leftrightarrow$ её функция правдоподобия представима в виде $L(\mathbf{X}_{1}, \ldots, X_{n} , \theta) \stackrel{\text{п.н.}}{=} h(\mathbf{X}) \cdot \Psi(S, \theta)$
\end{thm}

\begin{proof}
Рассмотрим только дискретный случай. Пусть $T(\mathbf{X})$~--- достаточная статистика. Если $T(\mathbf{X})=t$, то событие $\{\mathbf{X}=\mathbf{x}\} \subseteq \{T(\mathbf{X})=t\}$. Поэтому
\begin{multline*}
    L(\mathbf{x}, \theta) = \mathrm{P}_{\theta}(\mathbf{X}=\mathbf{x})=\mathrm{P}_{\theta}(\mathbf{X}=\mathbf{x} | T(\mathbf{X})=t) =\\
    = \underbrace{\mathrm{P}_{\theta}(T(\mathbf{X})=t)}_{g(T(\mathbf{x}), \theta)} \underbrace{\mathrm{P}_{\theta}(\mathbf{X}=\mathbf{x} | T(\mathbf{X})=t)}_{h(\mathbf{x}, t)}
\end{multline*}

Пусть теперь функция правдоподобия имеет вид $L(\mathbf{x}, \theta)=g(T(\mathbf{x}, \theta) h(\mathbf{x})$. Тогда, если $x$ таково, что $T(\mathbf{x})=t$, то:
\begin{multline*}
    \mathrm{P}(\mathbf{X}=\mathbf{x} | T(\mathbf{X})=t) =\frac{\mathrm{P}(\mathbf{X}=\mathbf{x}, T(\mathbf{X})=t)}{\mathrm{P}(T(\mathbf{X})=t)}
    =\frac{\mathrm{P}(\mathbf{X}=\mathbf{x})}{\sum_{\mathbf{x}^{\prime}: T(\mathbf{x}^{\prime})=t} \mathrm{P}(\mathbf{X}=\mathbf{x}^{\prime})} = \\
    = \frac{g(t, \theta) h(\mathbf{x})}{\sum_{\mathbf{x}^{\prime}: T(\mathbf{x}^{\prime})=t} g(t, \theta) h(\mathbf{x}^{\prime})}
    = \frac{h(\mathbf{x})}{\sum\limits_{\mathbf{x}^{\prime}: T(\mathbf{x}^{\prime})=t} h(\mathbf{x}^{\prime})}
\end{multline*}
\end{proof}

\begin{defn}
{\it Полная статистика} для параметра $\theta$~--- статистика $T(\mathbf{X})$, т.ч. $\mathbb{E} g(T)=0~\forall \theta \in \Theta \Rightarrow g(T) \stackrel{\text{п.н.}}{=}0$
\end{defn}

\section{Неравенство Рао—Крамера. Эффективные оценки}

Пусть $X_1, \ldots, X_n$  —  некоторая выборка с функцией правдоподобия $L(\mathbf{X}, \theta)$ относительно некоторой меры $\mu$. Введём функцию $\varphi(\theta)=\int\limits_{\mathbf{R}^{n}} T(x) L(x, \theta) \mu(d x)<\infty$, в дальнейшем считая, что она дифференцируема необходимое число раз.

\begin{defn}
Функция правдоподобия $L(\mathbf{X}, \theta)$ {\it удовлетворяет условиям регулярности для $m$-й производной}, если существует
\begin{equation*}
    \cfrac{d^{m} \varphi(\theta)}{d \theta^{m}}=\int\limits_{\mathbb{R}^{n}} T(x) \cfrac{\partial^{m} L(x, \theta)}{\partial \theta^{m}} \mu(d x),
\end{equation*}
причём множество $\left\{ {x\,|\,L(x,\theta) > 0} \right\}$ не зависит от параметра $\theta$.
\end{defn}

\begin{thm}[Неравенство Рао-Крамера]
Пусть $X_1, \ldots, X_n$ — выборка, $L(\mathbf{X}, \theta)$ удовлетворяет условиям регулярности для первой производной и $\theta$  —  дифференцируемая функция $\theta$. Тогда:
\begin{enumerate}
    \item Для любой $~T(\mathbf{X})$,~--- несмещённой оценки параметра $\theta$, справедливо неравенство:
    \begin{gather*}
        \mathbb{D}_{\theta} T(\mathbf{X}) \geq \cfrac{(\theta^{*})^{2}}{\mathbb{E}_{\theta} U^{2}(X, \theta)}~\forall \theta \in \Theta, \\
        \text{где}~ U(X, \theta)=\cfrac{\partial \ln L(\mathbf{X}, \theta)}{\partial \theta}~\text{(функция вклада)}
    \end{gather*}
    
    \item Равенство достигается $\Leftrightarrow \exists~ a_n(\theta):~ T(\mathbf{X})-\theta=a_{n}(\theta) \cdot U(X, \theta)$
\end{enumerate}
\end{thm}

\begin{proof}
$\int L(x, \theta) \mu(d x)=1 \Rightarrow \int \cfrac{\partial L(x, \theta)}{\partial \theta} \mu(d x)=0$

Из условий регулярности $L(\mathbf{X}, \theta)$ для следует:
\begin{equation*}
    \int T(x) L(x, \theta) \mu(d x)=\mathbb{E}_{\theta} T(\mathbf{X})=T(\theta) \Rightarrow \int T(x) \cfrac{\partial L(x, \theta)}{\partial \theta} \mu(d x)=(\theta)^{*}
\end{equation*}

Заметим, что
\begin{equation*}
    \cfrac{\partial L(x, \theta)}{\partial \theta}=\cfrac{\partial \ln L(x, \theta)}{\partial \theta} \cdot L(x, \theta)
\end{equation*}

Откуда следует:
\begin{gather*}
    \int U(x, \theta) L(x, \theta) \mu(d x)=0 \Leftrightarrow \mathbb{E}_{\theta} U(X, \theta)=0 \\
\int T(x) U(x, \theta) L(x, \theta) \mu(d x)=\theta^{*} \Leftrightarrow \mathbb{E}_{\theta} T(\mathbf{X}) U(X, \theta)=\theta^{*}
\end{gather*}

Вычитая из первого равенства, помноженного на $\theta$, второе, получаем:
\begin{equation*}
    \mathbb{E}_{\theta}(T(\mathbf{X})-T(\theta)) U(X, \theta)=\theta^{*}
\end{equation*}

В левой части полученного равенства стоит ковариация случайных величин $T(\mathbf{X})$ и $U(X,\theta)$:
\begin{equation*}
    \operatorname{cov}_{\theta}(T(\mathbf{X}), U(X, \theta))=\theta^{*}
\end{equation*}

Из неравенства Коши-Буняковского:
\begin{equation*}
    \left(\theta^{*}\right)^{2}=\operatorname{cov}_{\theta}^{2}(T(\mathbf{X}) U(X, \theta)) \leqslant \mathbb{D}_{\theta} T(\mathbf{X}) \mathbb{D}_{\theta} U(X, \theta)=\mathbb{D}_{\theta} T(\mathbf{X}) \mathbb{E}_{\theta} U^{2}(X, \theta)
\end{equation*}

...что равносильно п.1 теоремы:
\begin{equation*}
    \mathbb{D}_{\theta} T(\mathbf{X}) \geq \cfrac{\left[\theta^{*}\right]^{2}}{\mathbb{E}_{\theta} U^{2}(X, \theta)}
\end{equation*}

Неравенство достигается, если линейно связаны:
\begin{equation*}
    T(\mathbf{X})=\varphi(\theta) U(X, \theta)+\psi(\theta) \Rightarrow T(\theta)=\psi(\theta) \Rightarrow a_{n}(\theta)=\varphi(\theta)
\end{equation*}

\end{proof}

Рассмотрим некоторый класс оценок $K=\left\{\hat{\theta}\left(\mathbf{X}\right)\right\}$ параметра $\theta$.
\begin{defn}
    Говорят, что оценка $\theta^{*}\left(\mathbf{X}\right) \in K$ является эффективной оценкой параметра $\theta$ в классе $K$, если для любой другой оценки $\hat{\theta} \in K$ имеет место неравенство:
    \begin{equation*}
        E\left(\theta^{*}-\theta\right)^{2} \leqslant E(\hat{\theta}-\theta)^{2}~ \forall \theta \in \Theta
    \end{equation*}
\end{defn}
Обозначим класс несмещённых оценок:
\begin{equation*}
    K_{0}=\left\{\hat{\theta}\left(\mathbf{X}\right): E \hat{\theta}=\theta, \forall \theta \in \Theta\right\}
\end{equation*}
Оценка, эффективная в $K_0$ называется просто {\it эффективной}.

Для оценки $\theta^{*} \in K_{0}$ по определению дисперсии
\begin{equation*}
    \mathbb{E}\left(\theta^{*}-\theta\right)^{2}=\mathbb{E}\left(\theta^{*}-\mathbb{E} \theta^{*}\right)^{2}=\mathbb{D} \theta^{*}
\end{equation*}

Добавить Чернова стр 35

\begin{rmrk}
Если в неравенстве Рао---Крамера достигается равенство, то полученная оценка~--- эффективная.

Если существует эффективная оценка для функции $\tau(\theta)$, то ни для какой другой функции от $\theta$, кроме линейного преобразования $\tau(\theta)$, эффективной оценки существовать не будет. 
\end{rmrk}

\section{Теорема Рао—Блекуэлла—Колмогорова. Оптимальность оценок являющихся функцией полной достаточной статистики}

\begin{thm}[Теорема Рао—Блекуэлла—Колмогорова] Если оптимальная оценка параметра $\theta$ существует, то она является функцией от достаточной статистики.
\end{thm}

\begin{proof}
В доказательстве используются следующие свойства условного матожидания: 
\begin{gather*}
    \mathbb{E} f(x, z)=\mathbb{E}(\mathbb{E}(f(x, z) | z)) \\
    \mathbb{E}(g(z) | z)=g(z)
\end{gather*}

\begin{enumerate}
    \item Пусть $T(\mathbf{X})$~--- достаточная статистика, $T_1(\mathbf{X})$~--- несмещённая оценка параметра $\theta$, т.е. $\mathbb{E} T_{1}(\mathbf{X})=\theta$. Рассмотрим функцию $H(T)=\mathbb{E}\left(T_{1} | T\right)$. Тогда из первого свойства следует:
    \begin{equation*}
        \mathbb{E} H(T)=\mathbb{E}\left(\mathbb{E}\left(T_{1} |     T\right)\right)=\mathbb{E} T_{1}=\theta \Rightarrow H(T)     \text{~--- несмещённая оценка~} \theta
    \end{equation*}

    \item Докажем равномерную минимальность её дисперсии:
    \begin{multline*}
        \mathbb{E}((T_{1}-H(T))(H(T)-\theta)
        = \mathbb{E}(\mathbb{E}((T_{1}-H(T))(H(T)-\theta) | T)) 
        = \\
        = \mathbb{E}((H(T)-H(T))(H(T)-\theta))
        = 0
    \end{multline*}

    Тогда из свойств условного матожидания
    \begin{multline*}
        \mathrm{D}\left(T_{1}\right) 
        = \mathbb{E}\left(T_{1}-\theta\right)^{2}=\mathbb{E}\left(T_{1}-H(T)+H(T)-\theta\right)^{2} =\\
        = \mathbb{E}\left(T_{1}-H(T)\right)^{2}+\mathrm{D}(H(T)) \geqslant \mathrm{D}(H(T))
    \end{multline*}
\end{enumerate}
Таким образом, $H(T)$~--- оптимальная оценка $\theta$.

\end{proof}

\begin{thm}[Теорема Колмогорова]
Если $T(\mathbf{X})$~--- полная достаточная статистика, то она является оптимальной оценкой своего математического ожидания.
\end{thm}

\begin{proof}
Докажем, что $T(\mathbf{X})$ является единственной несмещённой оценкой для $\mathbb{E}T(\mathbf{X})$. Тогда $T(\mathbf{X})$ будет оптимальной оценкой. Предположим, что $T_1(\mathbf{X})$~--- оптимальная оценка для $\mathbb{E}T(\mathbf{X})$. Из теоремы Рао-Блекуэлла-Колмогорова получаем, что $T_{1}=H(T)$ и $\mathbb{E} T_{1}=\mathbb{E} T$. Тогда:

\begin{equation*}
    \mathbb{E} \underbrace{(T(\mathbf{X})-H(T(\mathbf{X})))}_{\varphi(T)}=0
\end{equation*}

Из условия полноты $T(\mathbf{X})$ следует, что $\varphi(T)=0$ с вероятностью 1, т.е. $T=H(T)$ с вероятностью 1.
\end{proof}

\section{Метод моментов. Свойства оценок, полученных методом моментов}

Пусть $X_1, \ldots, X_n$~--- выборка объёма $n$ из параметрического семейства распределений $\mathcal{F}_\theta$. Выберем функцию $g(y): \mathbb{R} \rightarrow \mathbb{R}$ так, чтобы существовал момент $\mathbb{E} g\left(X_{1}\right)=h(\theta)$ и функция $h(\theta)$ была обратима на $\Theta$. Разрешим полученное уравнение относительно $\theta$, а затем вместо истинного момента возьмём выборочный:

\begin{equation*}
    \theta=h^{-1}\left(\mathbb{E} g\left(X_{1}\right)\right), \quad \theta^{*}=h^{-1}(\overline{g(\mathbf{X})})=h^{-1}\left(\frac{1}{n} \sum_{i=1}^{n} g\left(X_{i}\right)\right)
\end{equation*}

Полученная оценка $\theta^{*}$~--- {\it оценка метода моментов} для параметра $\theta$. Чаще всего берут $g(y)=y^{k}$. В этом случае, при условии обратимости функции $h$ на $\Omega$:
\begin{equation*}
    \mathbb{E} X_{1}^{k}=h(\theta), \quad \theta=h^{-1}\left(\mathbb{E} X_{1}^{k}\right), \quad \theta^{*}=h^{-1}(\overline{X^{k}})=h^{-1}\left(\frac{1}{n} \sum_{i=1}^{n} X_{i}^{k}\right)
\end{equation*}

\begin{thm}
Пусть $\theta^{*}=h^{-1}(\overline{g(\mathbf{X})})$~--- оценка параметра $\theta$, полученная методом моментов, причём функция $h^{-1}$ непрерывна. Тогда оценка $\theta^{*}$ состоятельна.
\end{thm}

\begin{proof}
По ЗБЧ Хинчина имеем:

\begin{equation*}
    \overline{g(\mathbf{X})}=\frac{1}{n} \sum_{i=1}^{n} g\left(X_{i}\right) \xrightarrow[]{\mathrm{p}} \mathbb{E} g\left(X_{1}\right)=h(\theta)
\end{equation*}

Ввиду непрерывности функции $h^{-1}$:

\begin{equation*}
    \theta^{*}=h^{-1}(\overline{g(\mathbf{X})}) \xrightarrow[]{\mathrm{p}} h^{-1}\left(\mathbb{E} g\left(X_{1}\right)\right)=h^{-1}(h(\theta))=\theta
\end{equation*}
\end{proof}

\begin{defn}
{\it Асимптотически нормальная оценка} параметра $\theta$ с коэффициентом $\sigma^{2}(\theta)$~--- оценка $\theta^{*}$, т.ч. при $n \rightarrow \infty$ имеет место слабая сходимость к стандартному нормальному распределению: $\sqrt{n}(\theta^{*}-\theta) \Rightarrow \mathbf{N}(0, \sigma^{2}(\theta))$.
\end{defn}

\begin{lem}
Пусть функция $g(y)$ такова, что $0 \neq \mathbb{D} g\left(X_{1}\right)<\infty$. Тогда статистика $\overline{g(\mathbf{X})}$ является асимптотически нормальной оценкой для $\mathbb{E} g\left(X_{1}\right)$ с коэффициентом $\sigma^{2}(\theta)=\mathbb{D} g\left(X_{1}\right)$:

\begin{equation*}
    \sqrt{n} \cfrac{\overline{g(\mathbf{X})}-\mathbb{E} g\left(X_{1}\right)}{\sqrt{\mathbb{D} g\left(X_{1}\right)}} \Rightarrow \mathbf{N}(0,1)
\end{equation*}
\end{lem}

\begin{proof}
Следует непосредственно из ЦПТ.
\end{proof}

\begin{rmrk}
Следующая теорема утверждает асимптотическую нормальность оценок вида

\begin{equation*}
    \theta^{*}=H(\overline{g(\mathbf{X})})=H\left(\cfrac{g\left(X_{1}\right)+\ldots+g\left(X_{n}\right)}{n}\right)
\end{equation*}

которые обычно получаются при использовании метода моментов, при этом всегда $\theta=H\left(\mathbb{E} g\left(X_{1}\right)\right)$.
\end{rmrk}

\begin{thm}
Пусть функция $g(y)$ такова, что $0 \neq \mathbb{D} g\left(X_{1}\right)<\infty$, функция $H(y)$ дифференцируема в точке $a=\mathbb{E} g\left(X_{1}\right)$ и её производная в этой точке $H^{\prime}(a)=\left.H^{\prime}(y)\right|_{y=a}$ отлична от нуля. Тогда оценка $\theta^{*}=H(\overline{g(\mathbf{X})})$
является асимптотически нормальной
оценкой для параметра $\theta=H\left(\mathbb{E} g\left(X_{1}\right)\right)=H(a)$ с коэффициентом асимптотической нормальности $\sigma^{2}(\theta)=\left(H^{\prime}(a)\right)^{2} \cdot D g\left(X_{1}\right)$.
\end{thm}

\begin{proof}
Согласно ЗБЧ последовательность $\overline{g(\mathbf{X})}$ стремится к $a=\mathbb{E} g\left(X_{1}\right)$ по вероятности с ростом $n$: Функция

\begin{equation*}
    G(y)=\left\{\begin{array}{ll}
    \cfrac{H(y)-H(a)}{y-a}, & y \neq a \\
    H^{\prime}(a), & y=a
    \end{array}\right.  
\end{equation*}

по условию непрерывна в точке $a$: Поскольку сходимость по веро-
ятности сохраняется под действием непрерывной функции, получим,
что $G(\overline{g(\mathbf{X})}) \xrightarrow[]{\mathrm{p}} G(a)=H^{\prime}(a)$.

Заметим также, что по вышеприведённой лемме величина $\sqrt{n}(\overline{g(\mathbf{X})}-a)$ слабо сходится
к нормальному распределению $\mathbf{N}(0, \mathbb{D} g(X_{1}))$: Пусть $\xi$~--- случайная величина
из этого распределения. Тогда

\begin{equation*}
    \sqrt{n}(H(\overline{g(\mathbf{X})})-H(a))=\sqrt{n}(\overline{g(\mathbf{X})}-a) \cdot G(\overline{g(\mathbf{X})}) \Rightarrow \xi \cdot H^{\prime}(a)
\end{equation*}

Мы использовали следующее свойство слабой сходимости: если $\xi_{n} \Rightarrow \xi$ и $\eta_{n} \xrightarrow[]{\mathrm{p}} c=\mathrm{const}$, то $\xi_{n} \eta_{n} \Rightarrow c \xi$. Но распределение случайной величины $\xi \cdot H^{\prime}(a)$ есть $\mathbf{N}(0,(H^{\prime}(a))^{2} \cdot \mathbb{D} g(X_{1}))$, откуда следует

\begin{equation*}
    \sigma^{2}(\theta)=\left(H^{\prime}(a)\right)^{2} \cdot \mathbb{D} g\left(X_{1}\right)
\end{equation*}

\end{proof}

\section{Метод максимального правдоподобия. Свойства оценок максимального правдоподобия}

\begin{defn}
{\it Оценка максимального правдоподобия $\hat{\theta}$ параметра $\theta$}~--- точка параметрического множества $\Theta$, в которой функция правдоподобия $L(\mathbf{X},\theta)$ при заданном $X$ достигает максимума, т.е.:
\begin{equation*}
    L(\boldsymbol{x}, \hat{\theta})=\sup _{\theta \in \Theta} L(\boldsymbol{x}, \theta)
\end{equation*}
\end{defn}

\begin{rmrk}
Поскольку функция $\operatorname{ln}y$ монотонна, то точки максимума функций $L(\mathbf{X},\theta)$ и $ln L(\mathbf{X},\theta)$ совпадают.
\end{rmrk}

Если для каждого $X$ максимум функции правдоподобия достигается во внутренней точке $\Theta$, и $L(\mathbf{X},\theta)$ дифференцируема по $\theta$, то оценка максимального правдоподобия $\hat{\theta}$ удовлетворяет уравнению:

\begin{equation*}
    \cfrac{\partial \ln L(\mathbf{x}, \theta)}{\partial \theta}=0
\end{equation*}

Если $\theta$~--- векторный параметр: $\theta=\left(\theta_{1}, \ldots, \theta_{n}\right)$, то это уравнение заменяется системой уравнений:

\begin{equation*}
    \cfrac{\partial \ln L(\mathbf{x}, \theta)}{\partial \theta_{i}}=0,~ i=\overline{1, n} 
\end{equation*}


\begin{thm}
Если существует эффективная оценка $T(\mathbf{X})$ скалярного параметра $\theta$, то она совпадает с оценкой максимального правдоподобия.
\end{thm}

\begin{proof}
Если оценка $T(\mathbf{X})$ скалярного параметра $\theta$ эффективна, то в неравенстве Рао-Крамера достигается равенство:

\begin{equation*}
    U(X,\theta) = \cfrac{\partial \ln L(\mathbf{x}, \theta)}{\partial \theta}=\cfrac{T(\mathbf{X})-\theta}{a_n(\theta)}
\end{equation*}

\end{proof}

\begin{thm}
Если $T(\mathbf{X})$ достаточная статистика, а оценка максимального правдоподобия $\hat{\theta}$ существует и единственна, то она является функцией от $T(\mathbf{X})$.
\end{thm}

\begin{proof}
Из критерия факторизации следует, что если $T=T(\mathbf{X})$ достаточная статистика, то имеет место представление:

\begin{equation*}
    L(\mathbf{X}, \theta)=g(T(\mathbf{X}), \theta) h(\mathbf{X})
\end{equation*}

Таким образом, максимизации $L(\mathbf{X},\theta)$ сводится к максимизации $g(T(\mathbf{X}), \theta)$ по $\theta$, Следовательно $\hat{\theta}$ есть функция от $T(\mathbf{X})$.
\end{proof}

\begin{defn}
    {\it Асимптотически эффективная оценка}~---
\end{defn}

\begin{thm}
    Пусть выполнены следующие условия:
    \begin{enumerate}
        \item Функция правдоподобия $L(\mathbf{X}, \theta)$ удовлетворяет условиям регулярности для первых двух производных;
        \item $\exists!~ \theta^{*}$~--- оценка максимального правдоподобия для всех $\theta$, которая достигается во внутренней точке $\Theta$.
    \end{enumerate}
    Тогда оценка $\theta^{*}$:
    \begin{enumerate}
        \item асимптотически несмещёна
        \item состоятельна
        \item асимптотически эффективна
        \item асимптотически нормальна
    \end{enumerate}
\end{thm}

Добавить асимптотическую нормальность и эффективность + Чернова стр 39 теорема для состоятельности.

\section{Интервальное оценивание. Методы центральной статистики и использования точечной оценки}

\begin{defn}
{\it Доверительный интервал} для параметра $\theta$ с коэффициентом доверия $0 \leqslant \alpha \leqslant 1$~--- интервал $(T_1(\mathbf{X}), T_2(\mathbf{X}))$, т.ч. $\mathbb{P}_{\theta}(T_1(\mathbf{X}) < \theta < T_2(\mathbf{X})) \geq \alpha$.
\end{defn}

\begin{exmp}
Пусть $X_1, \ldots, X_n$~--- выборка из $\mathbf{N}(\theta, 1)$. Тогда

\begin{equation*}
    \theta^{*}
    = \overline{X}
    = \frac{1}{n} \sum_{i=1}^{n} X_{i} \sim \mathbf{N}\left(\theta, \frac{1}{n}\right)
    \Rightarrow (\overline{X}-\theta) \sqrt{n} \sim \mathbf{N}(0,1)
\end{equation*}

Для величины, имеющей стандартное нормальное распределение, строим доверительный интервал, т.е. находим такое $t_{\alpha / 2}$, что 

\begin{equation*}
    \mathrm{P}_{\theta}\left(|(\overline{X}-\theta) \sqrt{n}|<t_{\alpha / 2}\right)=\alpha
\end{equation*}

Решаем уравнение относительно $\theta$ и получаем
\begin{equation*}
    \mathrm{P}_{\theta}\left(\overline{X}-\cfrac{t_{\alpha / 2}}{\sqrt{n}}<\theta<\overline{X}+\cfrac{t_{\alpha / 2}}{\sqrt{n}}\right)=\alpha 
\end{equation*}

\end{exmp}

\begin{defn}
{\it Центральная статистика}~--- функция $G(X,\theta)$, т.ч.:
\begin{enumerate}
    \item $G(X,\theta)$ непрерывна и строго монотонна по $\theta$ при любом фиксированном $X$.
    \item $\mathbb{P}_{\theta}(G(X, \theta)<t)=F(t)$ непрерывна и не зависит от $\theta$.
\end{enumerate}
\end{defn}

\begin{rmrk}
Формально определённая выше величина не является статистикой, т.к. зависит от неизвестного параметра $\theta$.
\end{rmrk}

Построение доверительного интервала с помощью центральной статистики:
\begin{enumerate}
    \item Зафиксируем $\alpha_{1}, \alpha_{2} \in \mathbf{R}$, т.ч.
    \begin{equation*}
        \mathbb{P}_{\theta}(\alpha_{1} \leqslant G(X, \theta) \leqslant \alpha_{2})=\alpha~\forall \theta \Leftrightarrow F_{G}(\alpha_{2})-F_{G}(\alpha_{1})=\alpha
    \end{equation*}
    \item Пусть $G(X,\theta)$ возрастает. Из условий
    \begin{equation*}
        \left\{\begin{array}{l}
        G(X, \theta) \leqslant \alpha_{2} \\
        G(X, \theta) \geq \alpha_{1}
        \end{array}\right.
    \end{equation*}
    находятся статистики
    \begin{equation*}
        \left\{\begin{array}{l}
            T_{2}(\mathbf{X}): G(X, T_{2}(\mathbf{X}))=\alpha_{2} \\ 
            T_{1}(\mathbf{X}): G(X, T_{1}(\mathbf{X}))=\alpha_{1}
        \end{array} 
        \Leftrightarrow T_{1}(\mathbf{X}) \leqslant \theta \leqslant T_{2}(\mathbf{X})\right.
    \end{equation*}
    откуда $\mathbb{P}_{\theta}\left(T_{1}(\mathbf{X}) \leqslant \theta \leqslant T_{2}(\mathbf{X})\right) \geq \alpha~ \forall \theta$.
\end{enumerate}

\begin{defn}
{\it Центральный доверительный предел} для параметра $\theta$ с коэффициентом доверия $0 \leqslant \alpha \leqslant 1$~--- интервал $(T_1(\mathbf{X}), T_2(\mathbf{X}))$, т.ч. 
\begin{gather*}
    \mathbb{P}_{\theta}\left(T_{1}(\mathbf{X})>\theta\right)=\cfrac{1-\alpha}{2} \\
    \mathbb{P}_{\theta}\left(T_{2}(\mathbf{X})<\theta\right)=\cfrac{1-\alpha}{2}
\end{gather*}
\end{defn}

Построение доверительного интервала с помощью точечной оценки:

\begin{enumerate}
    \item Пусть $T(\mathbf{X})$~--- точечная оценка $\theta$. Обозначим $H(t, \theta)=\mathbb{P}_{\theta}(T(\mathbf{X})<t)$. $H(t,\theta)$~--- непрерывная и строго монотонная функция $\theta$ при любом фиксированном $t$. В этом случае
    \begin{equation*}
        \left\{\begin{array}{l}
            \mathbb{P}_{\theta}\left(T(\mathbf{X})>a_{1}(\theta)\right)
            = \cfrac{1-\alpha}{2} \\ 
            \mathbb{P}_{\theta}\left(T(\mathbf{X})<\alpha_{2}(\theta)\right)
            = \cfrac{1-\alpha}{2}
        \end{array}\right. 
        \Leftrightarrow 
        \left\{\begin{array}{l}
            1 - H(\alpha_{1}(\theta), \theta)=\cfrac{1-\alpha}{2} \\ 
            H(\alpha_{2}(\theta), \theta)=\cfrac{1-\alpha}{2}
        \end{array}\right.
    \end{equation*}
    
    \item Рассмотрим вспомогательную лемму.
    \begin{lem}
        Если $H(t, \theta)$ возрастает по $\theta$, то $\alpha_{1}(\theta)$ и $\alpha_{2}(\theta)$ убывают. Если же $H(t, \theta)$ убывает по $\theta$, то $\alpha_{1}(\theta)$ и $\alpha_{2}(\theta)$ возрастают.
    \end{lem}
    \begin{proof}
        Пусть $H(t, \theta)$ возрастает. Предположим, что $\theta_{1}<\theta_{2} \Rightarrow \alpha_{2}\left(\theta_{1}\right) \leqslant \alpha_{2}\left(\theta_{2}\right)$ и рассмотрим $a_{2}(\theta)$, учитывая, что $H(t, \theta)$, как и всякая функция распределения, неубывает по первому аргументу:
        \begin{equation*}
            \frac{1-a}{2} 
            = H\left(\alpha_{2}\left(\theta_{1}\right) \theta_{1}\right)
            < H\left(\alpha_{2}\left(\theta_{1}\right) \theta_{2}\right) 
            \leqslant H\left(\alpha_{2}\left(\theta_{2}\right) \theta_{2}\right)
            = \frac{1-\alpha}{2}
        \end{equation*}
        Полученное противоречие завершает доказательство.
    \end{proof}
    \item Из леммы следует, что для любого $\theta$
    \begin{equation*}
    \begin{aligned}
        \alpha_{1}(\theta) 
        < T(\mathbf{X})
        \Leftrightarrow \theta>\varphi_{1}(T(\mathbf{X}))
        \Rightarrow \mathrm{P}_{\theta}(\theta>\varphi_{1}(T(\mathbf{X})))
        = \cfrac{1-\alpha}{2} \\
        \alpha_{2}(\theta)>T(\mathbf{X}) 
        \Leftrightarrow \theta<\varphi_{2}(T(\mathbf{X})) \Rightarrow \mathrm{P}_{\theta}(\theta<\varphi_{2}(T(\mathbf{X})))
        = \cfrac{1-\alpha}{2} \\
        \Rightarrow P_{\theta}(\underbrace{\varphi_{2}(T(\mathbf{X}))}_{T_{1}(\mathbf{X})} 
        \leqslant \theta 
        \leqslant \underbrace{\varphi_{1}(T(\mathbf{X}))}_{T_{2}(\mathbf{X})})
        = \alpha
    \end{aligned}
    \end{equation*}

\end{enumerate}

\section{Проверка гипотез. Лемма Неймана—Пирсона}

\begin{defn}
{\it Гипотеза $H$}~--- любое предположение о распределении наблюдаемой случайной величины: $H=\left\{\mathcal{F}=\mathcal{F}_{1}\right\}$ или $H=\{\mathcal{F} \in \mathbb{F}\}$, где $\mathbb{F}$~--- некоторое подмножество в множестве всех распределений. Гипотеза называется {\it простой} в первом случае, {\it сложной} во втором. Если гипотез всего две, то одну из них принято называть {\it основной}, а другую~--- {\itальтернативой}.
\end{defn}

\begin{rmrk} Типичные задачи проверки гипотез:
\begin{enumerate}
    \item Гипотезы о виде распределения;
    \item Гипотезы о проверке однородности выборки: дано несколько выборок; основная гипотезасостоит в том, что эти выборки извлечены из одного распределения;
    \item Гипотеза независимости: по выборке $(X_1,Y_1), \ldots, (X_n,Y_n)$ из $n$ независимых наблюдений пары случайных величин проверяется гипотеза $H_{1}=\left\{X_{i} \text { и } Y_{i} \text { независимы }\right\}$ при альтернативе $H_{1}=\left\{H_{1} \text { неверна }\right\}$. Обе гипотезы являются сложными;
    \item Гипотеза случайности: в эксперименте наблюдаются $n$ случайныхвеличин $X_{1}, \ldots, X_{n}$ и проверяется сложная гипотеза $H_{1}=\left\{X_{1}, \ldots, X_{n}~ \text{независимы и одинаково распределены}\right\}$
\end{enumerate}
\end{rmrk}

Пусть дана выборка $X_{1}, \ldots, X_{n}$, относительно распределения которой выдвинуты две простые гипотезы $H_{0}$ и $H_1$.
\begin{defn}
{\it Критерий}~--- правило, согласно которому гипотеза $H_0$ принимается или отвергается.
\end{defn}
Выборка $\mathbf{X} = (X_1, \ldots, X_n$) объёма $n$~--- точка в пространстве $\mathbb{R}^{n}$. Выделим множество $S \subset \mathbb{R}^{n}$~--- {\it критическую область} для гипотезы $H_0$. В этом случае критерий можно сформулировать следующим образом:
\begin{compactlist}
    \item $\varphi(x) = 1 \Rightarrow$ отвергаем $H_0$, принимаем $H_1$;
    \item $\varphi(x) = 0 \Rightarrow$ отвергаем $H_1$, принимаем $H_0$;
\end{compactlist}

\begin{defn}
Говорят, что произошла {\it ошибка 1-го рода}, если критерий отверг верную гипотезу $H_0$. Вероятность ошибки 1-го рода (или {\it уровень значимости критерия}): 
\begin{equation*}
    \alpha(S)=P\left\{\mathbf{X} \in S \,|\, H_{0}\right\}=P_{0}\left\{\mathbf{X} \in S\right\}
\end{equation*}
Аналогично вероятность ошибки 2-го рода:
\begin{equation*}
    \beta(S)=P\left\{\mathbf{X} \notin S \,|\, H_{1}\right\}=P_{1}\left\{\mathbf{X} \notin S\right\}
\end{equation*}
\end{defn}

\begin{defn}
{\it Мощность критерия}:
\begin{equation*}
    \gamma(S)=1-\beta(S)=P_{1}\left\{\mathbf{X} \in S\right\}
\end{equation*}
\end{defn}

\begin{center}
\begin{tabular}{|c|c|c|}
\hline \multirow{2}{*} { Истинная гипотеза } & \multicolumn{2}{|c|} { Результат принятия решения } \\
\cline {2-3} & $H_{0}$ отклонена & $H_{0}$ принята \\
\hline$H_{0}$ & $\alpha$ & $1-\alpha$ \\
\hline$H_{1}$ & $1-\beta$ & $\beta$ \\
\hline
\end{tabular}
\end{center}

Если $\gamma(S)<\alpha(S)$, то попасть в $S$ при условии истинности гипотезы $H_1$ труднее, чем при условии истинности гипотезы $H_0$, т.е. $S$~--- критическая область скорее для $H_1$. Следовательно, неравенство должно иметь вид $\gamma(S)>\alpha(S)$.

\begin{defn}
    Критерий называется {\it несмещённым}, если выполняется условие
    \begin{equation*}
        \alpha(S) \leqslant \gamma(S)=1-\beta(S)
    \end{equation*}
\end{defn}

Зададим $\alpha_0$ и и будем иметь дело только с такими критериями, где $\alpha_{0} \geqslant \alpha(S)$ (т.е. вероятность ошибки первого рода не превосходитвеличины $\alpha_0$) и дополнительно будем решать задачу $\beta(S) \rightarrow \min\limits_{S}$.

Получаем две эквивалентные задачи определения критической области $S$:
\begin{equation*}
    \begin{array}{l}
    \left\{\begin{array}{l}
    \alpha_{0} \geqslant \alpha(S) \\
    \beta(S) \rightarrow \min\limits_{S}
    \end{array}\right.
    \Leftrightarrow~
    \left\{\begin{array}{l}
    \alpha_{0} \geqslant \alpha(S) \\
    \gamma(S) \rightarrow \max\limits_{S}
    \end{array}\right.
    \end{array}
\end{equation*}

Задачи в такой постановке не всегда решаемы, так как требуетсяответить точно <<да>> или <<нет>>. Такие статистические критерииназываются {\it нерандомизированными критериями}.

\begin{exmp}
Рассмотрим {\it критическую функцию} $\varphi(x)=I\{x \in S\}$. Тогда критерий примет вид:
\begin{compactlist}
    \item Если $\varphi\left(\mathbf{X}\right)=1$, тогда отвергаем гипотезу $H_0$, принимаем $H_1$.
    \item Если $\varphi\left(\mathbf{X}\right)=0$, тогда отвергаем гипотезу $H_1$, принимаем $H_0$.
\end{compactlist}
\end{exmp}

\begin{exmp}
Рассмотрим другую критическую функцию $\varphi(x)=P\left\{\bar{H}_{0} / \mathbf{X}=x\right\}$. В этом случае $\varphi\left(\mathbf{X}\right) \in[0,1]$~--- условная вероятность отклонения гипотезы $H_0$. При таком определении $\varphi(x)$ приходим к {\itрандомизированному критерию}, то есть, критерию, который при некоторых значениях $s$ может не давать ответа <<да>> или <<нет>> в отношении истинности гипотезы $H_0$. Тогда формулировка критерия следующая:
\begin{compactlist}
    \item с вероятностью $1 - \varphi\left(\mathbf{X}\right)$ следует принимать гипотезу $H_0$;
    \item с вероятностью $\varphi\left(\mathbf{X}\right)$ следует принимать гипотезу $H_0$ю
\end{compactlist}
\end{exmp}

\begin{rmrk}
При использовании введенного обозначения вероятность ошибки первого рода, вероятность ошибки второго рода и мощность критерия будем обозначать: $\alpha(\varphi)$, $\beta(\varphi)$ и $\gamma(\varphi)=1-\beta(\varphi)$ соответственно.
\end{rmrk}

Без ограничения общности будем предполагать, что существует плотность $f_{0}(x)$ для функции распределения $F_{0}(x)$, и существует плотность $f_{1}(x)$ для функции распределения $F_{1}(x)$. В дискретном случае все результаты аналогичны.

Если верна гипотеза $H_1$, то функция правдоподобия выборки $X$ имеет вид:
\begin{equation*}
    L_{1}\left(\mathbf{X}\right)=\prod_{i=1}^{n} f_{1}\left(X_{i}\right)
\end{equation*}

Для рандомизированного критерия получаем
\begin{gather*}
    P_{0}\left(\bar{H}_{0}\right)=\int\limits_{\mathbb{R}^{n}} \varphi(x) L_{0}(x) \mu^{n}(d x)=\alpha(\varphi) \\
    P_{1}\left(H_{0}\right)=\int\limits_{\mathbb{R}^{n}}(1-\varphi(x)) L_{1}(x) \mu^{n}(d x)=\beta(\varphi) \\
    \gamma(\varphi)=\int\limits_{\mathbb{R}^{n}} \varphi(x) L_{1}(x) \mu^{n}(d x), \quad \gamma(\varphi)=1-\beta(\varphi)
\end{gather*}

Тогда задача построения статистического критерия сводится к нахождению критической функции $\varphi(x)$ и будет формулироваться следующим образом:
\begin{equation*}
    \begin{array}{l}
    \left\{\begin{array}{l}
    \alpha_{0} \geqslant \alpha(\varphi) \\
    \beta(\varphi) \rightarrow \min\limits_{\varphi}
    \end{array}\right.
    \Leftrightarrow
    \left\{\begin{array}{l}
    \alpha_{0} \geqslant \alpha(\varphi) \\
    \gamma(\varphi) \rightarrow \max\limits_{\varphi}
    \end{array}\right.
    \end{array}
\end{equation*}
Таким образом, задача заключается в том, чтобы найти наиболее мощный критерий, когда вероятность ошибки первого рода не превосходит некоторого заданного порогового значения. Решение сформулированных задач даётся леммой Неймана-Пирсона.

\begin{thm}[Лемма Неймана---Пирсона]
Пусть $\alpha_{0} \in(0,1)$, тогда при фиксированной вероятности ошибки первого рода $\alpha_{0}$ наиболее мощный критерий имеет критическую функцию $\varphi^{*}$ вида
\begin{equation*}
    \varphi^{*}(x)=\left\{\begin{array}{ll}
    1, & \text { если } L_{1}(x)>c L_{0}(x) \\
    \varepsilon, & \text { если } L_{1}(x)=c L_{0}(x) \\
    0, & \text { если } L_{1}(x)<c L_{0}(x)
    \end{array}\right.
\end{equation*}
где $L_{j}(x)=\prod_{i=1}^{n} f_{j}\left(x_{i}\right)$ соответствует гипотезе $H_j, j = \overline{1,2}$, константы $c$ и $\varepsilon$ являются решениями уравнения $\alpha\left(\varphi^{*}\right)=\alpha_{0}$.
\end{thm}

\begin{proof}
\begin{enumerate}
    \item Покажем, что константы $c$ и $\varepsilon$ могут быть найдены из уравнения $\alpha\left(\varphi^{*}\right)=\alpha_{0}$. Заметим, что
    
    \begin{equation*}
        \begin{aligned} \alpha(\varphi^{*})
        = P_{0}(L_{1}(\mathbf{X}) > c L_{0}(\mathbf{X})) 
        + \varepsilon P_{0}(L_{1}(\mathbf{X}) = c L_{0}(\mathbf{X}))=\\ 
        = P_{0}\left(\frac{L_{1}(\mathbf{X})}{L_{0}(\mathbf{X})} > c\right) 
        + \varepsilon P_{0}\left(\frac{L_{1}(\mathbf{X})}{L_{0}(\mathbf{X})} = c \right) 
        \end{aligned}
    \end{equation*}

Если предположить, что $L_{0}(\mathbf{X})=0$, то

\begin{equation*}
    P_{0}\left\{L_{0}(\mathbf{X}) = 0\right\} = \int\limits_{\left\{x: L_{0}(x)=0\right\}} L_{0}(x) \mu(d x)=0
\end{equation*}

и, следовательно, вышеприведённое равенство корректно. Поэтому рассмотрим случайную величину $\eta(\mathbf{X}) = \cfrac{L_{1}(\mathbf{X})}{L_{0}(\mathbf{X})}$

Положим $F_{H_{0}, \eta}(t)=P\{\eta \leqslant t\}$, тогда
\begin{equation*}
    \alpha\left(\varphi^{*}\right)=1-F_{H_{0}, \eta}(c)+\varepsilon\left(F_{H_{0}, \eta}(c)-F_{H_{0}, \eta}(c-0)\right)
\end{equation*}

Пусть $g(c)=1-F_{H_{0}, \eta}(c)$, константу $c_{\alpha_{0}}$ можно выбрать так, чтобы было выполнено неравенство:
\begin{equation*}
    g(c_{\alpha_{0}}) \leqslant \alpha_{0} \leqslant g(c_{\alpha_{0}}-0)
\end{equation*}

Тогда
\begin{equation*}
    \varepsilon_{\alpha_{0}} = 
    \left\{\begin{array}{ll}
         0, & \text{ если }  g\left(c_{\alpha_{0}}\right)=g\left(c_{\alpha_{0}}-0\right) \\
         \cfrac{\alpha_{0}-g\left(c_{\alpha_{0}}\right)}{g\left(c_{\alpha_{0}}-0\right)-g\left(c_{\alpha_{0}}\right)} \in [0,1], & \text{ если } g\left(c_{\alpha_{0}}\right)<g\left(c_{\alpha_{0}}-0\right)
    \end{array}\right.
\end{equation*}

В обоих случаях выполнено равенство:
\begin{equation*}
    \alpha_{0}=g\left(c_{\alpha_{0}}\right)+\varepsilon_{\alpha_{0}}\left(g\left(c_{\alpha_{0}}-0\right)-g\left(c_{\alpha_{0}}\right)\right)=\alpha\left(\varphi^{*}\right)
\end{equation*}

\item Докажем, что $\varphi^{*}(x)$~--- критическая функция наиболее мощного критерия.

Выберем любую другую критическую функцию $\tilde{\varphi}(x)$ такую, что $\alpha(\tilde{\varphi}) \leqslant \alpha_{0}$, и сравним ее с критической функцией $\varphi^{*}(x)$. Заметим, что для любого $x$ справедливо неравенство:
\begin{equation*}
    \left(\varphi^{*}(x)-\tilde{\varphi}(x)\right)\left(L_{1}(x)-c_{\alpha_{0}} L_{0}(x)\right) \geqslant 0
\end{equation*}

Тогда
\begin{equation*}
    \int\limits_{\mathbb{R}^{n}}\left(\varphi^{*}(x)-\tilde{\varphi}(x)\right)\left(L_{1}(x)-c_{\alpha_{0}} L_{0}(x)\right) \mu^{n}(d x) \geqslant 0
\end{equation*}

Раскроем скобки и преобразуем:

\begin{equation*}
    \begin{array}{l}
\int\limits_{\mathbb{R}^{n}} \varphi^{*}(x) L_{1}(x) \mu^{n}(d x)-\int\limits_{\mathbb{R}^{n}} \tilde{\varphi}(x) L_{1}(x) \mu^{n}(d x) \geqslant \\
\quad \geqslant c_{\alpha_{0}}\left(\int\limits_{\mathbb{R}^{n}} \varphi^{*}(x) L_{0}(x) \mu^{n}(d x)-\int\limits_{\mathbb{R}^{n}} \tilde{\varphi}(x) L_{0}(x) \mu^{n}(d x)\right) \geqslant 0
\end{array}
\end{equation*}

Следовательно, $\gamma\left(\varphi^{*}\right)-\gamma(\tilde{\varphi}) \geqslant c_{\alpha_{0}}\left(\alpha\left(\varphi^{*}\right)-\alpha(\tilde{\varphi})\right)$, откуда получаем неравенство:

\begin{equation*}
    \gamma\left(\varphi^{*}\right) \geqslant \gamma(\tilde{\varphi})
\end{equation*}
\end{enumerate}
\end{proof}

\section{Критерии согласия Колмогорова и $\chi^{2}$}

{\bf Критерий Колмогорова}

Выборка $X_1, \ldots, X_n$ имеет функцию распределения $F(x)$ из семейства распределений . Требуется проверить гипотезу $F(x)=F_{0}(x)$. Непараметрический критерий Колмогорова основан на статистике
\begin{equation*}
    D_{n}(\mathbf{X})=\sup _{X} | F_{n}(x)-F_{0}(x)
\end{equation*}
где $F_{0}(x)$  —  непрерывная функция распределения, а $F_{n}(x)$~---  эмпирическая функция распределения, построенная повыборке $X_1, \ldots, X_n$.

Из того, что если $\xi$~--- случайная величина, $F_{\xi}(x)$~--- непрерывна, то случайная величина $\eta=F_{\xi}(\xi)$ равномерно распределенана $[0,1]$, следует что при $F_{0}(x)=t$ вероятность $\mathbb{P}\left(D_{n}(\mathbf{X})<t\right)$ независит от $\theta$ и $F_{0}(x)$.

\begin{thm}
    Для любой непрерывной $F(x)$ при $x > 0$ выполняется
    \begin{equation*}
        \lim _{n \to \infty} P\left(\sqrt{n} D_{n}(\mathbf{X})<t\right)=K(t)=\sum_{j=-\infty}^{+\infty}(-1)^{-2 j^{2}+2}
    \end{equation*}
\end{thm}

На основе этого предельного соотношения строится непараметрический критерий Колмогорова. Пусть $\gamma_{\alpha}$~--- $\alpha$-квантиль предельного распределения $K(t)$:
\begin{equation*}
    1-K\left(\gamma_{\alpha}\right)=\alpha \Leftrightarrow \mathrm{P}\left(\sqrt{n} D_{n}(\mathbf{X}) \geq \gamma_{a} | H_{0}=\alpha\right)
\end{equation*}

Тогда гипотеза о том, что выборка взята из распределения с функцией $F_{0}(x)$ принимается, если $\sqrt{n} D_{n}(\mathbf{X}) \leqslant \gamma_{a}$. Уровень значимости этого критерия равен приближённо $\alpha$.

{\bf Критерий $\chi^{2}$}
Пусть имеетсявыборка $X_1, \ldots, X_n$ и требуется проверить гипотезу $H_{0}: F(x)=F_{0}(x)$. Разобьём числовую прямую на $m$ промежутков $\Delta_{1}, \Delta_{2}, \ldots, \Delta_{m-1}, \Delta_{m}$. Обозначим $V_{k}$ — число наблюдений, попавших в интервал $\Delta_{k}$. Тогда если $\xi_{i}^{(k)}=\mathrm{I}\left(X_{i} \in \Delta_{k}\right),$ тo $v_{k}=\sum_{i=1}^{n} \xi_{i}^{(k)}$.

При этом имеет место сходимость
\begin{equation*}
    \frac{v_k}{n} \xrightarrow[n \to \infty]{} P\left(X_{1} \in \Delta_{k}\right)=\int\limits_{\Delta_{k}} d F_{0}(x)=p_{k}
\end{equation*}

Строится статистика

\section{Статистические выводы о параметрах нормального распределения. Распределения $\chi^{2}$ и Стьюдента. Теорема Фишера}

\begin{defn}
    Пусть $\zeta_{1}, \ldots, \zeta_{k}$ взаимно независимые случайные величины, $\zeta_{k} \sim \mathbf{N}(0,1)$. Распределение случайной величины $\tau_{k}=\zeta_{1}^{2}+\ldots+\zeta_{k}^{2}$ называется распределением $\chi^{2}$ с $k$ степенями свободы.
\end{defn}
\begin{rmrk}
    Распределение $\chi^{2}$ с $k$ степенями свободы представляет собой гамма-распределение с параметрами формы $\frac{k}{2}$ и масштаба $\frac{1}{2}$:
    \begin{equation*}
    f_{\tau}(x)=\left\{\begin{array}{ll}
        \left(\frac{1}{2}\right)^{\frac{k}{2}} \cfrac{x^{\frac{k}{2}-1}}{\Gamma\left(\frac{k}{2}\right)} e^{-\frac{x}{2}}, & x>0 \\
        0, & x \leqslant 0
    \end{array}\right.
    \end{equation*}
\end{rmrk}

\begin{defn}
    Пусть заданы случайные величины $\zeta \sim \mathbf{N}(0,1)$ и $\tau_{k} \sim \chi_{k}^{2}$. Пусть случайные величины $\zeta$ и $\tau_{k}$ взаимно независимы. Распределение случайной величины 
    \begin{equation*}
        \xi=\frac{\zeta}{\sqrt{\frac{\tau_{k}}{k}}}
    \end{equation*}
    называется {\it распределением Стьюдента} с $k$ степенями свободы и обозначается через $T_{k}$.
\end{defn}

\begin{thebibliography}{1}
	\bibitem{cher_terover}
		Н.И.Чернова.
		{\it Теория вероятностей},
		2007
	\bibitem{char_stat}
		Н.И.Чернова.
		{\it Математическая статистика},
		2014
\end{thebibliography}
\end{document}
