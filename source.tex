\documentclass[oneside,final,14pt]{extreport}

\usepackage{cmap}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{relsize}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\usepackage{hhline}
\usepackage{multirow}

\usepackage{etoolbox}
    \makeatletter
    \patchcmd{\@makechapterhead}{\vspace*{50\p@}}{}{}{}% Removes space above \chapter head
    \patchcmd{\@makeschapterhead}{\vspace*{50\p@}}{}{}{}% Removes space above \chapter* head
    \makeatother

\newcommand\mydef{{\bf Опр.}}
\newcommand\mynote{{\bf Замеч.}}
\newcommand\myst{{\bf Утв.}}
\newcommand\mycon{{\bf Следствие.}}
\newcommand\myth{{\bf Теорема.}}
\newcommand\myqed{{\bf Док-во.}}
\newcommand\myex{{\bf Пример.}}
\newcommand\myprob[1]{{\mathbf{P}(#1)}}
\newcommand\mydes{{\bf Обозн.}}

\renewcommand{\qedsymbol}{$\blacksquare$}
\renewenvironment{proof}{{\bfseries Доказательство.}}{\qed}

\newtheorem{thm}{Теорема}[section]
\newtheorem{lem}[thm]{Лемма}
\newtheorem*{rmrk}{Замечание}
\theoremstyle{definition}
\newtheorem{defn}{Определение}[section]
\newtheorem*{exmp}{Пример}

\newenvironment{compactlist}{
\begin{list}{{$\bullet$}}{
\setlength\partopsep{0pt}
\setlength\parskip{0pt}
\setlength\parsep{0pt}
\setlength\topsep{0pt}
\setlength\itemsep{0pt}
}
}{
\end{list}
}

\setpapersize{A4}
\setmarginsrb{2cm}{1.5cm}{2cm}{1.5cm}{0pt}{0mm}{0pt}{13mm}
\linespread{1.05}

\usepackage{indentfirst}
\sloppy

\usepackage{graphicx} 

\begin{document}
\begin{titlepage}
    \centering
    \vfill
    {\scshape\large
        Московский государственный университет\\
        Факультет вычислительной математики и кибернетики\\
    }
    \vskip1cm
    {\scshape\huge
        Теория вероятностей.\\
        Математическая статистика\\
    }
    \vskip0.5cm
    {\upshape\large
        Рожков И., Рыгин А.
    }    
    \vfill
    \includegraphics[width=8cm]{pic.png}
    \vfill
    {\upshape\large
        Москва\\
        ~2020
    }
\end{titlepage}

\tableofcontents
\chapter{Теория вероятностей}

\section{Вероятностное пространство. Операции над событиями. Свойства вероятности}
\mydef{} {\it Пространство элементарных исходов}~--- любое непустое множество \( \Omega \ne \varnothing \). Элементы \( \omega \in \Omega \)~--- {\it элементарные исходы}.

\mydef{} {\it Алгебра} \( \mathcal{A} \)~--- множество подмножеств \( \Omega \), обладающее следующими свойствами:

\begin{enumerate}
    \item \( \Omega \in \mathcal{A} \)
    \item \( A \in \mathcal{A} \Rightarrow \overline{A} \in \mathcal{A} \)
    \item \( A, B \in \mathcal{A} \Rightarrow A \cup B \in \mathcal{A} \) (по индукции: \( A_1, A_2, ..., A_n \in \mathcal{A} \Rightarrow \bigcup\limits_{i=1}^n A_i \in \mathcal{A} \))
\end{enumerate}

\mydef{} \( \sigma \text{\it{-алгебра~}} \mathcal{F} \)~--- множество подмножеств \( \Omega \), обладающее следующими свойствами:

\begin{enumerate}
    \item \( \Omega \in \mathcal{F} \)
    \item \( A \in \mathcal{F} \Rightarrow \overline{A} \in \mathcal{F} \)
    \item \( A_1, A_2,..., A_n,... \in \mathcal{F} \Rightarrow \bigcup\limits_{i=1}^\infty A_i \in \mathcal{F} \)
\end{enumerate}

\mynote{} Если \( A, B \in \mathcal{A} \), то \( A \cap B \equiv \overline{\overline{A} \cup \overline{B}} \in \mathcal{A} \)

\mynote{} Любая \( \sigma \text{-алгебра} \) является алгеброй. Первые два пункта определений идентичны, рассмотрим третий. Для любой конечной последовательности \( A_1, A_2,..., A_n \in \mathcal{A}\) составим соответствующую счётную последовательность \( A_1, A_2, ..., A_n, A_{n+1}=\varnothing, A_{n+2}=\varnothing,... \in \mathcal{A} \). По определению \( \sigma \text{-алгебры} \): \( \bigcup\limits_{i=1}^\infty A_i \in \mathcal{F} \Rightarrow \bigcup\limits_{i=1}^n A_i \in \mathcal{F} \), следовательно, выполнен третий пункт определения алгебры.

\mydef{} {\it Случайное событие} \(A\)~--- элемент \( \sigma \text{-алгебры~} \mathcal{F} \). \(A=\varnothing\)~---{\it достоверное событие}, \(A=\Omega\)~--- {\it невозможное событие}. Событие \( \overline{A} \)~--- {\it противоположное} \(A\), т.е. происходит тогда и только тогда, когда не происходит \(A\).

Операции над событиями:

\begin{compactlist}
    \item {\it Объединение} \(A \cup B \)~--- происходит тогда и только тогда, когда происходят или \(A\), или \(B\), или оба вместе.
    \item {\it Пересечение} \(A \cap B \) (или \(AB \))~--- происходит тогда и только тогда, когда происходят и \(A\) и \(B\) вместе.
    \item {\it Разность} \(A \setminus B \)~--- происходит тогда и только тогда, когда происходит \(A\) и не происходит \(B\).
    \item {\it Симметрическая разность} \(A \triangle B \) ~--- происходит тогда и только тогда, когда либо происходит \(A\) и не происходит \(B\), либо происходит \(B\) и не происходит \(A\).
\end{compactlist}

\mydef{} \( \sigma \text{-алгебра} \) {\it порождена классом \(K\)}, если она является пересечением всех \( \sigma \text{-алгебр}\), содержащих \(K\), т.е. является {\it минимальной \( \sigma \text{-алгеброй} \)}, содержащей \(K\).

\myex{} Пусть \( K = \{A\} \), тогда \( \sigma (K) = \{\varnothing, \Omega, A, \overline{A}\} \).

\mydef{} {\it \( \text{Борелевская~} \sigma \text{-алгебра~} \mathcal{B} \) }~--- \( \sigma \text{-алгебра}\), порождённая множеством всех открытых интервалов. Элемент \(B \in \mathcal{B}\)~--- {\it борелевское множество}.

\mydef{} \( (\Omega, \mathcal{A}) \) и \( (\Omega, \mathcal{F}) \)~--- {\it измеримые пространства}, элементы \(\mathcal{A}\) и \(\mathcal{F}\)~---{\it измеримые множества}.

\mydef{} {\it Вероятностная мера}~--- функция \( \mathbb{P}: \mathcal{F} \rightarrow \mathbb{R} \), обладающая следующими свойствами:

\begin{enumerate}
    \item \( \myprob{A} \geqslant 0~\forall A \in \mathcal{F} \) ({\it неотрицательность})
    \item \( \myprob{\Omega} = 1 \) ({\it нормировка})
    \item \( \forall A_1, A_2, ..., A_n... \in \mathcal{F},~ A_{i}A_{j} = \varnothing~ \forall i, j \in \mathbb{N}, i \ne j \Rightarrow \myprob{\bigcup\limits_{i=1}^\infty A_i} = \sum\limits_{i=1}^\infty \myprob{A_i} \) ({\it счётная аддитивность})
\end{enumerate}

\myst{} Свойства вероятности:
\begin{enumerate}
    \item \( \myprob{\varnothing}=0 \)
    \item \( A, B \in \mathcal{F}, B \subset A \Rightarrow \myprob{A} \geqslant \myprob{B} \)
    \item \( \myprob{A \setminus B} = \myprob{A} - \myprob{AB} \)
    \item \( \myprob{A \cup B} = \myprob{A} + \myprob{B} - \myprob{AB} \)
\end{enumerate}

\myqed{}
\begin{enumerate}
    \item Рассмотрим последовательность событий \( \Omega, \varnothing, \varnothing, ...\): \( \bigcup\limits_{i=1}^\infty A_i = \Omega \Rightarrow \myprob{\bigcup\limits_{i=1}^\infty A_i} = \myprob{\Omega} = 1 \). При этом \( A_{i}A_j = \varnothing~(i \ne j) \), следовательно, по п.3 определения вероятности: \( \sum\limits_{i=2}^\infty \myprob{\varnothing} = 0 \Rightarrow \myprob{\varnothing} = 0 \).
    \item \( B \subset A \Rightarrow A = (A \setminus B) \cup B \). Из неотрицательности вероятности и того, что \( (A \setminus B) \cap B = \varnothing \), следует, что \( \myprob{A} = \myprob{A \setminus B} + \myprob{B} \geqslant \myprob{B} \). Кроме того, \( \myprob{A \setminus B} = \myprob{A} \ \myprob{B})\).
    \item Следует из того, что \( A = (A \ B) \cup AB, (A \ B) \cap AB = \varnothing \).
    \item Следует из того, что \( A \cup B = (A \setminus AB) \cup B, (A \setminus AB) \cap B = \varnothing \).
\end{enumerate}

\mydef{} Тройка \( (\Omega, \mathcal{F}, \mathbb{P}) \)~--- {\it вероятностное пространство}.

\mydef{} Пусть \( \Omega = \{ w_1, w_2,..., w_n\} \)~--- конечное непустое множество, \( \mathcal{F} \)~--- множество всех подмножеств \( \Omega \). Положим \( \myprob{\{w_i\}} = p_i \). Вероятностное пространство, определённое таким образом,~--- {\it дискретное вероятностное пространство}. Тогда для любого события \( A = \{ w_1,...w_k\} \) его вероятность \( \myprob{A} = \sum\limits_{i=1}^k p_i \).

\mydef{} {\it Классическое определение вероятности}~--- \(p_1 = p_2 =...=p_n=\frac{1}{n} \). В этом случае \( \myprob{A} = \frac{k}{n} \).

\section {Условная вероятность. Независимость событий. Критерий независимости. Формула полной вероятности. Формула Байеса.}

\mydef{} Пусть задано вероятностное пространство \( (\Omega, \mathcal{F}, \mathbb{P}), A, B \in \mathcal{F}, \myprob{B} > 0 \). {\it Условная вероятность события \( A \) при событии \(B\)}~--- число \( \myprob{A|B}=\frac{\myprob{AB}}{\myprob{B}} \).

\myst{} Условная вероятность \( \myprob{A|B} \)~--- вероятность, заданная на \( \mathcal{F} \).

\myqed{} Проверим три аксиомы из определения вероятности.

\begin{enumerate}
    \item  \( \myprob{\Omega|B} = \frac{\myprob{B \cap \Omega}}{\myprob{B}} = \frac{\myprob{B}}{\myprob{B}} = 1\)
    \item \( \forall A \in \mathcal{F}~ \myprob{A|B} \geq 0, \text{т.к.}~ \myprob{AB} \geqslant 0,~ \myprob{B} > 0 \)
    \item Пусть дана некоторая последовательность $\it A_1, A_2, ..., A_n, ..., A_i \cap A_j = \varnothing ({\it i \ne j})$. Тогда: 
    
    $$\myprob{(\bigcup\limits_{i=1}^\infty A_i)|B}  = \dfrac{\myprob{(\bigcup\limits_{i=1}^\infty A_i) \cap B}}{\myprob{B}} = \dfrac{\myprob{\bigcup\limits_{i=1}^\infty(A_i \cap B)}}{\myprob{B}} = \dfrac{\sum\limits_{i=1}^\infty \myprob{A_i \cap B}}{\myprob{B}} = $$$$ = \sum\limits_{i=1}^\infty \myprob{A_i | B} ~~~ \square. $$
    
\end{enumerate}

Некоторые свойства условной вероятности:
\begin{enumerate}
    \item Если $A \cap B = \varnothing,$ то $\myprob {A | B} = 0.$
    \item Если $A \subset B$, то $\myprob{A|B} = 1.$ Например, $\myprob{B|B} = 1.$
\end{enumerate}

\newpage

\subsubsection{Независимость событий}
\mydef{} Пусть есть вероятностное пространство $(\Omega, \mathcal{F}, \mathbb{P})$. События $A_1, ..., A_n \in \mathcal{F}$ называются независимыми, если $\forall~2 \leq k \leq n$ и $\forall~1 \leq i_1 \leq i_2 \leq ... \leq i_k \leq n$ выполняется 
$$ \myprob {\bigcap\limits_{j=1}^k A_{i_j}} = \prod\limits_{j=1}^k \myprob{A_{i_j}}. $$

В частности, при $n = 2$: события $A_1$ и $A_2$ независимы, если $\myprob{A_1 \cap A_2} = \myprob{A_1}\myprob{A_2}$.

Некоторые свойства:

\begin{enumerate} 
    \item Если $A = \varnothing$, то для любого $B$ с ненулевой вероятностью, $A$ и $B$ независимы. Действительно, $AB = \varnothing \rightarrow 0 = \myprob{AB} = \myprob{A}\myprob{B} = 0 * \myprob{B} = 0$. Аналогично для $\myprob{A} = 0$.
    \item Если $\myprob{A} = 1$, то $A$ и $B$ независимы для любого $B$ с ненулевой вероятностью. Д-во аналогично.
    \item Пусть $A$ и $B$ независимы. Тогда события $\overline{A}$ и $B$, $A$ и $\overline{B}$, $\overline{A}$ и $\overline{B}$ также независимы. Докажем независимость $\Bar{A}$ и $B$. Для события $B$ справедливо представление $B = AB \cup \overline{A}B.$ Тогда $\myprob{B} = \myprob{AB} + \myprob{\overline{A}B}$, но $\myprob{AB} = \myprob{A}\myprob{B},$ следовательно, $\myprob{\overline{A}B} = \myprob{B} - \myprob{A}\myprob{B} = \myprob{B} (1 - \myprob{A}) = \myprob{\overline{A}}\myprob{B}$, и независимость $\overline{A}$ и $B$ доказана. Аналогично доказываются остальные утверждения.
    \item Пусть $A \subset B$ и $\myprob{A} > 0, \myprob{B} < 1$. Тогда $A$ и $B$ зависимы. Действительно, предположим, что они независимсы. Тогда $\myprob{AB} = \myprob{A}\myprob{B},$ но $\myprob{AB} = \myprob{A}$, следовательно, $\myprob{B} = 1$, что противоречит условию.
    \item Если события $A$ и $B$ независимы и $\myprob{B} > 0$, то условная вероятность $A$ при условии $B$ равна вероятности $A$. Действительно, $\myprob{A | B} = \dfrac{\myprob{AB}}{\myprob{B}} = \dfrac{\myprob{A}\myprob{B}}{\myprob{B}} = \myprob{A}$.
\end{enumerate}
\myex{}, когда из попарной независимости не следует независимость в совокупности.
Рассмотрим вероятностное пространство, в котором всего 4 различных элементарных исхода: $\Omega = \{ \omega_1, \omega_2, \omega_3, \omega_4 \}$. Пусть $\mathcal{F}$ --- множество всех подмножеств $\Omega, \myprob{\{\omega_i\}} = \dfrac{1}{4}, i = \overline{1,4}.$ Рассмотрим три события $A_1 = \{ \omega_1, \omega_4 \}, A_2 = \{ \omega_2, \omega_4 \}, A_3 = \{ \omega_3, \omega_4 \}$. $A_1A_2 = A_2A_3 = A_3A_1 = \{ \omega_4 \}, A_1A_2A_3 = \{ \omega_4 \}$.
$$ \myprob{A_1} = \myprob{A_2} = \dfrac{1}{2}, \myprob{A_1A_2} = \myprob{A_2A_3} = \myprob{A_3A_1} = \dfrac{1}{4}, $$$$ \myprob{A_1A_2A_3} = \dfrac{1}{4} \neq \dfrac{1}{8} = \myprob{A_1}\myprob{A_2}\myprob{A_3},$$ следовательно события $A_1, A_2$ и $A_3$ не являются независимыми. 

\subsubsection{Критерий независимости}

\mydes{} $A_{i}^{(\delta)}=\left\{\begin{array}{ll}A_{i}, & \delta=1 \\ \overline{A_{i}}, & \delta=0\end{array}\right.$

\myth{} {\bf (критерий независимости) }
События $A_1, ..., A_n$ независимы тогад и только тогда, когда $\forall ~ \delta_1, \delta_2, ... \delta_n$ (равных нулю или единице) $$\myprob{\bigcap_{i=1}^{n} A_{i}^{\left(\delta_{j}\right)}}=\prod_{i=1}^{n}\myprob{A_{i}^{\left(\delta_{i}\right)}}$$
\myqed{} Покажем, что если $A_1, ..., A_n$ независимы тогда и только тогда, когда $\forall ~ 2 \leq k \leq n, \forall ~ 1 \leq i_1 \leq ... \leq i_k \leq n, \forall ~ \delta_{i_1}, ..., \delta_{i_k} (= 0, 1)$ выполняется $\mathbf{P}\left(\bigcap_{j=1}^{k}A_{i_{j}}^{\left(\delta_{i}\right)}\right)=\prod_{j=1}^{k} \mathbf{P}\left(A_{i_{j}}^{\left(\delta_{j}\right)}\right)$. Пусть $A_1, ..., A_n$ независимы. Проведём индукцию по числу $\mu$ событий $A_{i_j}$, для которых $\delta_{i_j} = 0$. Если все $\delta_{i_j} = 1$, то утверждение превращается в определение независимости ($\mu = 0$ очевидно). 
Пусть утверждение справедливо для всех $\mu \leq m$. Докажем, что оно справедливо при $\mu = m + 1$. Покажем, что $$\mathbf{P}\left(\overline{A_{i_{1}}}~ \overline{A_{i_{2}}} \ldots \overline{A_{i_{m+1}}} A_{i_{m+2}} \cdots A_{i_{k}}\right)=\mathbf{P}(\overline{A_{i_{1}}}) \cdots \mathbf{P}(\overline{A_{i_{m+1}}}) \cdot \mathbf{P}\left(A_{i_{m+2}}\right) \cdots \mathbf{P}\left(A_{i_{n}}\right).$$
Воспользуемся свойством аддитивности вероятности. Заметим предварительно, что для события $\overline{A_{i_{1}}}~ \overline{A_{i_{2}}} \ldots \overline{A_{i_{m+1}}} A_{i_{m+2}} \cdots A_{i_{k}}$ допустимо разложение на два непересекающихся события $$\overline{A_{i_{1}}}~\overline{A_{i_{2}}} \cdots \overline{A_{i_{m}}} A_{i_{m+2}} \cdots A_{i_{k}}=\overline{A_{i_{1}}}~\overline{A_{i_{2}}} \cdots \overline{A_{i_{m+1}}} A_{i_{m+2}} \cdots A_{i_{k}} \cup \overline{A_{i_{1}}}~ \overline{A_{i_{2}}} \cdots A_{i_{m+1}} A_{i_{m+2}} \cdots A_{i_{k}}.$$
Тогда
$$\mathbf{P}\left(\overline{A_{i_{1}}}~\overline{A_{i_{2}}} \cdots \overline{A_{i_{m}}} A_{i_{m+2}} \cdots A_{i_{k}}\right)=\mathbf{P}\left(\overline{A_{i_{1}}}~\overline{A_{i_{2}}} \cdots \overline{A_{i_{m+1}}} A_{i_{m+2}} \cdots A_{i_{k}}\right)+
$$$$+\mathbf{P}\left(\overline{A_{i_{1}}}~\overline{A_{i_{2}}} \cdots \overline{A_{i_{m}}} A_{i_{m+1}} A_{i_{m+2}} \cdots A_{i_{k}}\right),$$
следовательно,
$$\mathbf{P}\left(\overline{A_{i_{1}}}~\overline{A_{i_{2}}} \cdots \overline{A_{i_{m+1}}} A_{i_{m+2}} \cdots A_{i_{k}}\right)=\mathbf{P}\left(\overline{A_{i_{1}}}~\overline{A_{i_{2}}} \cdots \overline{A_{i_{m}}} A_{i_{m+2}} \cdots A_{i_{k}}\right)-$$$$-\mathbf{P}\left(\overline{A_{i_{1}}}~\overline{A_{i_{2}}} \cdots \overline{A_{i_{m}}} A_{i_{m+1}} A_{i_{l_{m+2}}} \cdots A_{i_{k}}\right)$$
и
$$\begin{array}{c}
\mathbf{P}\left(\overline{A_{i_{1}}} \cdots \overline{A_{i_{m+1}}} A_{i_{m+2}} \cdots A_{i_{k}}\right)=\mathbf{P}(\overline{A_{i_{1}}}) \cdots \mathbf{P}(\overline{A_{i_{m}}}) \cdot \mathbf{P}\left(A_{i_{m+2}}\right) \cdots \mathbf{P}\left(A_{i_{k}}\right)
\\
\cdot\left[1-\mathbf{P}\left(A_{i_{m+1}}\right)\right]=
\mathbf{P}(\overline{A_{i_{1}}}) \cdots \mathbf{P}\left(A_{i_{m}}\right) \cdot \mathbf{P}(\overline{A_{i_{m+1}}}) \cdot \mathbf{P}\left(A_{i_{m+2}}\right) \cdots \mathbf{P}\left(A_{i_{k}}\right)
\end{array},$$
тем самым утверждение доказано для некоторого $k$, в частности справедливо
$$\mathbf{P}\left(A_{1}^{\delta_{1}} \cdots A_{n}^{\delta_{n}}\right)=\mathbf{P}\left(A_{1}^{\delta_{1}}\right) \cdots \mathbf{P}\left(A_{n}^{\delta_{n}}\right).$$
Докажем теперь, что оно справедливо для любого $k$. Проведем индукцию по $k$. При $k = n$ утверждение, очевидно, справедливо. Предположим, что оно справедливо $\forall~k \geq m + 1.$ Докажем его для $k = m~(m < n)$. Для события $A_{i_1}^{(\delta_1)}\cdots A_{i_m}^{(\delta_m)}$ справедливо представление в виде объединения двух непересекающихся событий
$$A_{i_{1}}^{\left(\delta_{1}\right)} \cdots A_{i_{m}}^{\left(\delta_{m}\right)}=A_{i_{1}}^{\left(\delta_{1}\right)} \cdots A_{i_{m}}^{\left(\delta_{m}\right)} A_{i_{m+1}}^{(1)} \cup A_{i_{1}}^{\left(\delta_{1}\right)} \cdots A_{i_{m}}^{\left(\delta_{m}\right)} A_{i_{m+1}}^{(0)}.$$
Тогда в силу аддитивности вероятности 
$$\mathbf{P}\left(A_{i_{1}}^{\left(\delta_{1}\right)} \cdots A_{i_{m}}^{\left(\delta_{m}\right)}\right)=\mathbf{P}\left(A_{i_{1}}^{\left(\delta_{1}\right)}\right) \cdots \mathbf{P}\left(A_{i_{m}}^{\left(\delta_{m}\right)}\right) \mathbf{P}\left(A_{i_{m+1}}^{(1)}\right)+$$$$+\mathbf{P}\left(A_{i_{1}}^{\left(\delta_{1}\right)}\right) \cdots \mathbf{P}\left(A_{i_{m}}^{\left(\delta_{m}\right)}\right) \mathbf{P}\left(A_{i_{m+1}}^{(0)}\right)
=\mathbf{P}\left(A_{i_{1}}^{\left(\delta_{1}\right)}\right) \cdots \mathbf{P}\left(A_{i_{m}}^{\left(\delta_{m}\right)}\right) ~ \square.$$

\subsubsection{Формула полной вероятности}
\myth{} Пусть даны события $A, B_1, B_2, ..., B_n, ..., \myprob{B_i} > 0, $ причём $B_i \cap B_j = \varnothing~(i \neq j)$ и $\bigcup\limits_{i=1}^{\infty}B_i \supset A~$ (например, $\bigcup\limits_{i=1}^{\infty}B_i = \Omega$). Тогда справедлива {\it формула полной вероятности:}
$$\mathbf{P}(A)=\sum_{i=1}^{\infty} \mathbf{P}\left(B_{i}\right) \cdot \mathbf{P}\left(A | B_{i}\right)$$
\myqed{} Достаточно заметить, что при вышеперечисленных условиях $A = \bigcup\limits_{i=1}^{\infty}(AB_i),$ и $AB_i \cap AB_j = \varnothing ~(i \neq j).$ Тогда, учитывая $\myprob{B_i} > 0$, получаем
$$\mathbf{P}(A)=\sum_{i=1}^{\infty} \mathbf{P}\left(A B_{i}\right)=\sum_{i=1}^{\infty} \mathbf{P}\left(B_{i}\right) \frac{\mathbf{P}\left(A B_{i}\right)}{\mathbf{P}\left(B_{i}\right)}=\sum_{i=1}^{\infty} \mathbf{P}\left(B_{i}\right) \cdot \mathbf{P}\left(A | B_{i}\right) ~~~ \square.$$

\subsubsection{Формулы Байеса}
\myth{} Пусть даны события $A, B_1, B_2, ..., B_n, ..., \myprob{B_i} > 0$, причём $B_i \cap B_j = \varnothing ~(i \neq j)$ и $\bigcup\limits_{i=1}^\infty B_i \supset A$ (например, $\bigcup\limits_{i=1}^{\infty}B_i = \Omega$). Пусть также $\myprob{A} > 0$. Тогда справедливы {\it формулы Байеса} для $i = 1, 2, ...~$:
$$\mathbf{P}\left(B_{i} | A\right)=\frac{\mathbf{P}\left(B_{i}\right) \cdot \mathbf{P}\left(A | B_{i}\right)}{\sum\limits_{j=1}^{\infty} \mathbf{P}\left(B_{j}\right) \cdot \mathbf{P}\left(A | B_{j}\right)}.$$
\myqed{} Согласно формуле полной вероятности в знаменателе дроби стоит вероятность $A$. Тогда $$\frac{\mathbf{P}\left(B_{i}\right) \cdot \mathbf{P}\left(A | B_{i}\right)}{\mathbf{P}(A)}=\frac{\mathbf{P}\left(B_{i}\right) \cdot \mathbf{P}\left(A B_{i}\right)}{\mathbf{P}(A) \cdot \mathbf{P}\left(B_{i}\right)}=\frac{\mathbf{P}\left(A B_{i}\right)}{\mathbf{P}(A)}=\mathbf{P}\left(B_{i} | A\right) ~~~ \square.$$

\section{Случайная величина. Порожденное и индуцированное вероятностные пространства. Функция распределения, ее свойства}
\subsubsection{Случайные величины}
\mydef{} Пусть даны $(\Omega, \mathcal{F})$ - измеримое пространство и $(\mathbb{R}, \mathcal{B})$, где $\mathcal{B}$ - борелевская $\sigma$-алгебра множеств на числовой прямой $\mathbb{R}$. Тогда измеримая функция $\xi: \Omega \to \mathbb{R}$ называется {\it случайной величиной}.
\\
\\
Очевидны следующие утверждения: 
\begin{enumerate}
    \item $\{\omega: \xi(\omega) < 0\} \in \mathcal{F}$
    \item $\{\omega: \xi(\omega) < a\} \in \mathcal{F}$
    \item $\forall~B \in \mathcal{B} \{\omega: \xi(\omega) \in B\} \in \mathcal{F}$
    \item Если $\mathcal{F} = (\varnothing, \Omega)$, то $\xi = C$.
    \item Если $\xi=\left\{\begin{array}{l}C_{1}, \omega \in A \\ C_{2}, \omega \in \bar{A}\end{array}(\omega: \xi(\omega) \in B)=\xi^{-1}(B)=A\right.$, то $C_{1}\in B, C_2 \notin B.$
\end{enumerate}
\subsubsection{Порожденное и индуцированное вероятностные пространства}
\mydes{} $\mathcal{F}_\xi = \{ \xi^{-1}(\omega), B \in \mathcal{B} \}.$

Отметим следующие факты:
\begin{enumerate}
    \item $\mathcal{F}_\xi \subset \mathcal{F}.$
    \item $\mathcal{F}_\xi - \sigma$-алгебра. Действительно, $\xi^{-1}(\overline{B}) = \overline{\xi^{-1}(B)}$ и $\xi^{-1}\left(\bigcup\limits_{i=1}^{\infty}B_i\right) = \bigcup\limits_{i=1}^\infty \xi^{-1}(B_i)$, если $B_i$ попарно не пересекаются.
\end{enumerate}

\mydef{} Вероятностное пространство $(\Omega,\mathcal{F}_\xi,\mathbf{P})$ называется вероятностным пространством, {\it порожденным} случайной величиной $\xi$.

\mydef{} Вероятностное пространство $(\mathbb{R},\mathcal{B},\mathbf{P}_\xi)$ называется вероятностым пространством, {\it индуцированным} случайной величиной $\xi$. При этом для $B \in \mathcal{B}$ --- $\myprob{B}_\xi = \myprob{\xi^{-1}(B)} = \myprob{\xi \in B}$ называется {\it распределением вероятностей} случайной величины $\xi$.

\subsubsection{Функция распределения, её свойства}

\mydef{} {\it Функцией распределения} $F_\xi (x)$ случайной величины $\xi$ называется функция, определённая для любого вещественного $x$ как
$$F_{\xi}(x)=\mathbf{P}_{\xi}((-\infty, x))=\mathbf{P}(\xi<x).$$

\myth{} $F_\xi(x)$ однозначно определяет $\mathbf{P}_\xi(B)$.
\myqed{} Действительно, любое борелевское множество может быть представлено в виде разности числовой оси, одной или двух полупрямых и не более чем счётного объединения отрезков. В силу однозначности определения $\mathbf{P}_\xi([a,b]) = F_\xi(b + 0) - F_\xi(a)$ утверждение теоремы справедливо.

Свойства функции распределения:
\begin{enumerate}
    \item $\forall x~ 0 \leq F_\xi(x) \leq 1.$ (как вероятность)
    \item $F_\xi(x)$ --- монотонно убывает (т.е. для $x_1 < x_2 \Rightarrow F_\xi(x_1) \leq F_\xi(x_2)).$
    
    \myqed{} $x_1 < x_2 \Rightarrow \{ \xi < x_1 \} \subseteq \{ \xi < x_2\}$; из монотонности вероятности $\myprob{\xi < x_1} = F_\xi(x_1) \leq \myprob{\xi < x_2} = F_\xi(x_2) ~~~ \square.$ 

    \item $\lim\limits_{x \rightarrow +\infty} F_\xi(x) = 1, \lim\limits_{x \rightarrow -\infty} F_\xi(x) = 0.$
    
    \myqed{} Пределы существуют в силу монотонности и ограниченности $F_\xi(x)$. Докажем, что $F_\xi(-n) \xrightarrow[n \rightarrow +\infty]{} 0.$ Рассмотрим последовательность вложенных событий $B_n = \{ \xi < -n \}$, 
    $B_{n+1} = \{\xi < -(n+1) \} \subseteq B_n = \{ \xi < -n \} ~ \forall n ~ \geq 1.$
    
    Далее, $$\bigcap\limits_{j = 1}^{\infty}B_j = \{ \omega | \xi(\omega) < x, \forall x \in \mathbb{R} \} \Rightarrow \bigcap\limits_{j = 1}^{\infty}B_j = \varnothing.$$
    
    $F_\xi(-n) = \myprob{B_n} \xrightarrow[n \rightarrow +\infty]{} \myprob{B} = 0$ (в силу непрерывности меры)
    
    Теперь, $F_\xi(n) \xrightarrow[n \rightarrow +\infty]{} 1 \Leftrightarrow 1 - F_\xi(n) = \myprob{\xi \geq n} \xrightarrow[n \rightarrow +\infty]{} 0.$ Докажем это.
    
    Аналогично, пусть $B_n = \{\xi \geq n\}; B_{n+1} = \{ \xi \geq (n+1) \} \subseteq B_n, ...$
    $$ \bigcap\limits_{j = 1}^{\infty}B_j = \varnothing \Rightarrow \xi(w) > x~\forall x \in \mathbb{R}. $$
    Следовательно, $$1 - F_\xi(n) = \myprob{B_n} \xrightarrow[n \rightarrow +\infty]{} \myprob{B} = 0 \Rightarrow F_\xi(n) \xrightarrow[n \rightarrow +\infty]{}1 ~~~ \square. $$
    
    \item $F_\xi(x)$ непрерывна слева. (т.е. $F_\xi(x_0 - 0) = \lim\limits_{x \rightarrow x_0 - 0}F_\xi(x) = F_\xi(x_0)$)
    
    \myqed{} Докажем, что $F_\xi(x_0 - \frac{1}{n}) \xrightarrow[n \rightarrow +\infty]{} F_\xi(x_0).$ Это $~\Leftrightarrow F_\xi(x_0) - F_\xi(x_0 - \frac{1}{n}) = \myprob{\xi < x_0} - \myprob{\xi < x_0 - \frac{1}{n}} = \myprob{x_0 - \frac{1}{n} \leq \xi \leq x_0} \xrightarrow[n \rightarrow +\infty]{} 0$ (в силу непрерывности вероятности) $\square.$
\end{enumerate}

\section{Дискретные, сингулярные и абсолютно непрерывные функции распределения и случайные величины. Плотность распределения. Теорема Лебега о разложении функции распределения}

\mydef{} {\it Точкой роста} функции распределения $F_\xi(x)$ назовём такую точку $x_0$, что 
$$ \forall \varepsilon > 0 \Rightarrow F_\xi(x_0 + \varepsilon) - F_\xi(x_0 - \varepsilon) > 0.$$
\mynote{} Возможен случай, когда точка роста является точкой разрыва:
$$\lim _{\varepsilon \rightarrow 0} F_{\xi}\left(x_{0}+\varepsilon\right)-F_{\xi}\left(x_{0}-\varepsilon\right)=F_{\xi}\left(x_{0}+0\right)-F_{\xi}\left(x_{0}\right)>0 \Longleftrightarrow $$ $$ \Longleftrightarrow \mathbf{P}\left(x_{0}-\varepsilon \leq \xi<x_{0}+\varepsilon\right)=\mathbf{P}\left(\xi=x_{0}\right)>0$$

\mydef{} Функция распределения $F_\xi(x)$ называется {\it дискретной}, если она имеет не более чем счётное число точек роста $(x_1, ..., x_n, ...).$ В этом случае $x_1, x_2, x_3, ...$ - точки разрыва функции распределения, которая принимает последовательность значений $p_i = \myprob{\xi = x_i}.$

\mydef{} Функция распределения $F_\xi(x)$ называется {\it абсолютно непрерывной}, если её можно представить в виде $F_{\xi}(x)=\int\limits_{-\infty}^{x} p_{\xi}(u)du$, где $p_\xi(u) \geq 0$ - {\it плотность распределения} случайной величины.

\mydef{} Функция распределения $F_\xi(x)$ называется {\it сингулярной}, если она непрерывна и множество точек её роста имеет нулевую меру Лебега.

\myth{} \textbf{(о разложении функции распределения) [А. Лебег].} 
Пусть $\xi$ - случайная величина с функцией распределения $F_\xi(x).$ Тогда существуют и единственны три функции $F_{ac}(x), F_s(x), F_d(x)$, соответственно абсолютно непрерывная, сингулярная и дискретная функции распределения, три числа $p_1, p_2, p_3 \geq 0, p_1 + p_2 + p_3 = 1$ такие, что 
$$F_{\xi}(x)=p_{1} F_{ac}(x)+p_{2} F_{s}(x)+p_{3} F_{d}(x).$$
\newpage
\myqed{} Рассмотрим два возможных случая:
\begin{enumerate}
    \item $F_\xi(x)$ имеет хотя бы одну точку разрыва.
    \item $F_\xi(x)$ непрерывна. В этом случае переходим к шагу II, положив $p_3 = 0$.
\end{enumerate}

\textbf{I}. Количество точек разрыва функции $F_\xi(x)$ не более, чем счётное. Это следует из того, что функция монотонна. Можно это доказать так: поскольку функция не убывает от 0 до 1, она может иметь не более двух скачков, больших или равных $\frac{1}{2}$. Затем, она может иметь не более четырёх скачков, больших или равных $\frac{1}{4}$, и так далее можно пересчитать все скачки функции $F_\xi(x).$ Пусть $x_1, x_2, ..., x_n, ... $ - точки разрыва, упорядоченные по возрастанию. Обозначим 
$$\varphi_{i}=F_{\xi}\left(x_{i}+0\right)-F_{\xi}\left(x_{i}\right).$$

Введём функцию
$$\hat{F}_{d}(x)=\left\{\begin{array}{c}
0, x \leq x_{1} \\
\varphi_{1}, x_{1}<x \leq x_{2} \\
\varphi_{1}+\varphi_{2}, x_{2}<x \leq x_{3} \\
\vdots
\end{array}\right..$$

Очевидно, $\hat{F}_d(x)$  не убывает и непрерывна слева, $$\lim\limits_{x \rightarrow -\infty} \hat{F}_d(x) = 0, \lim\limits_{x \rightarrow +\infty} \hat{F}_d(x) = \sum\limits_{i=1}^\infty \varphi_i \leq 1.$$

Возможны два случая:
\begin{enumerate}
    \item $\sum\limits_{i=1}^\infty \varphi_i = 1.$ Положим $p_3 = 1, p_1 = p_2 = 0$ и теорема доказана.
    \item $\sum\limits_{i=1}^\infty \varphi_i = \alpha < 1.$ В таком случае положим $F_{d}(x)=\frac{1}{\alpha} \hat{F}_{d}(x)$. Функция $\hat{F}_{c}(x)=F_{\xi}(x)-\hat{F}_{d}(x)$ будет непрерывной неубывающей функцией, $\lim _{x \rightarrow-\infty} \hat{F}_{c}(x)=0, \lim _{x \rightarrow+\infty} \hat{F}_{c}(x)=1-\alpha$. Тогда функция $$F_{c}(x)=\frac{\hat{F}_{c}(x)}{1-\alpha}$$ будет непрерывной функцией распределения.
\end{enumerate}

Таким образом, получено разложение функции $F_\xi(x)$ на дискретную и непрерывную части: 
$$F_{\xi}(x)=\hat{F}_{c}(x)+\hat{F}_{d}(x)=\alpha F_{d}(x)+(1-\alpha) F_{c}(x).$$

\textbf{II.} Разложим $F_c(x)$ на $F_{ac}(x)$ и $F_s(x).$ $F_c(x)$, как функция распределения порождает меру $v_{c}(dx).$ Рассмотрим кроме этой меру Лебега. Тогда в силу теоремы Лебега о разложении меры, существуют и единственны две меры $v_{ac}$ и $v_s$, такие что $v_{ac}$ абсолютно непрерывна относительно меры Лебега, а $v_s$ сингулярна относительно меры Лебега:
$$ v_c(B) = v_s(B) + v_{ac}(B), \forall B \in \mathcal{B}. $$
Каждая из этих мер порождает функцию распределения меры, и $$F_{c}(x)=v_{s}((-\infty, x))+v_{a c}((-\infty, x)).$$
Обозначим $\hat{F}_{s}(x)=v_{s}((-\infty, x)), \hat{F}_{a c}(x)=v_{a c}((-\infty, x)).~ \hat{F}_{s}(x), \hat{F}_{a c}(x)$ - непрерывные, неубывающие функции, $$\lim\limits_{x \rightarrow-\infty} \hat{F}_{s}(x)=\lim\limits_{x \rightarrow-\infty} \hat{F}_{a c}(x)=0, \lim\limits_{x \rightarrow+\infty} \hat{F}_{s}(x)=v_{s}(\mathbb{R}), \lim\limits_{x \rightarrow+\infty} \hat{F}_{a c}(x)=v_{a c}(\mathbb{R}).$$
Однако, $$1=v_{c}(\mathbb{R})=v_{s}(\mathbb{R})+v_{a c}(\mathbb{R}) \Rightarrow v_{s}(\mathbb{R})=\beta, 0 \leq \beta \leq 1 ; v_{a c}(\mathbb{R})=1-\beta.$$
Возможны три случая:
\begin{enumerate}
    \item $\beta=0 \Rightarrow p_{2}=0 \Rightarrow F_{a c}(x)=F_{c}(x).$
    \item $\beta=1 \Rightarrow p_{1}=0 \Rightarrow F_{s}(x)=F_{c}(x).$
    \item $0 < \beta < 1.$ Положим
    $$F_{s}(x)=\frac{1}{\beta} \hat{F}_{s}(x), F_{a c}(x)=\frac{1}{1-\beta} \hat{F}_{a c}(x) \Rightarrow F_{c}(x)=\beta \cdot F_{s}(x)+(1-\beta) \cdot F_{a c}(x).$$
\end{enumerate}
Таким образом, все три функции определены, и коэффициенты соответственно равны
$$p_1 = (1 - \beta)(1 - \alpha), p_2 = \beta(1 - \alpha), p_3 = \alpha. ~~~ \square$$

\section{Числовые характеристики случайных величин: моменты, математическое ожидание, дисперсия. Их свойства.}

\subsubsection{Математическое ожидание случайной величины}

\mydef{} {\it Математическим ожиданием (средним значением, первым моментом)} случайной величины $\xi$, имеющей дискретное распределение со значениями $a_1, a_2, ...$, называется число
$$\mathrm{E} \xi=\sum_{i} a_{i} p_{i}=\sum_{i} a_{i} \mathrm{P}\left(\xi=a_{i}\right),$$
если данный ряд абсолютно сходится, т.е. если $\sum |a_i|p_i < \infty.$ В противном случае говорят, что математическое ожидание не существует.

\mydef{} {\it Математическим ожиданием} случайной величины $\xi$, имеющей абсолютно непрерывное распределение с плотностью распределения $f(x)$, называется число
$$\mathrm{E} \xi=\int\limits_{-\infty}^{\infty} x f(x) dx,$$
если этот интеграл абсолютно сходится, т.е. если $\int\limits_{-\infty}^{+\infty}|x|f(x)dx < \infty.$ Иначе математическое ожидание не существует.

\subsubsection{Свойства математического ожидания}
\mynote{} Везде далее предполагается, что рассматриваемые математические ожидания существуют.
\begin{enumerate}
    \item Для произвольной функции $g(x)$ со значениями в $\mathbb{R}$
    $$\mathrm{E} g(\xi)=\left\{\begin{array}{l}\sum\limits_{k} g\left(a_{k}\right) \mathrm{P}\left(\xi=a_{k}\right), \text { если распределение } \xi \text { дискретно; } \\ \int\limits_{-\infty}^{+\infty} g(x) f_{\xi}(x) d x, \text { если распределение } \xi \text { абсолютно непрерывно. }\end{array}\right.$$
    
    Такое же свойство верно и для числовых функций нескольких аргументов $g(x_1, ...,x_n)$, если $\xi$ - вектор из $n$ случайных величин, а в сумме и в интеграле участвует их совместное распределение. Например, для $g(x,y) = x + y$ и для случайных величин $\xi$ и $\eta$ с плотностью совместного распределения $f(x,y)$ верно: 
    \begin{equation}
        \mathrm{E}(\xi+\eta)=\int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty}(x+y) f(x, y) d x d y
    \end{equation}
    
    \item Математическое ожидание постоянной равно ей самой: $\mathrm{E} c=c.$
    \item Постоянный множитель можно вынести за знак математического ожидания: $\mathrm{E}(c\xi) = c\mathrm{E}\xi.$
    
    Это следует из свойства 1. при $g(x) = cx$.
    \item Математическое ожидание суммы {\it любых} случайных величин равно сумме их математических ожиданий: $\mathrm{E}(\xi + \eta) = \mathrm{E}\xi + \mathrm{E}\eta.$
    
    \myqed{} Воспользуемся равенством (1) и теоремой о совместном распределении:
    $$\begin{aligned}
    \mathrm{E}(\xi+\eta) &=\int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty}(x+y) f(x, y) d x d y=\\
    &=\int\limits_{-\infty}^{\infty} x d x \int\limits_{-\infty}^{\infty} f(x, y) d y+\int\limits_{-\infty}^{\infty} y d y \int\limits_{-\infty}^{\infty} f(x, y) d x=\\
    &=\int\limits_{-\infty}^{\infty} x f_{\xi}(x) d x+\int\limits_{-\infty}^{\infty} y f_{\eta}(y) d y=\mathrm{E} \xi+\mathrm{E} \eta ~~~ \square.
    \end{aligned}$$
    \item Если $\xi \geq 0$, то $\mathrm{E}\xi \geq 0.$
    
    \myqed{} Неотрицательность $\xi$ означает, что $a_i \geq 0$ при всех $i$ в случае дискретного распределения, либо $f_\xi(x) = 0$ при $x < 0$ - для абсолютно непрерывного распределения. И в том, и в другом случае имеем:
    $$\mathrm{E} \xi=\sum a_{i} p_{i} \geqslant 0 \quad \text { или } \quad \mathrm{E} \xi=\int_{0}^{\infty} x f(x) d x \geqslant 0 ~~~ \square.$$
    
    \mycon{} Если $\xi \leq \eta$, то $\mathrm{E}\xi \leq \mathrm{E}\eta.$
    
    \mycon{} Если $a \leq \xi \leq b$, то $a \leq \mathrm{E}\xi \leq b$.
    
    \item Математическое ожидание произведения {\it независимых} случайных величин равно произведению их математических ожиданий.
    
    \myqed{} В равенстве (1) заменим сложение умножением и плотность совместного распределениея произведением плотностей (это возможно в силу независимости случайных величин):
    $$\begin{aligned}
    \mathrm{E}(\xi \eta) &=\int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} x y f_{\xi}(x) f_{\eta}(y) d x d y=\\
    &=\int\limits_{-\infty}^{\infty} x f_{\xi}(x) d x \int\limits_{-\infty}^{\infty} y f_{\eta}(y) d y=\mathrm{E} \xi \mathrm{E} \eta ~~~ \square.
    \end{aligned}$$
    
    \mynote{} Обратное неверно: из равенства $\mathrm{E}(\xi \eta) = \mathrm{E}\xi \mathrm{E} \eta$ {\it не следует} независимость величин $\xi$ и $\eta$.
\end{enumerate}

\subsubsection{Дисперсия и моменты старших порядков}

\mydef{}. Пусть $\mathrm{E}|\xi|^k < \infty.$ Число $\mathrm{E}\xi^k$ называется {\it моментом порядка $k$ или $k$-м моментом} случайной величины $\xi$, число $\mathrm{E}|\xi|^k$ - {\it абсолютным $k$-м моментом}, число $\mathrm{E}(\xi - \mathrm{E}\xi)^k$ - {\it центральным $k$-м моментом}, и число $\mathrm{E}|\xi - \mathrm{E}\xi|^k$ - {\it абсолютным центральным $k$-м моментом} случайной величины $\xi$. 

\mydef{} Число $\mathrm{D}\xi = \mathrm{E}(\xi - \mathrm{E}\xi)^2$ (центральный момент второго порядка) называется {\it дисперсией} случайной величины $\xi$.

\mydef{} Число $\sigma = \sqrt{\mathrm{D}\xi}$ называют {\it среднеквадратичным отклонением} случайной величины $\xi$.

\myth{} Если существует момент порядка $t > 0$ случайной величины $\xi$, то существует и ее момент порядка $s$, где $0 < s < t$.

\myqed{} Заметим, что $|\xi|^s \leq |\xi|^t + 1.$ В силу следствия из свойства 5 для математического ожидания можно получить из неравенства для случайных величин такое же неравенство для их математических ожиданий: $\mathrm{E}|\xi|^s \leq \mathrm{E}|\xi|^t + 1 < \infty. ~~~ \square$

\myth{} \textbf{Неравенство Йенсена}. (без доказательства)

Пусть функция $g(x)$ на своей области определения выпукла, т.е. область над графиком этой функции есть выпуклое множество. Тогда для любой случайно величины $\xi$ верно неравенство: 
$$\mathrm{E}g(\xi) \geq g(\mathrm{E}\xi).$$
Для вогнутых функций знак неравенства меняется на противоположный.

В частности,
$$\begin{array}{ccc}
\mathrm{E} e^{\xi} \geqslant e^{\mathrm{E} \xi}, & \mathrm{E} \xi^{2} \geqslant(\mathrm{E} \xi)^{2}, & \mathrm{E}|\xi| \geqslant|\mathrm{E} \xi| \\
\mathrm{E} \ln \xi \leqslant \ln (\mathrm{E} \xi), & \mathrm{E} \frac{1}{\xi} \geqslant \frac{1}{\mathrm{E} \xi}, & \mathrm{E} \sqrt{\xi} \leqslant \sqrt{\mathrm{E} \xi}
\end{array}$$

Последние три неравенства верны для положительных $\xi$.

\subsubsection{Свойства дисперсии}

\mynote{} Во всех свойствах предполагается существование вторых моментов случайных величин. Тогда (в силу вышеописанной теоремы) существуют и сами матожидания.

\begin{enumerate}
    \item Дисперсия может быть вычислена по формуле: $\mathrm{D}\xi = \mathrm{E}\xi^2 - (\mathrm{E}\xi)^2$.
    
    \myqed{} Обозначим для удобства $a = \mathrm{E}\xi.$ Тогда
    $$\mathrm{D} \xi=\mathrm{E}(\xi-a)^{2}=\mathrm{E}\left(\xi^{2}-2 a \xi+a^{2}\right)=\mathrm{E} \xi^{2}-2 a \mathrm{E} \xi+a^{2}=\mathrm{E} \xi^{2}-a^{2}. ~~~ \square$$
    
    \item При умножении случайной величины на постоянную $c$ дисперсия увеличивается в $c^2$ раз: $\mathrm{D}(c\xi) = c^2\mathrm{D}\xi.$
    \item Дисперсия всегда неотрицательна: $\mathrm{D}\xi \geq 0.$
    
    \myqed{} Пусть $a = \mathrm{E}\xi.$ Дисперсия есть математичекое ожидание неотрицательной случайной величины $(\xi - a)^2$, откуда (и из свойства 5 матожидания) следует неотрицательность дисперсии. $ ~~~ \square$
    
    \item Дисперсия обращается в нуль лишь для вырожденного распределения: если $\mathrm{D}\xi = 0$, то $\xi = const$, и наоборот.
    
    \myqed{} $\mathrm{D}\xi = 0 \Rightarrow (\xi - a)^2 = 0, \xi = a = const.$ И наоборот: если $\xi = c$, то $\mathrm{D} \xi=\mathrm{E}(c-\mathrm{E} c)^{2}=\mathrm{E} 0=0. ~~~ \square$
    
    \item Дисперсия не зависит от сдвига случайной величины на постоянную: $\mathrm{D}(\xi + c) = \mathrm{D}\xi.$
    
    \item Если $\xi$ и $\eta$ независимы, то $\mathrm{D}(\xi + \eta) = \mathrm{D}\xi + \mathrm{D}\eta.$
    
    \myqed{} Действительно, применяя свойство (6) матожидания, получим:
    $$\begin{aligned}
    \mathrm{D}(\xi+\eta) &=\mathrm{E}(\xi+\eta)^{2}-(\mathrm{E}(\xi+\eta))^{2}=\\
    &=\mathrm{E} \xi^{2}+\mathrm{E} \eta^{2}+2 \mathrm{E}(\xi \eta)-(\mathrm{E} \xi)^{2}-(\mathrm{E} \eta)^{2}-2 \mathrm{E} \xi \mathrm{E} \eta=\mathrm{D} \xi+\mathrm{D} \eta
    \end{aligned}. ~~~ \square$$
    
    \mynote{} Обратное, аналогично замечанию к свойству (6) матожидания, неверно.
    
    \mycon{} Если $\xi$ и $\eta$ независимы, то $\mathrm{D}(\xi-\eta)=\mathrm{D} \xi+\mathrm{D} \eta$. 
    
    \myqed{} Из свойств (6) и (2) получим: 
    $$\mathrm{D}(\xi-\eta)=\mathrm{D}(\xi+(-\eta))=\mathrm{D} \xi+\mathrm{D}(-\eta)=\mathrm{D} \xi+(-1)^{2} \mathrm{D} \eta=\mathrm{D} \xi+\mathrm{D} \eta. ~~~ \square$$
    
    \mycon{} Для произвольных случайных величин $\xi$ и $\eta$ имеет место равенство:
    $$\mathrm{D}(\xi \pm \eta)=\mathrm{D} \xi+\mathrm{D} \eta \pm 2(\mathrm{E}(\xi \eta)-\mathrm{E} \xi \mathrm{E} \eta).$$
    
\end{enumerate}

\section {Числовые характеристики случайных величин: квантили. Медиана и ее свойства. Интерквартильный размах.}

\mydef{} {\it Медианой} распределения случайной величины $\xi$ называется любое из {\it чисел} $\mu$ таких, что
$$\myprob{\xi \leqslant \mu} \geqslant \frac{1}{2}, ~~~ \myprob{\xi \geqslant \mu} \geqslant \frac{1}{2}.$$

\mynote{} Медиана распределения всегда существует, но может быть не единственна.

\mydef{} Пусть, для простоты, функция распределения $F$ непрерывна и строго монотонна. Тогда {\it квантилью} уровня $\delta$, где $\delta \in (0, 1), $ называется решение $x_\delta$ уравнения $F(x_\delta) = \delta.$ 

Квантиль уровня $\delta$ отрезает от области под графиком плотности область с площадью с площадью $\delta$ слева от себя. Справа от $x_\delta$ площадь области равна $1 - \delta$.

\mydef{} Квантили уровней, кратных $0,01$, называют {\it процентилями}, квантили уровней, кратных $0,1$, — {\it децилями}, уровней, кратных $0,25$, — {\it квартилями}.

\mynote{} Медиана является одной из квантилей распределения, уровня $\frac{1}{2}.$

\subsubsection{Свойства медианы}

Экстремальные свойства:

\begin{enumerate}
    \item Медиана $\mathrm{Med}~\xi$ случайной величины $\xi$ минимизирует средний модуль её отклонения:
    $$\mathrm{E}|\xi-\operatorname{Med} \xi|=\min _{a} \mathrm{E}|\xi-a|$$
    
    \myqed{}
    $$\mathrm{E}|\xi - a| - \mathrm{E}|\xi| = a\myprob{\xi \leqslant a} - a\myprob{\xi > a} - 2\mathrm{E}(\xi; 0 < \xi \leqslant a) \geqslant$$ $$\geqslant a\myprob{\xi \leqslant a} - a\myprob{\xi > a} - 2a\myprob{0 < \xi \leqslant a} = a(\myprob{\xi \leqslant 0} - \myprob{\xi > 0}) \geqslant 0. ~~~\square $$
\end{enumerate}

\subsubsection{Интерквантильный размах}

\mydef{} {\it Интерквартильным размахом} называется разность между третьим и первым квартилями, то есть ${\displaystyle x_{0{,}75}-x_{0{,}25}}.$

\section {Испытания Бернулли. Биномиальное распределение. Теорема Пуассона. (док-во в Ульянове) Распределение Пуассона.}

\mydef{} {\it Схемой Бернулли} называется последовательность независимых испытаний, в каждом из которых возможны лишь два исхода - <<успех>> и <<неудача>>, при этом успех в каждом испытании происходит с одной и той же вероятностью $p \in (0,1)$, а неудача - с вероятностью $q = 1 - p.$

\mydes{} $\mathrm{v}_n$ - число успехов, случившихсся в $n$ испытаниях схемы Бернулли. 

\myth{} (формула Бернулли). Для любого $k = 0, 1, ..., n$ вероятность получить в $n$ испытаниях ровно $k$ успехов равна
\begin{equation}
    \mathbf{P}\left(\mathrm{v}_{n}=k\right)=C_{n}^{k} p^{k} q^{n-k}.
\end{equation}

\mydef{} Набор вероятностей (2) называется {\it биномиальным распределением} вероятностей.

\myqed{} Событие $A = \{ \mathrm{v}_n = k \}$ означает, что в $n$ испытаниях схемы Бернулли произошло ровно $k$ успехов. Рассмотрим один элементарный исход из события $A$:
$$(\underbrace{y, y, \ldots, y}_{k}, \underbrace{\text{\it н}, \text{\it н}, \ldots,\text{\it н}}_{n-k}),$$
когда первые $k$ испытаний завершились успехом (у), остальные неудачей (н). Поскольку испытания независимы, вероятность такого элементарного исхода равна $p^k(1 - p)^{n-k}.$ Другие элементарные исходы из события $A$ отличаются лишь расположением $k$ успехов на $n$ местах. Поэтому событие $A$ состоит из $C_n^k$ элементарых исходов, вероятность каждого из которых равна $p^kq^{n-k}.~~~ \square$

\myth {} \textbf{(Пуассона)}. Пусть проводится $n$ обобщённых испытаний Бернулли
(то есть, вероятность успеха испытания зависит от $n$) с вероятностью успеха
$p_n$, $S_n$ — количество успехов в этих испытаниях и $n p_{n} \underset{n \rightarrow \infty}{\longrightarrow} a$. Тогда

$$\forall k \in \mathbb{Z}: 0 \leqslant k \leqslant n \quad \mathrm{P}\left(S_{n}=k\right) \underset{n \rightarrow \infty}{\longrightarrow} \frac{a^{k}}{k !} e^{-a}.$$

\mydef{} Набор вероятностей $\{ \frac{\lambda^k}{k!} e^{-\lambda} \}$, где $k$ принимает значения $0, 1, 2, ...$, называется {\it распределением Пуассона} с параметром $\lambda > 0$.

\myqed{} 

$$\begin{aligned}
\mathrm{P}\left(S_{n}=k\right) &=C_{n}^{k} p_{n}^{k}\left(1-p_{n}\right)^{n-k}=\frac{p_{n}^{k} n !}{k !(n-k) !}\left(1-p_{n}\right)^{n}\left(1-p_{n}\right)^{-k} \\
&=\frac{\left(n p_{n}\right)^{k}}{k !} \cdot \frac{n !}{(n-k) ! n^{k}} \cdot \frac{\left(1-p_{n}\right)^{n}}{\left(1-p_{n}\right)^{k}}
\end{aligned},$$
$k$ - константа. Поэтому
$$\lim\limits_{n \rightarrow \infty} \frac{\left(n p_{n}\right)^{k}}{k !}=\frac{a^{k}}{k !}.$$
Далее,
$$\lim _{n \rightarrow \infty} \frac{n !}{(n-k) ! n^{k}}=\lim _{n \rightarrow \infty}(1)\left(1-\frac{1}{n}\right)\left(1-\frac{2}{n}\right) \cdot \ldots \cdot\left(1-\frac{k-2}{n}\right)\left(1-\frac{k-1}{n}\right) =$$
$$=\lim _{n \rightarrow \infty}\left(1-\frac{1}{n}\right) \lim _{n \rightarrow \infty}\left(1-\frac{2}{n}\right) \cdot \ldots \cdot \lim _{n \rightarrow \infty}\left(1-\frac{k-2}{n}\right) \lim _{n \rightarrow \infty}\left(1-\frac{k-1}{n}\right)=1.$$
При выяснении предела третьей дроби требуется второй замечательный предел и
непрерывность показательной функции. Кроме того, отметим, что раз $n p_{n} \underset{n \rightarrow \infty}{\longrightarrow} a$, то $p_{n} \underset{n \rightarrow \infty}{\longrightarrow} 0$. Таким образом
$$\begin{aligned}
\lim _{n \rightarrow \infty} \frac{\left(1-p_{n}\right)^{n}}{\left(1-p_{n}\right)^{k}} &=\frac{\lim _{n \rightarrow \infty}\left(1-p_{n}\right)^{n}}{\lim _{n \rightarrow \infty}\left(1-p_{n}\right)^{k}}=\frac{\lim _{n \rightarrow \infty}\left(1-p_{n}\right)^{n}}{1}=\lim _{n \rightarrow \infty}\left(1-p_{n}\right)^{\frac{1}{p_{n}} n p_{n}} \\
&=\lim _{n \rightarrow \infty}\left(\left(1-p_{n}\right)^{-\frac{1}{p_{n}}}\right)^{-n p_{n}}=\lim _{n \rightarrow \infty}\left(\left(1-p_{n}\right)^{-\frac{1}{p_{n}}}\right)^{a-n p_{n}-a} \\
&=\lim _{n \rightarrow \infty}\left(\left(1-p_{n}\right)^{-\frac{1}{p_{n}}}\right)^{-a}\left(\left(1-p_{n}\right)^{-\frac{1}{p_{n}}}\right)^{a-n p_{n}} \\
&=\lim _{n \rightarrow \infty}\left(\left(1-p_{n}\right)^{-\frac{1}{p_{n}}}\right)^{-a} \lim _{n \rightarrow \infty}\left(\left(1-p_{n}\right)^{-\frac{1}{p_{n}}}\right)^{a-n p_{n}}
\end{aligned}$$
Последовательность $a - np_n$ является бесконечно малой, обозначим её как $\gamma_n.$ Вторая скобка:
$$\lim _{n \rightarrow \infty}\left(\left(1-p_{n}\right)^{-\frac{1}{p_{n}}}\right)^{\gamma_{n}}=\lim _{n \rightarrow \infty} e^{\gamma_{n} \frac{\ln \left(1-p_{n}\right)}{p_{n}}}=e^{\lim\limits _{n \rightarrow \infty} \gamma_{n} \frac{\ln \left(1-p_{n}\right)}{p_{n}}}=e^{\lim\limits _{n \rightarrow \infty} \gamma_{n} \lim\limits_{n \rightarrow \infty} \frac{\ln \left(1-p_{n}\right)}{p_{n}}}=e^{0 \cdot 1}=1.$$
Первая скобка:
$$\lim _{n \rightarrow \infty}\left(\left(1-p_{n}\right)^{-\frac{1}{p_{n}}}\right)^{-a}=\left(\lim _{n \rightarrow \infty}\left(1-p_{n}\right)^{-\frac{1}{p_{n}}}\right)^{-a}=e^{-a}.$$
Собирая всё вышеполученное:
$$\lim _{n \rightarrow \infty} \mathrm{P}\left(S_{n}=k\right)=\lim _{n \rightarrow \infty} \frac{\left(n p_{n}\right)^{k}}{k !} \cdot \frac{n !}{(n-k) ! n^{k}} \cdot \frac{\left(1-p_{n}\right)^{n}}{\left(1-p_{n}\right)^{k}} = $$
$$=\lim _{n \rightarrow \infty} \frac{\left(n p_{n}\right)^{k}}{k !} \cdot \lim _{n \rightarrow \infty} \frac{n !}{(n-k) ! n^{k}} \cdot \lim _{n \rightarrow \infty} \frac{\left(1-p_{n}\right)^{n}}{\left(1-p_{n}\right)^{k}}=\frac{a^{k}}{k !} \cdot 1 \cdot e^{-a}=\frac{a^{k}}{k !} e^{-a} \cdot \square$$

\section {Испытания Бернулли. Геометрическое распределение. Теорема Реньи. Показательное распределение.}

\myth{} Вероятность того, что первый успех произойдёт в испытании с номером $k \in \mathbb{N} = \{1, 2, 3, ...\}$, равна $\myprob{\tau = k} = pq^{k-1}$.

\myqed{} Вероятность первым $k - 1$ испытаниям завершиться неудачей, а последнему - успехом, равна 
$$\mathrm{P}(\tau=k)=\mathrm{P}(\mu, \ldots, \mu, y)=p q^{k-1}. ~~~ \square$$

\mydef{} Набор вероятностей $\{p q^{k-1}\}$, где $k$ принимает любые значения из множества натуральных чисел, называется {\it геометрическим распределением} вероятностей. 

\mydef{} Случайная величина $\xi$ имеет {\it показательное (экспоненциальное) распределение} с параметром $\alpha > 0$, если $\xi$ имеет следующие плотность и функцию распределения:
$$f(x)=\left\{\begin{array}{cc}
0, & \text { если } x<0 \\
\alpha e^{-\alpha x}, & \text { если } x \geqslant 0 ;
\end{array} \quad F(x)=\left\{\begin{array}{cc}
0, & \text { если } x<0 \\
1-e^{-\alpha x}, & \text { если } x \geqslant 0
\end{array}\right.\right.$$

\myth{} \textbf{(Реньи)} (без доказательства). Пусть случайная величина $N$ имеет геометрическое распределение. $X_1, X_2, ...$ - независимые одинаково распределённые случайные величины, $X_i \geqslant 0$ и $0 < a = \mathrm{E}X < \infty$, $S_n = \sum\limits_{i=1}^N X_i.$

Тогда $$\sup _{x}\left|P\left(\frac{p}{a} S_{N}<x\right)-G(x)\right| \underset{p \rightarrow 0}{\longrightarrow} 0,$$
где $G(x)$ - функция стандартного показательного распределения.

Если $b^2 = \mathrm{E}X_i^2$, тогда $$\sup_{x}\left|P\left(\frac{p}{a} S_{N}<x\right)-G(x)\right| \leqslant \frac{p b^{2}}{(1-p) a^{2}}.$$
 
\section{Испытания Бернулли. Теорема Муавра—Лапласа. Нормальное распределение.}

\myth{} \textbf{(Локальная предельная теорема Муавра-Лапласа.)} (без доказательства) Пусть $S_n$ - число успехов в $n$ испытаниях Бернулли с вероятностью успеха $p$. Если $n p(1-p) \underset{n \rightarrow \infty}{\longrightarrow} \infty$, то
$$\forall m \in \mathbb{Z}: 0 \leqslant m \leqslant n \quad \mathrm{P}\left(S_{n}=m\right)=\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{x^{2}}{2}}\left(1+\underline{O}\left(\frac{1}{\sigma}\right)\right),$$
где $x = \frac{m - np}{\sigma},$ а $\sigma=\sqrt{\mathrm{D} S_{n}}=\sqrt{n p(1-p)}$.

\myth{} \textbf{(Интегральная теорема Муавра-Лапласа)} Если выполнено условие локальной теоремы и $C$ - произвольная положительная константа, то равномерно по $a$ и $b$ из отрезка $[-C,C]$ (пусть $b \geqslant a$)
$$\mathrm{P}\left(a \leqslant \frac{S_{n}-n p}{\sqrt{n p(1-p)}} \leqslant b\right) \underset{n \rightarrow \infty}{\longrightarrow} \frac{1}{\sqrt{2 \pi}} \int\limits_{a}^{b} e^{-\frac{x^{2}}{2}} d x$$

\mynote{} Утверждение верно $\forall a,b \in \mathbb{R}$. Ограничение требуется для простоты доказательства.

\myqed{} Обозначим за $q$ величину $1 - p$ и проведём небольшие выкладки с использованием локальной предельной теоремы в переходе, помеченном $\triangle$. В переходе $*$ - перенос знака.

$$\mathrm{P}\left(a \leqslant \frac{S_{n}-n p}{\sqrt{n p(1-p)}} \leqslant b\right)=\mathrm{P}\left(n p+a \sqrt{n p q} \leqslant S_{n} \leqslant n p+b \sqrt{n p q}\right) \stackrel{*}{=}$$

Пусть множество $M$ - это все целые числа $m$, такие что выполнено неравенство в последней вероятности. То есть,

$$\begin{array}{c}
M=\{m: n p+a \sqrt{n p q} \leqslant m \leqslant n p+b \sqrt{n p q}\} \Rightarrow \\
\stackrel{*}{=} \sum\limits_{m \in M} \mathrm{P}\left(S_{n}=m\right) \triangleq \sum_{m \in M} \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{x_{m}^{2}}{2}}\left(1+\underline{O}\left(\frac{1}{\sigma}\right)\right),
\end{array}$$
где $x_m$ понимается как $\frac{m - np}{\sigma}.$ Обозначим как $\Delta x_{m}$ разность $x_m - x_{m-1}.$ Рассмотрим эту разность:
$$\Delta x_{m}=x_{m}-x_{m-1}=\frac{m-n p}{\sigma}-\frac{m-1-n p}{\sigma}=\frac{m-n p-m+1+n p}{\sigma}=\frac{1}{\sigma}.$$
Таким образом после замены в последней сумме $\frac{1}{\sigma}$ на $\Delta x_m$ ясно, что это интегральная сумма Римана для интегрируемой на любом отрезке функции $e^{-\frac{x^2}{2}}$ плюс $\underline{O}\left(\frac{1}{\sigma}\right)$:
$$\Delta x_{m}=\frac{1}{\sigma}=\frac{1}{\sqrt{n p(1-p)}} \underset{n \rightarrow \infty}{\longrightarrow} 0$$ по условию локальной теоремы.

Таким образом, на отрезке $[-C,C]$ сумма сходится к интегралу:
$$\sum_{m \in M} \frac{1}{\sqrt{2 \pi}} \Delta x_{m} e^{-\frac{x_{m}^{2}}{2}}\left(1+\underline{O}\left(\Delta x_{m}\right)\right) \underset{n \rightarrow \infty}{\longrightarrow} \frac{1}{\sqrt{2 \pi}} \int\limits_{a}^{b} e^{-\frac{x^{2}}{2}} d x,$$

следовательно сойдется и на любом подотрезке. Причём в
силу свойств сумм Римана, разность между суммой и предельным значением будет
меньше $\varepsilon$ при $n$ не меньше некоторого $N_\varepsilon$, которое для всех допустимых $a$ и $b$ будет одним и тем же - это и есть равномерность (что верно в силу критерия Дарбу о сходимости). $~~~ \square$

\mydef{} Случайная величина $\xi$ имеет {\it нормальное (гауссовское) распределение} с параметрами $a$ и $\sigma^2$, где $a \in \mathbb{R}, \sigma > 0$, если $\xi$ имеет следующую плотность распределения: 
$$f(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-a)^{2}}{2 \sigma^{2}}}, \quad x \in \mathbb{R}.$$

\section{Совокупности случайных величин. Совместная функция распределения. Независимость случайных величин. Критерии независимости. Ковариация, коэффициент корреляции.}

\subsubsection{Совместное распределение, его свойства}

Пусть случайные величины $\xi_1, ..., \xi_n$ заданы на одном вероятностном пространстве $(\Omega, \mathcal{F}, \mathbb{P})$.

\mydef{} Функция $$F\left(x_{1}, \ldots, x_{n}\right)=\mathrm{P}\left(\xi_{1}<x_{1}, \ldots, \xi_{n}<x_{n}\right)$$
называется функцией распределения вектора $(\xi_1, ..., \xi_n)$ или функцией {\it совместного} распределения случайных величин $\xi_1, ... \xi_n$.

Свойства для двумерного случая $(\xi, \eta)$:

\begin{enumerate}
    \item Функция распределения $F(x,y) = \myprob{\xi < x, \eta < y}$ не убывает по $x$ и по $y$.
    \item $F(x,y)$ стремится к нулю, если любую из переменных устремить к $-\infty$.
    \item $F(x,y)$ стремится к единице, если {\it обе} переменные устремить к $+\infty$.
    \item $F(x,y)$ непрерывна слева по каждой переменной
    \item Чтобы по функции совместного распределения восстановить функцию распределения $\xi$ и $\eta$ в отдельности, следует устремить <<лишнюю>> переменную к $+\infty$:
    $$\lim _{x \rightarrow+\infty} F(x, y)=F_{\eta}(y), \quad \lim _{y \rightarrow+\infty} F(x, y)=F_{\xi}(x)$$
\end{enumerate}

\subsubsection{Виды многомерных распределений}

\mydef{} Совместное распределение величин $\xi$ и $\eta$ {\it дискретно}, если каждая из них имеет дискретное распределение. Если $\xi$ принимает значения $a_1, a_2, ...$, а $\eta$ принимает значения $b_1, b_2, ...$, то пара $(\xi, \eta)$ принимает всевозможные значения $(a_i, b_j)$. Таблицу, на пересечении $i$-ой строки и $j$-го столбца которой стоит вероятность $p_{i,j} = \myprob{\xi = a_i, \eta = b_j}$, называют {\it таблицей совместного распределения} случайных величин $\xi$ и $\eta$. Вероятности $p_{i,j}$ в сумме дают единицу:
$$\sum_{i} \sum_{j} p_{i, j}=1$$

\mydef{} Говорят, что случайные величины $\xi$ и $\eta$ имеют {\it абсолютно непрерывное совместное распределение}, если существует неотрицательная функция $f(x,y)$ такая, что для любых $a < b, c < d$ имеет место равенство
$$\mathrm{P}(a<\xi<b, c<\eta<d)=\int_{a}^{b} d x \int_{c}^{d} f(x, y) d y.$$
Если такая функция $f(x,y)$ существует, она называется {\it плотностью совместного распределения} случайных величин $\xi$ и $\eta$.

Если случайные величины $\xi$ и $\eta$ имеют абсолютное непрерывное совместное распределение, то для любых $x, y$ имеет место равенство:
\begin{equation}F(x, y)=\mathrm{P}(\xi<x, \eta<y)=\int\limits_{-\infty}^{x} d u \int\limits_{-\infty}^{y} f(u, v) d v\end{equation}

Плотность совместного распределения имеет те же свойства, что и плотность распределения одной случайной величины: неотрицательность и нормированность:
$$f(x, y) \geqslant 0, \quad \int\limits_{-\infty}^{\infty} d x \int\limits_{-\infty}^{\infty} f(x, y) d y=1.$$

По функции совместного распределения его плотность находится как смешанная частная производная (в точках, где она существует):
$$f(x, y)=\frac{\partial^{2}}{\partial x \partial y} F(x, y).$$

\myth{} Если случайные величины $\xi$ и $\eta$ имеют абсолютное непрерывное совместное распределение с плотностью $f(x,y)$, то $\xi$ и $\eta$ в отдельности тоже имеют абсолютно непрерывные распределения с плотностями:
$$f_{\xi}(x)=\int_{-\infty}^{\infty} f(x, y) d y ; \quad f_{\eta}(y)=\int_{-\infty}^{\infty} f(x, y) d x.$$

\myqed{} TODO

\subsubsection{Независимость случайных величин}

\mydef{} Случайные величины $\xi_1, ..., \xi_n$ называют независимыми, если для любых множеств $B_1, ..., B_n$ имеет место равенство:

$$\mathrm{P}\left(\xi_{1} \in B_{1}, \ldots, \xi_{n} \in B_{n}\right)=\mathrm{P}\left(\xi_{1} \in B_{1}\right) \cdot \ldots \cdot \mathrm{P}\left(\xi_{n} \in B_{n}\right).$$

Можно сформулировать и другое, равносильное определение.

\mydef{} Случайные величины $\xi_1, ..., \xi_n$ независимы, если функция совместного распределения распадается в произведение частных функций распределения, т.е. для любых $x_1, ..., x_n$ имеет место равенство:
$$F\left(x_{1}, \ldots, x_{n}\right)=F_{\xi_{1}}\left(x_{1}\right) \cdot \ldots \cdot F_{\xi_{n}}\left(x_{n}\right).$$

Для случайных величин с дискретными или с абсолютно непрерывными
распределениями эквивалентные определения независимости выглядят так.

\mydef{} Случайные величины $\xi_1, ..., \xi_n$ с дискретным распределением независимы, если для любых чисел $a_1, ..., a_n$ имеет место равенство: $\mathrm{P}\left(\xi_{1}=a_{1}, \ldots, \xi_{n}=a_{n}\right)=\mathrm{P}\left(\xi_{1}=a_{1}\right) \cdot \ldots \cdot \mathrm{P}\left(\xi_{n}=a_{n}\right)$.

\mydef{} Случайные величины $\xi_1, ..., \xi_n$ с абсолютно непрерывным совместным распределением независимы, если плотность совместного распределения распадается в произведение плотностей, т.е. для любых $x_1, ..., x_n$ имеет место равенство: $f\left(x_{1}, \ldots, x_{n}\right)=f_{\xi_{1}}\left(x_{1}\right) \cdot \ldots \cdot f_{\xi_{n}}\left(x_{n}\right)$.

TODO Критерии независимости

\subsubsection{Ковариация, коэффициент корреляции, их свойства}

\mydef{} {\it Ковариацией} $\operatorname{cov}(\xi,\eta)$ случайных величин $\xi$ и $\eta$ называется число $$\operatorname{cov}(\xi,\eta)=\mathrm{E}((\xi-\mathrm{E} \xi)(\eta-\mathrm{E} \eta)).$$

\mynote{} $$\begin{aligned} \operatorname{cov}(\xi, \eta) &=\mathrm{E}((\xi-\mathrm{E} \xi)(\eta-\mathrm{E} \eta))=\mathrm{E}(\xi \eta-\eta \mathrm{E} \xi-\xi \mathrm{E} \eta+\mathrm{E} \xi \mathrm{E} \eta) \\ &=\mathrm{E} \xi \eta-\mathrm{E} \eta \mathrm{E} \xi-\mathrm{E} \xi \mathrm{E} \eta+\mathrm{E} \xi \mathrm{E} \eta=\mathrm{E} \xi \eta-\mathrm{E} \xi \mathrm{E} \eta \end{aligned}$$

Свойства:

\begin{enumerate}
    \item $\operatorname{cov}(\xi, \xi)=\mathrm{D} \xi.$
    \item $\operatorname{cov}(\xi, \eta)=\operatorname{cov}(\eta, \xi).$
    \item $\operatorname{cov}(c \xi, \eta)=c \operatorname{cov}(\xi, \eta).$
    \item Дисперсия суммы нескольких случайных величин вычисляется по любой из следующих формул:
    $$\mathrm{D}\left(\xi_{1}+\ldots+\xi_{n}\right)=\sum_{i=1}^{n} \mathrm{D} \xi_{i}+\sum_{i \neq j} \operatorname{cov}\left(\xi_{i}, \xi_{j}\right)=\sum_{i=1}^{n} \mathrm{D} \xi_{i}+2 \sum_{i<j} \operatorname{cov}\left(\xi_{i}, \xi_{j}\right).$$
\end{enumerate}

\mydef{} {\it Коэффициентом корреляции} $\rho(\xi,\eta)$ случайных величин $\xi$ и $\eta$, дисперсии которых существуют и отличны от нуля, называется число
$$\rho(\xi, \eta)=\frac{\operatorname{cov}(\xi, \eta)}{\sqrt{D \xi} \sqrt{D \eta}}.$$

Свойства:

\begin{enumerate}
    \item Коэффициент корреляции независимых случайных величин равен нулю.
    
    \myqed{} В числителе дроби, которой равен коэффициент корреляции,
окажется ноль. В знаменателе нуля быть не должно, это обеспечивается определением.
$ ~~~ \square$

    \item Для любых двух случайных величин (для которых выполнены условия определения) их коэффициент корреляции по модулю не превосходит единицы.
    
    \myqed{} Обозначим эти две случайные величины как $X$ и $Y$ и центрируем: $X_c = X - \mathrm{E}X$ и $Y_c = Y - \mathrm{E}Y$. Так как $\operatorname{cov}(X, Y)=\operatorname{cov}\left(X_{c}, Y_{c}\right)$, а дисперсия случайной величины не меняется от смещения случайной величины на константу, коэффициент корреляции не изменится.
    
    Далее, т.к. $\mathrm{E} X_{c}=\mathrm{E} Y_{c}=0$:
    $$\mathrm{D} X_{c}=\mathrm{E} X_{c}^{2}-\left(\mathrm{E} X_{c}\right)^{2}=\mathrm{E} X_{c}^{2}, \mathrm{D} Y_{c}=\mathrm{E} Y_{c}^{2},$$$$ \operatorname{cov}\left(X_{c}, Y_{c}\right)=\mathrm{E}\left(X_{c} Y_{c}\right)-\mathrm{E} X_{c} \mathrm{E} Y_{c}=\mathrm{E}\left(X_{c} Y_{c}\right)$$
    
    Далее идут те же рассуждения, что часто используются при доказательстве неравенства Коши-Буняковского:
    
    $$\forall a \in \mathbb{R} \quad 0 \leqslant \mathrm{D}\left(X_{c}-a Y_{c}\right)=\mathrm{E}\left(X_{c}-a Y_{c}\right)^{2}-\left(\mathrm{E}\left(X_{c}-a Y_{c}\right)\right)^{2}=\mathrm{E}\left(X_{c}-a Y_{c}\right)^{2}.$$
    
    Полученное неравенство можно рассматривать как квадратное неравенство относительно $a$, а именно
    
    $$\mathrm{E}\left(X_{c}-a Y_{c}\right)^{2}=\mathrm{E} X_{c}^{2}-2 a \mathrm{E}\left(X_{c} Y_{c}\right)+a^{2} \mathrm{E} Y_{c}^{2} \geqslant 0$$
    
    Поскольку верно это для любого $a$, то дискриминанту нельзя ни в коем случае быть больше нуля. То есть:
    
    $$\left(\mathrm{E}\left(X_{c} Y_{c}\right)\right)^{2}-\mathrm{E} X_{c}^{2} \mathrm{E} Y_{c}^{2} \leqslant 0 \Longleftrightarrow\left|\mathrm{E}\left(X_{c} Y_{c}\right)\right| \leqslant \sqrt{\mathrm{E} X_{c}^{2} \mathrm{E} Y_{c}^{2}} $$$$\Rightarrow\left|\operatorname{cov}\left(X_{c}, Y_{c}\right)\right| \leqslant \sqrt{\mathrm{D} X_{c} \mathrm{D} Y_{c}}.$$
    
    По доказанному выше <<стирание>> индексов не изменит коэффициентов. $\square$

    \item Если $|\rho(X,Y)| = 1$, то с вероятностью один $X$ и $Y$ линейно выражаются друг через друга. То есть,
    $$|\rho(X, Y)|=1 \Longrightarrow \exists b \neq 0, c \in \mathbb{R}: \mathrm{P}(X-b Y=c)=1.$$
    
    \myqed{} Доказательство этого свойства целиком опирается на доказательство предыдущего: если выполнилось равенство $|\operatorname{cov}(X, Y)|=\sqrt{\mathrm{D} X \mathrm{D} Y}$, то квадратное неравенство относительно $a$ может обращаться в равенство при некотором $a = b$. Но это равенство означает, что равна нулю $\mathrm{D}(X-b Y)$, а это сразу говорит о том, что с вероятностью один $X - bY$ равна константе. Обозначим эту константу за $c$ и получим то, что нужно было доказать. $~~~\square$

\end{enumerate}

\section{Виды сходимости последовательностей случайных величин.}

Везде далее каждая случайная величина $\xi_n(\omega)$ задана на вероятностном пространстве $(\Omega_n,\mathcal{F}_n,\mathbb{P})$.

\mydef{} Последовательность случайных величин $\{ \xi_n \}$ сходится к случайной величине $\xi$ {\it по вероятности}: $\lim\limits_{n \rightarrow \infty} \xi_{n} \overset{\mathbb{P}}{=} \xi$, если для $\forall \varepsilon > 0 \Rightarrow \lim _{n \rightarrow \infty} \mathbf{P}\left(\left|\xi_{n}-\xi\right|>\varepsilon\right)=0.$

\mydef{} Последовательность случайных величин $\{ \xi_n \}$ сходится к случайной величине $\xi$ {\it с вероятностью 1 (почти всюду)}: $\xi_n \xrightarrow[n \rightarrow \infty]{\text{с вероятностью}~1} \xi$, если $\mathbf{P}\left(\omega: \xi_{n}(\omega) \rightarrow \xi(\omega)\right)=1$. Рассматривается на вероятностном пространстве $(\Omega, \mathcal{F}, \mathbb{P})$, $\{ \xi_n(\omega) \}$ - последовательность случайных величин (по определению измеримых), следовательно, определение корректно.

Введём множество $A=\left\{\left(\omega: \xi_{n}(\omega) \rightarrow \xi(\omega)\right\}\right.$. Его можно представить в виде 
$$A=\bigcap_{k=1}^{\infty} \bigcup_{N=1}^{\infty} \bigcap_{n=N}^{\infty}\left(\omega:\left|\xi_{n}(\omega)-\xi(\omega)\right|<\frac{1}{k}\right)=\bigcap_{k=1}^{\infty} \varliminf_{n \rightarrow \infty}\left(\omega:\left|\xi_{n}(\omega)-\xi(\omega)\right|<\frac{1}{k}\right),$$ $A \in \mathcal{F}.$

или, что то же самое, $A=\left(\omega: \forall k \in \mathbf{N}, \exists N: \forall n \geq N \Rightarrow\left|\xi_{n}(\omega)-\xi(\omega)\right|<\frac{1}{k}\right).$

Тогда определение сходимости с вероятностью 1 можно переписать как $\myprob{A} = 1.$

\mydef{} Последовательность случайных величин $\{ \xi_n \}$ сходится к случайной величине $\xi$ {\it в среднем порядка} $\alpha > 0: \xi_n \rightarrow \xi$, если $\lim _{n \rightarrow \infty} \mathbf{E}\left|\xi_{n}-\xi\right|^{\alpha}=0.$

\mydef{} Последовательность случайных величин $\{ \xi_n \}$ сходится к случайной величине $\xi$ {\it по распределению}: $\xi_n \rightarrow \xi$, если $F_{\xi_n}(x) \rightarrow F_\xi(x), \forall x_0 \Rightarrow F_\xi(x_0)$ непрерывна.

\mydef{} Последовательность случайных величин $\{ \xi_n \}$ сходится к случайной величине $\xi$ {\it слабо}: $\xi_n \Rightarrow \xi$, если для любой непрерывной ограниченной функции $\phi$ верно:
$$\lim _{n \rightarrow \infty} \mathbf{E} \varphi\left(\xi_{n}\right)=\mathbf{E} \varphi(\xi).$$

\section{Неравенства Маркова, Чебышева и Гаусса. Правило «трех сигм». Закон больших чисел в форме Чебышева.}

\myth{} \textbf{(неравенство Маркова)} Если $\mathrm{E}|\xi| < \infty$, то для любого $x > 0$
$$\myprob{|\xi| \geqslant x} \leqslant \frac{\mathrm{E}|\xi|}{x}.$$

\mydef{}  Назовём индикатором события $A$ случайную величину $I(A)$, равную единице, если событие $A$ произошло, и нулю, если $A$ не произошло.

\myqed{} По определению, величина $I(A)$ имеет распределение Бернулли с параметром $p = \myprob{I(A) = 1} = \myprob{A}$ и её математическое ожидание равно вероятности успеха $p = \myprob{A}.$ Индикаторы прямого и противоположного событий связаны равенством $I(A) + I(\overline{A}) = 1.$ Поэтому
$$|\xi|=|\xi| \cdot I(|\xi|<x)+|\xi| \cdot I(|\xi| \geqslant x) \geqslant|\xi| \cdot I(|\xi| \geqslant x) \geqslant x \cdot I(|\xi| \geqslant x).$$
Тогда $\mathrm{E}|\xi| \geqslant \mathrm{E}(x \cdot I(|\xi| \geqslant x))=x \cdot \mathrm{P}(|\xi| \geqslant x)$. Осталось разделить обе части этого неравенства на положительное число $x$. $~~~\square$

\mycon{} \textbf{(обобщённое неравенство Чебышёва)}. Пусть функция $g$ не убывает и неотрицательна на $\mathbb{R}.$ Если $\mathrm{E}g(\xi) < \infty$, то для любого $x \in \mathbb{R}$
$$P(\xi \geqslant x) \leqslant \frac{E g(\xi)}{g(x)}.$$

\myqed{} Заметим, что $\myprob{\xi \geqslant x} \leqslant \myprob{g(\xi) \geqslant g(x)}$, поскольку функция $g$ не убывает. Оценим последнюю вероятность по неравенству Маркова, которое можно применять в силу неотрицательности $g$:
$$\mathrm{P}(g(\xi) \geqslant g(x)) \leqslant \frac{\mathrm{E} g(\xi)}{g(x)}. ~~~ \square$$

\mycon{} \textbf{(неравенство Чебышёва)} Если $\mathrm{D}\xi$ существует, то для любого $\varepsilon > 0$
$$\mathrm{P}(|\xi-\mathrm{E} \xi| \geqslant \varepsilon) \leqslant \frac{\mathrm{D} \xi}{x^{2}}.$$

\myqed{} Для $x > 0$ неравенство $|\xi - \mathrm{E}\xi| \geqslant x$ равносильно неравенству $(\xi - \mathrm{E}\xi)^2 \geqslant x^2$, поэтому
$$\mathrm{P}(|\xi-\mathrm{E} \xi| \geqslant x)=\mathrm{P}\left((\xi-\mathrm{E} \xi)^{2} \geqslant x^{2}\right) \leqslant \frac{\mathrm{E}(\xi-\mathrm{E} \xi)^{2}}{x^{2}}=\frac{\mathrm{D} \xi}{x^{2}}. ~~~\square$$

\myth{} \textbf{(неравенство Гаусса)} (без доказательства) Пусть $X$ - случайная величина с модой $m$ и пусть $\tau^2$ - математическое ожидание $(X - m)^2.$ Тогда верно следующее:

$$\mathbb{P}(|X-m|>k) \leq\left\{\begin{array}{ll}
\left(\frac{2 \tau}{3 k}\right)^{2}, & \text { если } k \geq \frac{2 \tau}{\sqrt{3}} \\
1-\frac{k}{\tau \sqrt{3}}, & \text { если } 0 \leq k \leq \frac{2 \tau}{\sqrt{3}}
\end{array}\right.$$

В неравенстве Чебышева в качестве $\varepsilon$ можно брать любое положительное число. Если взять в качестве $\varepsilon$ величину $3\sigma$, где $\sigma$ — стандартное отклонение (то есть именно корень из дисперсии), то получится
$$\mathrm{P}(|X-\mathrm{E} X|>3 \sigma) \leqslant \frac{\mathrm{D} X}{9 \mathrm{D} X}=\frac{1}{9}$$
или то же самое $$\mathrm{P}(|X-\mathrm{E} X| \leqslant 3 \sigma) \geqslant 1-\frac{1}{9}=\frac{8}{9}.$$ 
Это соотношение и зовётся {\it правилом трёх сигм}.

\mydef{} Говорят, что последовательность случайных величин $\xi_1, \xi_2, ...$ с конечными первыми моментами {\it удовлетворяет закону Больших чисел (ЗБЧ)}, если
$$\frac{\xi_{1}+\ldots+\xi_{n}}{n}-\frac{\mathrm{E} \xi_{1}+\ldots+\mathrm{E} \xi_{n}}{n} \stackrel{\mathrm{p}}{\longrightarrow} 0 \text { при } n \rightarrow \infty.$$

\myth{} \textbf{(ЗБЧ Чебышёва)} Для любой последовательности $\xi_1, \xi_2, ...$ попарно независимых и одинаково распределённых случайных величин с конечным вторым моментом $\mathrm{E}\xi_1^2 < \infty$ имеет место сходимость
$$\frac{\xi_{1}+\ldots+\xi_{n}}{n} \stackrel{\mathrm{p}}{\longrightarrow} \mathrm{E} \xi_{1}.$$

\myqed{} Обозначим через $S_n = \xi_1 + ... + \xi_n$ сумму первых $n$ случайных величин. Из линейности матожидания получим
$$\mathrm{E}\left(\frac{S_{n}}{n}\right)=\frac{\mathrm{E} \xi_{1}+\ldots+\mathrm{E} \xi_{n}}{n}=\frac{n \mathrm{E} \xi_{1}}{n}=\mathrm{E} \xi_{1}.$$
Пусть $\varepsilon > 0.$ Воспользуемся неравенством Чебышёва:
$$\mathrm{P}\left(\left|\frac{S_{n}}{n}-\mathrm{E}\left(\frac{S_{n}}{n}\right)\right| \geqslant \varepsilon\right) \leqslant \frac{\mathrm{D}\left(\frac{S_{n}}{n}\right)}{\varepsilon^{2}}=\frac{\mathrm{D} S_{n}}{n^{2} \varepsilon^{2}}=\frac{\mathrm{D} \xi_{1}+\ldots+\mathrm{D} \xi_{n}}{n^{2} \varepsilon^{2}}=$$$$=\frac{n \mathrm{D} \xi_{1}}{n^{2} \varepsilon^{2}}=\frac{\mathrm{D} \xi_{1}}{n \varepsilon^{2}} \rightarrow 0 \text { при } n \rightarrow \infty,$$
так как $\mathrm{D}\xi_1 < \infty$. Дисперсия суммы превратилась в сумму дисперсий в силу попарной независимости слагаемых, из-за которой все ковариации $\operatorname{cov}(\xi_i, \xi_j)$ по свойству ковариации обратились в нуль при $i \neq j. ~~~ \square$

\section{Характеристические функции и их свойства.}

\mydef{} Пусть $\xi$ - случайная величина. {\it Характеристической функцией случайной величины} $\xi$ называется функция, определённая $\forall t \in \mathbb{R}$ как
$$\varphi_{\xi}(t)=\mathbf{E} e^{i t \xi}=\mathbf{E} \cos (t \xi)+i \cdot \mathbf{E} \sin (t \xi).$$

Свойства:
\begin{enumerate}
    \item $\varphi_\xi(0) = 1, |\varphi_\xi(1)| \leqslant 1~\forall t$, следовательно, для любой случайно величины $\xi$ и для любого $t$ функция $\varphi_\xi(t)$ определена.
    \item $\varphi_\xi(t)$ равномерно непрерывна на всей числовой оси:
    $$\left|\varphi_{\xi}(t+\Delta)-\varphi_{\xi}(t)\right|=\left|\mathbf{E} e^{i(t+\Delta) \xi}-\mathbf{E} e^{i t \xi}\right| \leq \mathbf{E}\left|e^{i \Delta \xi}-1\right|.$$
    \item Следующие утверждения эквивалентны:
    \begin{enumerate}
        \item $\varphi_\xi(t)$ принимает лишь действительные значения
        \item $\varphi_\xi(t)$ - чётная функция, то есть $\varphi_\xi(-t) = \varphi_\xi(t).$
        \item $\xi$ имеет симметричное распределение (то есть $\xi$ и $-\xi$ одинаково распределены):
        $$\forall x~F_{\xi}(x)=\mathbf{P}(\xi<x)=\mathbf{P}(-\xi<x)=\mathbf{P}(\xi>-x)=1-\mathbf{P}(\xi \leq-x)=1-F_{\xi}(-x+0)$$
        
        \myqed{} Действительно, $\varphi_{\xi}(t)=\mathbf{E} e^{i t \xi}, \quad \varphi_{\xi}(-t)=\overline{\varphi_{\xi}(t)} \Leftarrow e^{i(-t) \xi}=\overline{e^{i t \xi}}$, что устанавливает эквивалетность первых двух утверждений. Далее, имеем $\varphi_{\xi}(-t)=\mathbf{E} e^{i(-t) \xi}=\mathbf{E} e^{i t(-\xi)}=\varphi_{-\xi}(t)$, а так как $$\varphi(t)=\int e^{i t \xi} d F_{\xi}(x),$$ утверждение доказано. $~~~\square$
    \end{enumerate}
    
    \item \myth{} \textbf{(Бохнера-Хинчина)} Функция $\varphi_\xi(t)$ является характеристической функцией случайной величины $\xi$ тогда и только тогда, когда $\varphi_\xi(0) = 1$, $\varphi_\xi(t)$ положительно определена, то есть $$\forall n, \forall t_{1}, \ldots, t_{n} \in \mathbf{R}, \forall c_{1}, \ldots, c_{n} \in \mathbf{C} \text { выполняется } \sum_{j=1}^{n} \sum_{k=1}^{n} \varphi_{\xi}\left(t_{j}-t_{k}\right) \cdot c_{j} \overline{c_{k}} \geq 0.$$
    
    \item $\varphi_{\xi}(t) \equiv \varphi_{\eta}(t)$, тогда и только тогда, когда $F_\xi(x) \equiv F_\eta(x).$
    \item Если $\xi$ абсолютно непрерывна с плотностью $p_\xi(x)$, то
    $$\varphi_{\xi}(t)=\int_{-\infty}^{+\infty} e^{i t x} p_{\xi}(x) d x, p_{\xi}(x)=\frac{1}{2 \pi} \int_{-\infty}^{+\infty} e^{-t x} \varphi_{\xi}(t) d t$$
    \item Пусть у случайной величины $\xi$ существует момент $n$-го порядка $E \xi^{n}<\infty$. Тогда
    $$\exists \frac{d^{k} \varphi(t)}{d t^{k}}=\varphi^{(k)}(t), \forall k=1, \ldots, n \text { и } \varphi^{(k)}(0)=i^{k} \mathbf{E} \xi.$$
    
    Так, например, если существует $\varphi^{(2k)}(t)|_0$, то существует $\mathbf{E} \xi^{2 k},$ и $\varphi^{(2 k)}(0)=(-1)^{k} \cdot \mathbf{E} \xi^{2 k}$.
\end{enumerate}

\section{Закон больших чисел в форме Хинчина}

\section{Центральная предельная теорема}
\begin{thm}[Центральная предельная теорема]
    Пусть $\xi_{1}, \xi_{2}, \ldots$~--- последовательность независимых одинаково распределенных (невырожденных) случайных величин с $\mathrm{E} X_{1}^{2}<\infty$ и $S_{n}=X_{1}+\ldots+X_{n}$. Тогда
    \begin{equation*}
        \mathbb{P}\left(\frac{S_{n}-\mathrm{E} S_{n}}{\sqrt{\mathrm{D} S_{n}}} \leqslant x\right)
        \xrightarrow[n \rightarrow \infty]{}
        \Phi(x) = \frac{1}{\sqrt{2 \pi}} \int\limits_{-\infty}^{x} e^{-\frac{z^{2}}{2}} dz~ \forall x \in \mathbb{R}
    \end{equation*}
\end{thm}
\begin{proof}
Пусть $\mathrm{E} X_{1}=m, \mathrm{D} X_{1}=\sigma^{2}$ и $\varphi(t)=\mathrm{E} e^{i t\left(X_{1}-m\right)}$. Введём
\begin{equation*}
    \varphi_{n}(t)=\mathrm{E} e^{i t \frac{S_{n}-\mathrm{E} S_{n}}{\sqrt{D S_{n}}}} = \
    \left[\varphi\left(\frac{t}{\sigma \sqrt{n}}\right)\right]^{n}
\end{equation*}

В силу разложения характеристической функции
\begin{equation*}
    \varphi_{X}(t)=1+i t \mathrm{E} X+\ldots+\frac{(i t)^{n}}{n !} \mathrm{E} X^{n}+R_{n}(t)
\end{equation*}
при $n=2$ получим 
\begin{equation*}
    \varphi(t)=1-\frac{\sigma^{2} t^{2}}{2}+\bar{o}\left(t^{2}\right), \quad t \rightarrow 0
\end{equation*}
Следовательно, для любого $t \in \mathbb{R}$ при $n \rightarrow \infty$
\begin{equation*}
    \varphi_{n}(t)=\left[1-\frac{\sigma^{2} t^{2}}{2 \sigma^{2} n}+\bar{o}\left(\frac{1}{n}\right)\right] \rightarrow e^{-\frac{t^{2}}{2}}
\end{equation*}

Функция $e^{-\frac{t^{2}}{2}}$ является характеристической функцией стандартного нормальногораспределения. В силу теорем о непрерывном соответсвии между функциями распределения и характеристическими функциями центральная предельная теорема доказана.
\end{proof}

\section{Условное математическое ожидание.}

\mydef{} Расстоянием от точки $y$ до множества $A$ называется по определению {\it проекция точки $y$ на множество $A$}: $\min _{x \in A} \rho(x, y)$.

На вероятностном пространстве $(\Omega, \mathcal{F}, \mathbb{P})$ рассмотрим вероятностное пространство, порождённое случайной величиной $\xi: (\Omega, \mathcal{F}_\xi, \mathbb{P})$, $\mathcal{F}_\xi = $ $\left\{\xi^{-1}(\omega), B \in \mathcal{B}\right\} \subset \mathcal{F}$ - минимальная $\sigma$-алгебра, в которой $\xi$ измерима. Зафиксируем $\mathcal{F}_\xi.$ Множество величин разбивается на две части: измеримых в $\mathcal{F}_\xi$ и неизмеримых в $\mathcal{F}_\xi$. Рассмотрим множество случайных величин, измеримых относительно $\mathcal{F}_\xi = \mathcal{F}_1$.

\mydef{} Две случайные величины $\xi$ и $\eta$ называются {\it эквивалентными}, если $\myprob{\xi \neq \eta} = 0.$

\mydef{} {\it Расстоянием между} $\xi$ и $\eta$ называется $\mathbf{E}(\xi-\eta)^{2}$.

Напомним определение условной вероятности: $$\mathbf{P}(A | B)=\frac{\mathrm{P}(A B)}{\mathbf{P}(B)}=\mathbf{P}_{B}(A), \mathbf{P}(B)>0.$$

\mydef{} Пусть есть случайная величина $\xi$, $\mathbf{E} \xi<\infty \Leftrightarrow \int_{\Omega} \xi(\omega) \mathbf{P}(d \omega)<\infty.$ Рассмотрим интеграл от той же функции относительно меры $\mathbf{P}_{B}(A)$ (относительно события $B$). {\it Условное математическоке ожидание} случайной величины $\xi$ относительно события $B$, имеющего ненулевую вероятность, определяется как интеграл
$$\mathbf{E}(\xi | B)=\int_{\Omega} \xi(\omega) \mathbf{P}_{B}(d \omega)=\int_{B} \xi(\omega) \frac{\mathbf{P}(d \omega)}{\mathbf{P}(B)}.$$

Последнее равенство следует из того, что $\mathbf{P}_{B}(\omega) = 0$, если $\omega \notin B.$ Отсюда следует, что $$\mathbf{P}(B) \mathbf{E}(\xi | B)=\int_{B} \xi(\omega) \mathbf{P}(d \omega).$$

Рассмотрим счётное разбиение $\hat{\mathcal{F}} = (B_1, B_2, ...)$ множества $\Omega: \bigcup_{i=1}^{\infty} B_{i}=\Omega, B_{i} B_{j}=\varnothing(i \neq j), \myprob{B_i} > 0$ и рассмотрим случайную величину $\mathbf{E}(\xi | \hat{\mathcal{F}}) = \mathbf{E}\left(\xi | B_{i}\right), \omega \in B_{i}$. $\mathcal{F}_1$ - минимальная $\sigma$-алгебра, порождённая разбиением $\hat{\mathcal{F}}=\left(B_{1}, B_{2}, \ldots\right): \quad \mathcal{F}_{1}=\sigma(\hat{\mathcal{F}}).$ Если $A \in \sigma(\hat{\mathcal{F}})$, то $\exists B_{j_{k}} \in \hat{\mathcal{F}}: A=\bigcup_{k} B_{j_{k}}$. Также если $A \in \sigma(\hat{\mathcal{F}})$, то для любого $A$, входящего в минимальную $\sigma$-алгебру, порождённую разбиением выполняется
$$\int_{A} \mathbf{E}(\xi | \hat{\mathcal{F}}) \mathbf{P}(d \omega)=\int_{A} \xi(\omega) \mathbf{P}(d \omega)$$

\mydef{} Пусть имеется $(\Omega, \mathcal{F}, \mathbb{P})$, $\xi$ - случайная величина на этом вероятностном пространстве, $\mathbf{E} \xi<\infty, \mathcal{F}_{1} \subset \mathcal{F}, \mathcal{F}_{1}$ - $\sigma$-алгебра. {\it Условным математическим ожиданием случайной величины $\xi$ относительно $\sigma$-алгебры $\mathcal{F}_1$} называется случайная величина, которая удовлетворяет следующим двум условиям:
\begin{enumerate}
    \item $\mathbf{E}\left(\xi | \mathcal{F}_{1}\right)$ измерима относительно $\mathcal{F}_1$. (В случае конечного разбиения она будет кусочно-постоянной и, следовательно, измеримой)
    \item $\forall A \in \mathcal{F}_1$ выполняется:
    $$\int_{A} \mathbf{E}\left(\xi | \mathcal{F}_{1}\right)(d \omega)=\int_{A} \xi(\omega) \mathbf{P}(d \omega).$$
\end{enumerate}

Пусть $\xi \geqslant 0.$ Обозначим $\nu(A) =\int_{A} \xi(\omega) \mathbf{P}(d \omega)$. Если потребовать $A \in \mathcal{F}_1$, то $\nu$ будет мерой на $\mathcal{F}_1$. Из свойств интеграла Лебега следует, что $\nu$ абсолютно непрерывна относительно меры $\mathbb{P}$. В силу теоремы Радона-Никодима существует и почти всюду единственна измеримая относительно $\mathcal{F}_1$ (по мере $\mu$) функция $\mathbf{E}\left(\xi | A_{1}\right)$ такая, что $\nu(A)=\int_{A} \mathbf{E}\left(\xi | \mathcal{F}_{1}\right) \mu(d \omega)$.

\mydef{} Пусть $\xi$ и $\eta$ - случайные величины, $\mathbf{E} \xi<\infty$. Тогда {\it условным математическим ожиданием случайной величины $\xi$ относительно случайной величины $\eta$} назовём 
$$\mathbf{E}(\xi | \eta)=\mathbf{E}(\xi | \sigma(\eta)), \sigma(\eta)=\left(\eta^{-1}(B), B \in \mathcal{B}\right).$$

\mydef{} {\it Условным математическим ожиданием события $A$ относительно $\sigma$-алгебры $\mathcal{F}_1$} назовём $\mathbf{P}\left(A | \mathcal{F}_{1}\right)=\mathbf{E}\left(\mathbf{I}(A) | \mathcal{F}_{1}\right).$

Свойства:

\begin{enumerate}
    \item $\xi \geq 0 \Rightarrow \mathbf{E}\left(\xi | \mathcal{F}_{1}\right) \geq 0.$
    \item $\xi$ измерима относительно $\mathcal{F}_1 \Rightarrow \mathbf{E}\left(\xi | A_{1}\right)=\xi$. Это следует из единственности функции в теореме Радона-Никодима (матожидание константы равно константе).
    \item $\mathbf{E}\left(\mathbf{E}\left(\xi | \mathcal{F}_{1}\right)\right)=\mathbf{E}(\xi)$.
    \item Линейность: $\forall a, b \in \mathbf{R}, \forall$ случайных величин $\xi, \eta: \mathbf{E} \xi<\infty, \mathbf{E} \eta<\infty$ верно
    $$\mathbf{E}\left(a \xi+b \eta | \mathcal{F}_{1}\right)=a \cdot \mathbf{E}\left(\xi | \mathcal{F}_{1}\right)+b \cdot \mathbf{E}\left(\eta | \mathcal{F}_{1}\right)$$
    \item Если $\xi$ и $\eta$ независимы, причём $\mathbf{E} \xi<\infty,$ то $\mathbf{E}(\xi | \eta)=\mathbf{E}(\xi)$.
    \item Если $\xi$ и $\eta$ - случайные величины, причём $\mathbf{E} \varphi(\xi, \eta)<\infty$ и $\xi$ измерима относительно $\mathcal{F}_1, \varphi(\xi,\eta)$ - случайная величина, зависящая от $\xi$ и $\eta$. Тогда
    $$\mathbf{E}\left(\varphi(\xi, \eta) | \mathcal{F}_{1}\right)=\left. \mathbf{E}\left(\varphi(u, \eta) | \mathcal{F}_{1}\right)\right|_{u=\xi}.$$
    \item Если $\mathcal{F}_1 \subset \mathcal{F}_2 \subset \mathcal{F}$, $\xi$ - случайная величина, $\mathbf{E} \xi<\infty$, то существует такая измеримая функция $\varphi$, что $\mathbf{E}(\xi|\eta) = \varphi(\eta)$.
\end{enumerate}

\subsubsection{Вычисление условного матожидания}

Если $\mathbf{E}(\xi|\mathcal{F}_1)$ принимает не более счётного числа значений, то оно может быть вычислено по формуле
$$\frac{\sum_{i} \int_{B_{i}} \xi(\omega) \mathbf{P}(d \omega)}{\mathbf{P}(B)}.$$

Рассмотрим $\mathbf{E}(\xi|\eta)$, если $(\xi,\eta)$ - абсолютно непрерывный случайный вектор с совместной плотностью $p(u,v)$ распределения $\xi$ и $\eta$. Тогда $p_{\xi}(u)=\int_{-\infty}^{+\infty} p(u, v) d v, p_{\eta}(v)=\int_{-\infty}^{+\infty} p(u, v) d u$.

\mydef{} {\it Условной плотностью распределения $\xi$ при условии $\eta = v$} называется $p_{\eta}(u, v)=\frac{p(u, v)}{p_{\eta}(v)}$.

Справедлива формула
$$\mathbf{E}(\xi | \eta)=\int_{-\infty}^{+\infty} u p_{\eta}(u, \eta) d u=\int_{-\infty}^{+\infty} \frac{u p(u, \eta)}{p_{\eta}(\eta)} d u.$$

\chapter{Математическая статистика}

\section{Статистическая структура. Выборка. Статистика. Порядковые статистики. Вариационный ряд. Эмпирическая функция распределения}

\begin{defn}
{\it Статистическая структура}~--- совокупность $(\Omega, \mathcal{A}, \mathcal{F})$, где $\Omega$~---множество элементарных исходов, $\mathcal{A}$~--- $\sigma$-алгебра событий, $\mathcal{F}$~--- семейство вероятностных мер, определённых на $\mathcal{A}$, параметризованное одно- или многомерным числовым параметром: $\mathcal{F} = (\mathcal{F}_{\theta}~|~\theta \in \Theta \subset R^{m})$.
\end{defn}

\begin{defn}
{\it Выборка} $\mathbf{X} = (X_{1}, \ldots, X_{n})$ объёма $n$~--- набор из $n$ независимых и одинаково распределённых случайных величин, имеющих такое же распределение, как и наблюдаемая случайная величина $\xi$.
\end{defn}

До того, как эксперимент проведён, выборка~--- набор случайных величин, после~--- набор чисел из множества возможных значений случайной величины. Числовой набор $\mathbf{X}(\omega_0) = (X_{1}(\omega_0), \ldots, X_{n}(\omega_0)) = (x_1, \ldots, x_n)$~--- {\it реализация выборки} на элементарном исходе $\omega_0$.

\begin{defn}
{\it Статистика}~--- измеримая функция от выборки $T(\mathbf{X})$.
\end{defn}

\begin{defn}
{\it Вариационный ряд}~--- выборка $X_{1}, \ldots, X_{n}$, упорядоченная по возрастанию на каждом элементарном исходе.
\end{defn}
Вариационный ряд строится следующим образом:
\begin{gather*}
    X_{(1)}(\omega)=\min (X_{1}(\omega), \ldots, X_{n}(\omega)) \\
    X_{(k)}(\omega)=\{\forall \omega \in \Omega \Rightarrow \exists m \leq i_{1}, \ldots, i_{k-1}, i_{k}, i_{k+1}, \ldots, i_{n} \leq n, i_{j} \neq i_{m}(j \neq m): \\ 
    X_{(k)}(\omega)=X_{i_{k}}(\omega) \\
    X_{i_{1}}(\omega), \ldots, X_{i_{k-1}}(\omega) \leq X_{i_{k}}(\omega); X_{i_{k+1}}(\omega), \ldots, X_{i_{n}}(\omega)>X_{i_{k}}(\omega)\}, 2 \leq k \leq n-1 \\
    X_{(n)}(\omega)=\max (X_{1}(\omega), \ldots, X_{n}(\omega))
\end{gather*}

Элемент $X_{(k)}$~--- {\it $k$-я порядковая статистика}.

\begin{defn}
{\it Эмпирическая функция распределения}, построенная по выборке $X_{1}, \ldots, X_{n}$ объёма $n$~--- случайная функция $F_{n}^{*}: \mathbb{R} \times \Omega \rightarrow[0,1]$, при каждом $y \in \mathbb{R}$ равная:
$F_{n}^{*}(y) =\frac{1}{n} \sum_{i=1}^{n} \mathrm{I}\left(X_{i}<y\right)$
\end{defn}

Эмпирическая функция распределенния строится по вариационному ряду следующим образом:

\begin{equation*}
    F_{n}^{*}(y)=\left\{\begin{array}{ll}
    0, & \text { если } y \leqslant X_{(1)} \\
    \frac{k}{n}, & \text { если } X_{(k)}<y \leqslant X_{(k+1)} \\
    1 & \text { при } y>X_{(n)}
    \end{array}\right.
\end{equation*}

\begin{exmp}
Найдём эмпирические функции распределения для крайних порядковых статистик.

\begin{gather*}
    \begin{aligned}
        F_{(1)}(x)=\mathbb{P}(X_{(1)} < x) 
    = 1 - \mathbb{P} (\mathrm{X}_{(1)} \geq x) 
    = 1 - \mathbb{P}(x_{1} \geq x, \ldots, x_{n} \geq x) = \\
    = 1 - \prod_{i=1}^{n} \mathbb{P}(x_{i} \geq x) 
    = 1 - (\mathbb{P}({x}_{1} \geq x))^{n} 
    = 1 - (1 - F(x))^{n} 
    \end{aligned} \\
    \begin{aligned}
        F_{(n)}(x) 
        = \mathbb{P}(X_{(n)} < x) 
        = \mathbb{P}(x_{1} < x, \ldots, x_{n} < x) = \\
        = \prod_{i=1}^{n} \mathbb{P}(x_{i} < x) 
        = (\mathbb{P}({x}_{1} < x))^{n} 
        = F^{n}(x)
    \end{aligned}
\end{gather*}
\end{exmp}

\begin{thm} Свойства эмпирической функции распределения:
\begin{enumerate}
    \item Пусть $X_{1}, \ldots, X_{n}$~--- выборка из распределения $\mathcal{F}$ с функцией распределения $F$ и пусть $F_{n}^{*}$ — эмпирическая функция распределения, построенная по этой выборке. Тогда $F_{n}^{*}(y) \xrightarrow[n \to \infty]{\mathrm{p}} F(y)$ для любого $y \in \mathbb{R}.$
    \item Для любого y $\in \mathbb{R}$:
    \begin{enumerate}[label={\arabic*)}]
        \item $\mathbb{E} F_{n}^{*}(y)=F(y)$, т.е. $F_{n}^{*}(y)$~--- несмещённая оценка для $F(y)$.
        \item $\mathbb{D} F_{n}^{*}(y)=\cfrac{F(y)(1-F(y))}{n}$
        \item $\sqrt{n}(F_{n}^{*}(y)-F(y)) \Rightarrow \mathbf{N}(0, (1-F(y))F(y))$, т.е. $F_{n}^{*}(y)$~--- асимптотически нормальная оценка для $F(y)$.
        \item $n F_{n}^{*}(y) \sim \mathbf{B}(n, F(y))$
    \end{enumerate}
\end{enumerate}
\end{thm}
\pagebreak
\begin{proof}\leavevmode
\begin{enumerate}
    \item $F_{n}^{*}(y)=\frac{1}{n} \sum_{i=1}^{n} \mathrm{I}(X_{i}<y)$, при этом случайные величины $\mathrm{I}(X_{1}<y),~ \mathrm{I}(X_{2}<y), \ldots$ независимы и одинаково распределены, их математическое ожидание конечно:
    \begin{equation*}
        \mathbb{E}\mathrm{I}(X_{1}<y)=1 \cdot \mathrm{P}(X_{1}<y)+0 \cdot \mathrm{P}(X_{1} \geqslant y)=\mathrm{P}(X_{1}<y)=F(y)<\infty
    \end{equation*}
    Следовательно, применим ЗБЧ в форме Хинчина:
    \begin{equation*}
        F_{n}^{*}(y)=\cfrac{\sum_{i=1}^{n} \mathrm{I}(X_{i}<y)}{n} \xrightarrow[]{\mathrm{p}} \mathbb{E}\mathrm{I}(X_{1}<y)=F(y) 
    \end{equation*}
    \item Заметим:
    \begin{gather*}
        \mathrm{I}(X_{1}<y) \sim  \mathbf{Bi}(F(y)) \Rightarrow \mathbb{E}\mathrm{I}(X_{1}<y) = F(y) \\
        \mathbb{D}\mathrm{I}(X_{1}<y) = F(y)(1-F(y))
    \end{gather*}
    \begin{enumerate}[label={\arabic*)}]
        \item Случайные величины $\mathrm{I}(X_{i}<y)$ одинаково распределены, поэтому:
        \begin{equation*}
            \mathbb{E} F_{n}^{*}(y)=\mathbb{E} \cfrac{\sum_{i=1}^{n} \mathrm{I}(X_{i}<y)}{n}=\cfrac{\sum_{i=1}^{n} \mathbb{E}\mathrm{I}(X_{i}<y)}{n}=\cfrac{n \mathbb{E}\mathrm{I}(X_{1}<y)}{n}=F(y)  
        \end{equation*}
        
        \item Случайные величины $\mathrm{I}(X_{i}<y)$ независимы и одинаково распределены, поэтому:
        \begin{multline*}
            \mathbb{D}\mathrm{I}_{n}^{*}(y)
            = \mathbb{D} \cfrac{\sum_{i=1}^{n} \mathrm{I}(X_{i}<y)}{n}
            = \cfrac{\sum_{i=1}^{n} \mathbb{D}\mathrm{I}(X_{i}<y)}{n^{2}}
            = \\
            = \cfrac{n\mathbb{D}\mathrm{I}(X_{1}<y)}{n^{2}}
            = \cfrac{F(y)(1-F(y))}{n}
        \end{multline*}
    
        \item Применим ЦПТ:
        \begin{multline*}
            \sqrt{n}\left(F_{n}^{*}(y)-F(y)\right)
            = \sqrt{n}\left(\cfrac{\sum \mathrm{I}(X_{i}<y)}{n}-F(y)\right) 
            = \\
            = \cfrac{\sum_{i=1}^{n} \mathrm{I}(X_{i}<y)-n F(y)}{\sqrt{n}} 
            = \cfrac{\sum_{i=1}^{n} \mathrm{I}(X_{i}<y)-n \mathbb{E}\mathrm{I}(X_{1}<y)}{\sqrt{n}} 
            \Rightarrow \\
            \Rightarrow \mathbf{N}(0, \mathbb{D}\mathrm{I}(X_{1}<y))
            = \mathbf{N}(0, (1-F(y))F(y))
        \end{multline*}
        \item Следует из устойчивости по суммированию биномиального распределения. Поскольку $\mathrm{I}\left(X_{i}<y\right)$ независимы и имеют распределение Бернулли $\mathbf{Bi}(F(y))$, то их сумма
        \begin{equation*}
            n F_{n}^{*}(y)=\mathrm{I}\left(X_{1}<y\right)+\ldots+\mathrm{I}\left(X_{n}<y\right)
        \end{equation*}
        имеет биномиальное распределение $\mathbf{B}(n, F(y))$.
    \end{enumerate}
\end{enumerate}  
\end{proof}

\section{Выборочные моменты. Их свойства}

Рассмотрим случайную величину $\xi^{*}$ с эмпирическим распределением, введём для последнего числовые характеристики.

\begin{defn}
{\it Выборочное математическое ожидание:} 
\begin{equation*}
    \tilde{\mathbb{E}} \xi^{*}=\sum_{i=1}^{n} \frac{1}{n} X_{i}=\frac{1}{n} \sum_{i=1}^{n} X_{i}=\overline{X}
\end{equation*}

Выборочное матожидание функции $g(\xi^{*})$:
\begin{equation*}
    \tilde{\mathbb{E}} g\left(\xi^{*}\right)=\frac{1}{n} \sum_{i=1}^{n} g\left(X_{i}\right)=\overline{g(\mathbf{X})}
\end{equation*}
\end{defn}

\begin{defn}
{\it Выборочная дисперсия:}
\begin{equation*}
    \tilde{\mathbb{D}} \xi^{*}=\sum_{i=1}^{n} \frac{1}{n}(X_{i}-\tilde{\mathbb{E}} \xi^{*})^{2}=\frac{1}{n} \sum_{i=1}^{n}(X_{i}-\overline{X})^{2}=S^{2}
\end{equation*}
\end{defn}

\begin{defn}
{\it Несмещённая выборочная дисперсия:} 
\begin{equation*}
    S_{0}^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}
\end{equation*}
\end{defn}

\begin{defn}
{\it Выборочный момент $k$-го порядка:}
\begin{equation*}
    \tilde{\mathbb{E}}(\xi^{*})^{k}=\sum_{i=1}^{n} \frac{1}{n} X_{i}^{k}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{k}=\overline{X^{k}}
\end{equation*}
\end{defn}

Все вышеперечисленные характеристики являются случайными величинами как функции от выборки $X_{1}, \ldots, X_{n}$ и оценками для истинных моментов искомого распределения.

\begin{thm}
Выборочное среднее $\overline{X}$ является несмещённой, состоятельной и асимптотически нормальной оценкой для теоретического среднего (математического ожидания):

\begin{enumerate}[label={\arabic*.}]
    \item Если $\mathbb{E}|X_{1}|<\infty$, то $\mathbb{E}\overline{X}=\mathbb{E} X_{1}=a$
    \item Если $\mathbb{E}|X_{1}|<\infty$, то $\overline{X} \xrightarrow[]{\mathrm{p}} \mathbb{E} X_{1}=a$ при $n \rightarrow \infty$.
    \item Если $\mathbb{D} X_{1}<\infty, \quad \mathbb{D} X_{1} \neq 0$, то $\sqrt{n}(\overline{X}-\mathbb{E} X_{1}) \Rightarrow \mathbf{N}(0, \mathbb{D} X_{1})$.
\end{enumerate}
\end{thm}

\begin{proof}
\begin{enumerate}[label={\arabic*.}]
    \item $\mathbb{E} \overline{X}=\frac{1}{n}(\mathbb{E} X_{1}+\ldots+\mathbb{E} X_{n})=\frac{1}{n} \cdot n \mathbb{E} X_{1}=\mathbb{E} X_{1}=a$
    \item Из ЗБЧ в форме Хинчина:
    \begin{equation*}
        \overline{X}
        = \cfrac{X_{1}+\ldots+X_{n}}{n} \xrightarrow[]{\mathrm{p}} \mathbb{E} X_{1} 
        = a
    \end{equation*}

    \item Из ЦПТ:
    \begin{equation*}
        \sqrt{n}\left(\overline{X}-\mathbb{E} X_{1}\right) 
        = \cfrac{\sum_{i=1}^{n} X_{i}-n \mathbb{E} X_{1}}{\sqrt{n}} 
        \Rightarrow \mathbf{N}(0, \mathbb{D} X_{1})
    \end{equation*}
\end{enumerate}
\end{proof}

\begin{rmrk}
    Аналогичными свойствами обладает выборочный $k$-й момент, являющийся несмещённой, состоятельной и асимптотически нормальной оценкой для теоретического $k$-го момента.
\end{rmrk}

\begin{thm}
Пусть $\mathbb{D} X_{1}<\infty$.
\begin{enumerate}
    \item Выборочные дисперсии $S^{2}$ и $S^{2}_0$ являются состоятельными оценками для истинной дисперсии:
    \begin{equation*}
        S^{2} \xrightarrow[]{\mathrm{p}} \mathbb{D} X_{1}=\sigma^{2}, \quad S_{0}^{2} \xrightarrow[]{\mathrm{p}} \mathbb{D} X_{1}=\sigma^{2}
    \end{equation*}
    \item Величина $S^{2}$~--- смещённая оценка дисперсии, а $S^{2}_0$~--— несмещённая:
    \begin{equation*}
        \mathbb{E} S^{2}=\frac{n-1}{n} \mathbb{D} X_{1}=\frac{n-1}{n} \sigma^{2} \neq \sigma^{2}, \quad \mathbb{E} S_{0}^{2}=\mathbb{D} X_{1}=\sigma^{2}
    \end{equation*}
    
    \item Если $0 \neq \mathbb{D}(X_{1}-\mathbb{E}X_{1})^{2}<\infty$, то $S^{2}$ и $S^{2}_0$ являются асимптотически нормальными оценками истинной дисперсии:
    \begin{equation*}
        \sqrt{n}\left(S^{2}-\mathbb{D} X_{1}\right) \Rightarrow \mathbf{N}(0, \mathbb{D}(X_{1}-\mathbb{E} X_{1})^{2})
    \end{equation*}
\end{enumerate}
\end{thm}

\begin{proof}
\begin{enumerate}
    \item $S^{2}=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}=\overline{X^{2}}-(\overline{X})^{2}$

    Используя состоятельность первого и второго выборочных моментов и свойства сходимости по вероятности, получаем:
    \begin{gather*}
        S^{2}=\overline{X^{2}}-(\overline{X})^{2} \xrightarrow[]{\mathrm{p}} \mathbb{E} X_{1}^{2}-\left(\mathbb{E} X_{1}\right)^{2}=\sigma^{2} \\
        \cfrac{n}{n-1} \rightarrow 1 \Rightarrow S_{0}^{2}=\frac{n}{n-1} S^{2} \xrightarrow[]{\mathrm{p}} \sigma^{2}
    \end{gather*}
    
    \item Используя несмещённость первого и второго выборочных моментов:
    \begin{multline*}
        \mathbb{E} S^{2} = \mathbb{E}\left(\overline{X^{2}}-(\overline{X})^{2}\right)
        = \mathbb{E} \overline{X^{2}}-\mathbb{E}(\overline{X})^{2}
        = \mathbb{E} X_{1}^{2}-\mathbb{E}(\overline{X})^{2} = \\
        = \mathbb{E} X_{1}^{2}-\left((\mathbb{E} \overline{X})^{2}+\mathbb{D} \overline{X}\right)
        = \mathbb{E} X_{1}^{2}-\left(\mathbb{E} X_{1}\right)^{2}-\mathbb{D}\left(\frac{1}{n} \sum_{i=1}^{n} X_{i}\right) = \\
        = \sigma^{2}-\frac{1}{n^{2}} n \mathbb{D} X_{1}
        = \sigma^{2}-\frac{\sigma^{2}}{n}
        = \frac{n-1}{n} \sigma^{2}
    \end{multline*}
    
    Откуда следует:
    \begin{equation*}
        \mathbb{E} S_{0}^{2}=\frac{n}{n-1} \mathbb{E} S^{2}=\sigma^{2}
    \end{equation*}
    
    \item Введём случайные величины $Y_{i}=X_{i}-a$; $\mathbb{E}Y_{i} = 0, \mathbb{D} Y_{1}=\mathbb{D} X_{1}=\sigma^{2}$.
    \begin{gather*}
        S^{2}=\frac{1}{n} \sum_{i=1}^{n}(X_{i}-\overline{X})^{2}=\frac{1}{n} \sum_{i=1}^{n}(X_{i}-a-(\overline{X}-a))^{2}=\overline{Y^{2}}-(\overline{Y})^{2} \\
        \begin{aligned}
            \sqrt{n}(S^{2}-\sigma^{2}) = \sqrt{n}(\overline{Y^{2}}-(\overline{Y})^{2}-\sigma^{2})
            = \sqrt{n}t(\overline{Y^{2}}-\mathbb{E} Y_{1}^{2})-\sqrt{n}(\overline{Y})^{2} = \\
            =\frac{\sum_{i=1}^{n} Y_{i}^{2}-n \mathbb{E} Y_{1}^{2}}{\sqrt{n}}-\overline{Y} \cdot \sqrt{n} \overline{Y} \Rightarrow \mathbf{N}(0, \mathbb{D}(X_{1}-a)^{2})
    \end{aligned}
    \end{gather*}
    ...поскольку $\cfrac{\sum_{i=1}^{n} Y_{i}^{2}-n \mathbb{E} Y_{1}^{2}}{\sqrt{n}} \Rightarrow \mathbf{N}(0, \mathbb{D} Y_{1}^{2})$ по ЦПТ, а $\overline{Y} \cdot \sqrt{n} \overline{Y} \Rightarrow 0$ как произведение последовательностей $\overline{Y} \xrightarrow[n \rightarrow \infty]{p} 0$ и $\sqrt{n} \overline{Y} \Rightarrow \mathbf{N}(0, \mathbb{D} X_{1})$.
\end{enumerate}
\end{proof}

\section{Точечная оценка. несмещённость, состоятельность, оптимальность. Теорема о единственности оптимальной оценки}
\begin{defn}
{\it Статистика} или {\it оценка}~--- измеримая функция от выборки $T(\mathbf{X})$.
\end{defn}

\begin{defn}
{\it Несмещённая оценка} парамаетра $\theta$~--- статистика $T(\mathbf{X})$, т.ч. $\forall \theta \in \Theta: \mathbb{E}T(\theta) = \theta$.
\end{defn}

\begin{defn}
{\it Асимптотически несмещённая оценка} параметра $\theta$~--- статистика $T(\mathbf{X})$, т.ч. $\forall \theta \in \Theta: \mathbb{E}T(\theta) \xrightarrow[n \rightarrow \infty]{} \theta$.
\end{defn}

\begin{defn}
{\it Состоятельная оценка} параметра $\theta$~--- статистика $T(\mathbf{X})$, т.ч. $\forall \theta \in \Theta: T(\theta) \xrightarrow[n \rightarrow \infty]{p} \theta$.
\end{defn}

Оценки также могут вводиться и для функций $\tau(\theta)$ параметра $\theta$; они обладают всеми аналогичными свойствами.

Несмещённость означает отсутствие ошибки «в среднем», т. е. при систематическом использовании данной оценки. Несмещённость является желательным, но не обязательным свойством оценок. Достаточно, чтобы смещение оценки (разница между её средним значением и истинным параметром) уменьшалось с ростом объёма выборки. Поэтому асимптотическая несмещённость является весьма желательным свойством оценок. Свойство состоятельности означает, что последовательность оценок приближается к неизвестному параметру при увеличении количества наблюдений. В отсутствие этого свойства оценка совершенно «несостоятельна» как оценка.

\begin{rmrk}
    Отметим некоторые свойства несмещённых и состоятельных оценок.
    \begin{enumerate}
        \item несмещённые оценки не единственны.
        
        К примеру в качестве несмещённой оценки для математического ожидания $\mathbb{E} X$ могут выступать $\mathbb{E} X_{1}$ или$\mathbb{E} \overline(\mathbf{X})$.
        
        \item несмещённые оценки могут не существовать.
        \begin{exmp}
            Дано распределение $\operatorname{Pois}(\theta)$, над которым произведено одно наблюдение. Найти несмещённую оценку для функции $\tau(\theta) = \cfrac{1}{\theta}$.
                \begin{equation*}
                    \mathbb{E}T(\theta) 
                    = \mathlarger{\mathlarger{\sum}}_{x=0}^{\infty}T(x)e^{-\theta}\cfrac{\theta^{x}}{x!} 
                    = \cfrac{1}{\theta}
                    \Rightarrow \mathlarger{\mathlarger{\sum}}_{x=0}^{\infty}T(x)\cfrac{\theta^{x+1}}{x!}
                    = \mathlarger{\mathlarger{\sum}}_{r=0}^{\infty}\cfrac{\theta^{r}}{r!}
                    \Rightarrow T(x) \equiv \cfrac{1}{\theta}
                \end{equation*}
            Т.к. полученная статистика зависит от $\theta$, искомой несмещённой оценки для $\tau(\theta)$ не существует.
        \end{exmp}
        
    \item несмещённые оценки могут существовать, но быть бессмысленными.
    \begin{exmp}
        Дано отрицательное биноминальное распределение $\operatorname{B}(1, \theta)$, над которым произведено одно наблюдение. Найти несмещённую оценку для параметра $\theta$.
        \begin{gather*}
            \mathbb{E}T(\theta) 
            = \sum_{x=0}^{\infty}T(x)\theta^{x} 
            = \cfrac{\theta}{1-\theta} 
            = \sum_{r=1}^{\infty}\theta^{r} \\
            T(x) = 
            \left\{\begin{array}{ll}
                0, & \text { если } x = 0 \\
                1, & \text { если } x \geq 1
            \end{array}\right.
        \end{gather*}
    Значения этой статистики не принадлежат параметрическому множеству $\Theta = (0, 1)$, следовательно, эта оценка бессмысленна.
    \end{exmp}
    
    \item Состоятельные оценки не единственны.
    
    К примеру, выборочная дисперсия $S^{2}$ и несмещённая выборочная дисперсия $S_0^{2}$ являются состоятельными оценками теоретической дисперсии.
    
    \item Состоятельные оценки могут быть смещёнными.
    
    Как было показано ранее, выборочная дисперсия является состоятельной, но смещённой оценкой теоретической дисперсии
    
    \end{enumerate}
\end{rmrk}

Для несмещённой оценки $T(\mathbf{X})$ функции $\tau(\theta)$: $\mathbb{E}_{\theta}(T(\mathbf{X})-\tau(\theta))^{2}=\mathbb{D}_{\theta} T(\mathbf{X})$. Т.к. для двух разных оценок $T_1(\mathbf{X})$, $T_2(\mathbf{X})$ соответствующие дисперсии могут быть несравнимыми, введём понятие оптимальной оценки.

\begin{defn}
{\it Оптимальная оценка} функции $\tau(\theta)$~--- статистика $T(\mathbf{X})$, т.ч.:
\begin{enumerate}
    \item $T(\mathbf{X})$~--- несмещённая.
    \item $T(\mathbf{X})$ имеет равномерно минимальную дисперсию, т.е. для любой другой несмещённой оценки $T^{*}(\mathbf{X})$ функции $\tau(\theta)$: $\mathbb{D}_{\theta} T(\mathbf{X}) \leq \mathbb{D}_{\theta} T_{1}(\mathbf{X})~ \forall X$.
\end{enumerate}
\end{defn}
\pagebreak
\begin{thm}
Если существует оптимальная оценка функции $\tau(\theta)$, то она единственна.
\end{thm}

\begin{proof}
Предположим обратное: пусть существуют две оптимальные оценки $T_1(\mathbf{X})$ и $T_2(\mathbf{X})$ функции $\tau(\theta)$. Тогда в силу их несмещённости: $\mathbb{E}_{\theta} T_{1}(\mathbf{X})=\mathbb{E}_{\theta} T_{2}(\mathbf{X})=T(\theta)$, а а в силу того, что они имеют равномерно минимальную дисперсию: $\mathbb{D}_{\theta} T_{1}(\mathbf{X})=\mathbb{D}_{\theta} T_{2}(\mathbf{X})~ \forall \theta$.

Введём новую статистику: 
\begin{equation}
    T_{3}(\mathbf{X})=\cfrac{T_{1}(\mathbf{X})+T_{2}(\mathbf{X})}{2}
\end{equation}

Так как $\mathbb{E}_{\theta} T_{3}(\mathbf{X})=\cfrac{\mathbb{E}_{\theta} T_{1}(\mathbf{X})+\mathbb{E}_{\theta} T_{2}(\mathbf{X})}{2}=\tau(\theta)$, то $T_{3}(\mathbf{X})$~--- несмещённая оценка функции $\tau(\theta)$.

Имеем также:
\begin{equation*}
    \mathbb{D}_{\theta} T_{3}(\mathbf{X})=\cfrac{\mathbb{D}_{\theta}\left(T_{1}(\mathbf{X})+T_{2}(\mathbf{X})\right)}{4} =
    \cfrac{\mathbb{D}_{\theta} T_{1}(\mathbf{X})+\mathbb{D}_{\theta} T_{2}(\mathbf{X})+2 \operatorname{cov}\left(T_{1}(\mathbf{X}) T_{2}(\mathbf{X})\right)}{4}
\end{equation*}

В силу свойства
\begin{equation*}
    \mathbb{E}_{\theta} \xi^{2}<\infty, \mathbb{E}_{\theta} \eta^{2}<\infty \Rightarrow|\operatorname{cov}(\xi, \eta)| = | \mathbb{E}(\xi-\mathbb{E} \xi)(\eta-\mathbb{E} \eta)| \leq \sqrt{\mathbb{D} \xi} \sqrt{\mathbb{D} \eta},
\end{equation*}
где равенство достигается тогда и только тогда, когда $\xi=a \eta+b$, получаем:
\begin{equation*}
    \mathbb{D}_{\theta} T_{3}(\mathbf{X}) \leq \cfrac{\mathbb{D}_{\theta} T_{1}(\mathbf{X})+\mathbb{D}_{\theta} T_{2}(\mathbf{X})+2 \sqrt{\mathbb{D}_{\theta} T_{1}(\mathbf{X})} \sqrt{\mathbb{D}_{\theta} T_{2}(\mathbf{X})})}{4} =\mathbb{D}_{\theta} T_{1}(\mathbf{X})
\end{equation*}

В силу того, что $T_1(\mathbf{X})$ и $T_2(\mathbf{X})$ — оптимальные, дисперсия $T_3(\mathbf{X})$ не может быть меньше дисперсии $T_1(\mathbf{X})$, следовательно, справедливо равенство, достигаемое при следующих условиях:
\begin{equation*}
\begin{aligned}
    T_{1}(\mathbf{X})=a T_{2}(\mathbf{X})+b \Rightarrow \mathbb{E} T_{1}(\mathbf{X})
    = a \mathbb{E} T_{2}(\mathbf{X})+b 
    \Leftrightarrow \\
    \Leftrightarrow T(\theta) = a T(\theta)+b~ \forall \theta \Rightarrow a = 1, b = 0
\end{aligned}
\end{equation*}

\end{proof}

\section{Функция правдоподобия. Достаточные статистики, полные статистики. Теорема факторизации}

В зависимости от типа распределения $\mathcal{F}_\theta$ обозначим через $f_{\theta}(y)$ одну из следующих функций:
\begin{equation*}
    f_{\theta}(y) =
    \left\{\begin{array}{ll}
    \text { плотность } f_{\theta}(y), & \text { если } \mathcal{F}_{\theta} \text { абсолютно непрерывно, } \\
    P_{\theta}\left(X_{1}=y\right), & \text { если } \mathcal{F}_{\theta} \text { дискретно. }
    \end{array}\right.
\end{equation*}

\begin{defn}
{\it Функция правдоподобия} выборки $\mathbf{X}$:
\begin{equation*}
    L(\mathbf{X} , \theta)=f_{\theta}\left(X_{1}\right) \cdot f_{\theta}\left(X_{2}\right) \cdot \ldots \cdot f_{\theta}\left(X_{n}\right)=\prod_{i=1}^{n} f_{\theta}\left(X_{i}\right)
\end{equation*}
\end{defn}

В дискретном случае функция правдоподобия принимает вид:
\begin{equation*}
\begin{aligned}
    L(\mathbf{x} , \theta)=\prod_{i=1}^{n} f_{\theta}(x_{i}) 
    = \mathrm{P}_{\theta}(X_{1}=x_{1}) \cdot \ldots \cdot \mathrm{P}_{\theta}(X_{n}=x_{n}) = \\
    = \mathrm{P}_{\theta}(X_{1}=x_{1}, \ldots, X_{n}=x_{n})
\end{aligned}
\end{equation*}

Таким образом, смысл функции правдоподобия~--- вероятность попасть в заданную точку при соответствующем параметре $\theta$ в дискретном случае; для абсолютно непрерывного аналогично~--- вероятность попасть в куб с центром в $x_1, \ldots, x_n$ и сторонами $dx_1, \ldots, dx_n$.

\begin{defn}
{\it Достаточная статистика} для параметра $\theta$~--- статистика $T(\mathbf{X})$, т.ч. $\forall t,~ \forall B \in \mathfrak{B}(\mathbb{R}^{n})$ условное распределение $\mathbb{P}(X_1, \ldots, X_n \in B~|~T=t)$ не зависит от параметра $\theta$.
\end{defn}

Иными словами, если значение статистики $T$ известно и фиксировано, то даже знание её распределения больше не даёт никакой информации о параметре; достаточно лишь вычислить $T$ по выборке.

\begin{thm}[Критерий факторизации]
$T(\mathbf{X})$~--- достаточная статистика $\Leftrightarrow$ её функция правдоподобия представима в виде $L(\mathbf{X}_{1}, \ldots, X_{n} , \theta) \stackrel{\text{п.н.}}{=} h(\mathbf{X}) \cdot \Psi(S, \theta)$
\end{thm}

\begin{proof}
Рассмотрим только дискретный случай. Пусть $T(\mathbf{X})$~--- достаточная статистика. Если $T(\mathbf{X})=t$, то событие $\{\mathbf{X}=\mathbf{x}\} \subseteq \{T(\mathbf{X})=t\}$. Поэтому
\begin{multline*}
    L(\mathbf{x}, \theta) = \mathrm{P}_{\theta}(\mathbf{X}=\mathbf{x})=\mathrm{P}_{\theta}(\mathbf{X}=\mathbf{x} | T(\mathbf{X})=t) =\\
    = \underbrace{\mathrm{P}_{\theta}(T(\mathbf{X})=t)}_{g(T(\mathbf{x}), \theta)} \underbrace{\mathrm{P}_{\theta}(\mathbf{X}=\mathbf{x} | T(\mathbf{X})=t)}_{h(\mathbf{x}, t)}
\end{multline*}

Пусть теперь функция правдоподобия имеет вид $L(\mathbf{x}, \theta)=g(T(\mathbf{x}, \theta) h(\mathbf{x})$. Тогда, если $x$ таково, что $T(\mathbf{x})=t$, то:
\begin{multline*}
    \mathrm{P}(\mathbf{X}=\mathbf{x} | T(\mathbf{X})=t) =\frac{\mathrm{P}(\mathbf{X}=\mathbf{x}, T(\mathbf{X})=t)}{\mathrm{P}(T(\mathbf{X})=t)}
    =\frac{\mathrm{P}(\mathbf{X}=\mathbf{x})}{\sum_{\mathbf{x}^{\prime}: T(\mathbf{x}^{\prime})=t} \mathrm{P}(\mathbf{X}=\mathbf{x}^{\prime})} = \\
    = \frac{g(t, \theta) h(\mathbf{x})}{\sum_{\mathbf{x}^{\prime}: T(\mathbf{x}^{\prime})=t} g(t, \theta) h(\mathbf{x}^{\prime})}
    = \frac{h(\mathbf{x})}{\sum\limits_{\mathbf{x}^{\prime}: T(\mathbf{x}^{\prime})=t} h(\mathbf{x}^{\prime})}
\end{multline*}
\end{proof}

\begin{defn}
{\it Полная статистика} для параметра $\theta$~--- статистика $T(\mathbf{X})$, т.ч. $\mathbb{E} g(T)=0~\forall \theta \in \Theta \Rightarrow g(T) \stackrel{\text{п.н.}}{=}0$
\end{defn}

\section{Неравенство Рао—Крамера. Эффективные оценки}

Пусть $X_1, \ldots, X_n$  —  некоторая выборка с функцией правдоподобия $L(\mathbf{X}, \theta)$ относительно некоторой меры $\mu$. Введём функцию $\varphi(\theta)=\int\limits_{\mathbf{R}^{n}} T(x) L(x, \theta) \mu(d x)<\infty$, в дальнейшем считая, что она дифференцируема необходимое число раз.

\begin{defn}
Функция правдоподобия $L(\mathbf{X}, \theta)$ {\it удовлетворяет условиям регулярности для $m$-й производной}, если существует
\begin{equation*}
    \cfrac{d^{m} \varphi(\theta)}{d \theta^{m}}=\int\limits_{\mathbb{R}^{n}} T(x) \cfrac{\partial^{m} L(x, \theta)}{\partial \theta^{m}} \mu(d x),
\end{equation*}
причём множество $\left\{ {x~|~L(x,\theta) > 0} \right\}$ не зависит от параметра $\theta$.
\end{defn}

\begin{thm}[Неравенство Рао-Крамера]
Пусть $X_1, \ldots, X_n$ — выборка, $L(\mathbf{X}, \theta)$ удовлетворяет условиям регулярности для первой производной и $\tau(\theta)$  —  дифференцируемая функция $\theta$. Тогда:
\begin{enumerate}
    \item Для любой $~T(\mathbf{X})$,~--- несмещённой оценки функции $\tau(\theta)$, справедливо неравенство:
    \begin{gather*}
        \mathbb{D}_{\theta} T(\mathbf{X}) \geq \cfrac{(\tau^{\prime}(\theta))^{2}}{\mathbb{E}_{\theta} U^{2}(X, \theta)}~\forall \theta \in \Theta, \\
        \text{где}~ U(X, \theta)=\cfrac{\partial \ln L(\mathbf{X}, \theta)}{\partial \theta}~\text{(функция вклада)}
    \end{gather*}
    
    \item Равенство достигается $\Leftrightarrow \exists~ a_n(\theta):~ T(\mathbf{X})-\tau(\theta)=a_{n}(\theta) \cdot U(X, \theta)$
\end{enumerate}
\end{thm}

\begin{proof}
$\int L(x, \theta) \mu(d x)=1 \Rightarrow \int \cfrac{\partial L(x, \theta)}{\partial \theta} \mu(d x)=0$

Из условий регулярности $L(\mathbf{X}, \theta)$ для следует:
\begin{equation*}
    \int T(x) L(x, \theta) \mu(d x)=\mathbb{E}_{\theta} T(\mathbf{X})=T(\theta) \Rightarrow \int T(x) \cfrac{\partial L(x, \theta)}{\partial \theta} \mu(d x)=\tau^{\prime}(\theta)
\end{equation*}

Заметим, что
\begin{equation*}
    \cfrac{\partial L(x, \theta)}{\partial \theta}=\cfrac{\partial \ln L(x, \theta)}{\partial \theta} \cdot L(x, \theta)
\end{equation*}

Откуда следует:
\begin{gather*}
    \int U(x, \theta) L(x, \theta) \mu(d x)=0 \Leftrightarrow \mathbb{E}_{\theta} U(X, \theta)=0 \\
\int T(x) U(x, \theta) L(x, \theta) \mu(d x)=\tau^{\prime}(\theta) \Leftrightarrow \mathbb{E}_{\theta} T(\mathbf{X}) U(X, \theta)=\tau^{\prime}(\theta)
\end{gather*}

Вычитая из первого равенства, помноженного на $\tau(\theta)$, второе, получаем:
\begin{equation*}
    \mathbb{E}_{\theta}(T(\mathbf{X})-T(\theta)) U(X, \theta)=\tau^{\prime}(\theta)
\end{equation*}

В левой части полученного равенства стоит ковариация случайных величин $T(\mathbf{X})$ и $U(X,\theta)$:
\begin{equation*}
    \operatorname{cov}_{\theta}(T(\mathbf{X}), U(X, \theta))=T^{\prime}(\theta)
\end{equation*}

Из неравенства Коши-Буняковского:
\begin{equation*}
    \left(\tau^{\prime}(\theta)\right)^{2}=\operatorname{cov}_{\theta}^{2}(T(\mathbf{X}) U(X, \theta)) \leq \mathbb{D}_{\theta} T(\mathbf{X}) \mathbb{D}_{\theta} U(X, \theta)=\mathbb{D}_{\theta} T(\mathbf{X}) \mathbb{E}_{\theta} U^{2}(X, \theta)
\end{equation*}

...что равносильно п.1 теоремы:
\begin{equation*}
    \mathbb{D}_{\theta} T(\mathbf{X}) \geq \cfrac{\left[\tau^{\prime}(\theta)\right]^{2}}{\mathbb{E}_{\theta} U^{2}(X, \theta)}
\end{equation*}

Неравенство достигается, если линейно связаны:
\begin{equation*}
    T(\mathbf{X})=\varphi(\theta) U(X, \theta)+\psi(\theta) \Rightarrow T(\theta)=\psi(\theta) \Rightarrow a_{n}(\theta)=\varphi(\theta)
\end{equation*}

\end{proof}

Рассмотрим некоторый класс оценок $K=\left\{\hat{\theta}\left(\mathbf{X}\right)\right\}$ параметра $\theta$.
\begin{defn}
    Говорят, что оценка $\theta^{*}\left(\mathbf{X}\right) \in K$ является эффективной оценкой параметра $\theta$ в классе $K$, если для любой другой оценки $\hat{\theta} \in K$ имеет место неравенство:
    \begin{equation*}
        E\left(\theta^{*}-\theta\right)^{2} \leqslant E(\hat{\theta}-\theta)^{2}~ \forall \theta \in \Theta
    \end{equation*}
\end{defn}
Обозначим класс несмещённых оценок:
\begin{equation*}
    K_{0}=\left\{\hat{\theta}\left(\mathbf{X}\right): E \hat{\theta}=\theta, \forall \theta \in \Theta\right\}
\end{equation*}
Оценка, эффективная в $K_0$ называется просто {\it эффективной}.

Для оценки $\theta^{*} \in K_{0}$ по определению дисперсии
\begin{equation*}
    \mathbb{E}\left(\theta^{*}-\theta\right)^{2}=\mathbb{E}\left(\theta^{*}-\mathbb{E} \theta^{*}\right)^{2}=\mathbb{D} \theta^{*}
\end{equation*}

Добавить Чернова стр 35

\begin{rmrk}
Если в неравенстве Рао---Крамера достигается равенство, то полученная оценка~--- эффективная.

Если существует эффективная оценка для функции $\tau(\theta)$, то ни для какой другой функции от $\theta$, кроме линейного преобразования $\tau(\theta)$, эффективной оценки существовать не будет. 
\end{rmrk}

\section{Теорема Рао—Блекуэлла—Колмогорова. Оптимальность оценок являющихся функцией полной достаточной статистики}

\begin{thm}[Теорема Рао—Блекуэлла—Колмогорова] Если оптимальная оценка функции $\tau(\theta)$ существует, то она является функцией от достаточной статистики.
\end{thm}

\begin{proof}
В доказательстве используются следующие свойства условного матожидания: 
\begin{gather*}
    \mathbb{E} f(x, z)=\mathbb{E}(\mathbb{E}(f(x, z) | z)) \\
    \mathbb{E}(g(z) | z)=g(z)
\end{gather*}

\begin{enumerate}
    \item Пусть $T(\mathbf{X})$~--- достаточная статистика, $T_1(\mathbf{X})$~--- несмещённая оценка функции $\tau(\theta)$, т.е. $\mathbb{E} T_{1}(\mathbf{X})=\tau(\theta)$. Рассмотрим функцию $H(T)=\mathbb{E}\left(T_{1} | T\right)$. Тогда из первого свойства следует:
    \begin{equation*}
        \mathbb{E} H(T)=\mathbb{E}\left(\mathbb{E}\left(T_{1} |     T\right)\right)=\mathbb{E} T_{1}=\tau(\theta) \Rightarrow H(T)     \text{~--- несмещённая оценка~} \tau(\theta)
    \end{equation*}

    \item Докажем равномерную минимальность её дисперсии:
    \begin{multline*}
        \mathbb{E}((T_{1}-H(T))(H(T)-\tau(\theta))
        = \mathbb{E}(\mathbb{E}((T_{1}-H(T))(H(T)-\tau(\theta)) | T)) 
        = \\
        = \mathbb{E}((H(T)-H(T))(H(T)-\tau(\theta)))
        = 0
    \end{multline*}

    Тогда из свойств условного матожидания
    \begin{multline*}
        \mathrm{D}\left(T_{1}\right) 
        = \mathrm{E}\left(T_{1}-\tau(\theta)\right)^{2}=\mathrm{E}\left(T_{1}-H(T)+H(T)-\tau(\theta)\right)^{2} =\\
        = \mathrm{E}\left(T_{1}-H(T)\right)^{2}+\mathrm{D}(H(T)) \geqslant \mathrm{D}(H(T))
    \end{multline*}
\end{enumerate}
Таким образом, $H(T)$~--- оптимальная оценка $\tau(\theta)$.

\end{proof}

\begin{thm}
{\it Теорема Колмогорова:} Если $T(\mathbf{X})$~--- полная достаточная статистика, то она является оптимальной оценкой своего математического ожидания.
\end{thm}

\begin{proof}
Докажем, что $T(\mathbf{X})$ является единственной несмещённой оценкой для $\mathbb{E}T(\mathbf{X})$. Тогда $T(\mathbf{X})$ будет оптимальной оценкой. Предположим, что $T_1(\mathbf{X})$~--- оптимальная оценка для $\mathbb{E}T(\mathbf{X})$. Из теоремы Рао-Блекуэлла-Колмогорова получаем, что $T_{1}=H(T)$ и $\mathbb{E} T_{1}=\mathbb{E} T$. Тогда:

\begin{equation*}
    \mathbb{E} \underbrace{(T(\mathbf{X})-H(T(\mathbf{X})))}_{\varphi(T)}=0
\end{equation*}

Из условия полноты $T(\mathbf{X})$ следует, что $\varphi(T)=0$ с вероятностью 1, т.е. $T=H(T)$ с вероятностью 1.
\end{proof}

\section{Метод моментов. Свойства оценок, полученных методом моментов}

Пусть $X_1, \ldots, X_n$~--- выборка объёма $n$ из параметрического семейства распределений $\mathcal{F}_\theta$. Выберем функцию $g(y): \mathbb{R} \rightarrow \mathbb{R}$ так, чтобы существовал момент $\mathbb{E} g\left(X_{1}\right)=h(\theta)$ и функция $h(\theta)$ была обратима на $\Theta$. Разрешим полученное уравнение относительно $\theta$, а затем вместо истинного момента возьмём выборочный:

\begin{equation*}
    \theta=h^{-1}\left(\mathbb{E} g\left(X_{1}\right)\right), \quad \theta^{*}=h^{-1}(\overline{g(\mathbf{X})})=h^{-1}\left(\frac{1}{n} \sum_{i=1}^{n} g\left(X_{i}\right)\right)
\end{equation*}

Полученная оценка $\theta^{*}$~--- {\it оценка метода моментов} для параметра $\theta$. Чаще всего берут $g(y)=y^{k}$. В этом случае, при условии обратимости функции $h$ на $\Omega$:
\begin{equation*}
    \mathbb{E} X_{1}^{k}=h(\theta), \quad \theta=h^{-1}\left(\mathbb{E} X_{1}^{k}\right), \quad \theta^{*}=h^{-1}(\overline{X^{k}})=h^{-1}\left(\frac{1}{n} \sum_{i=1}^{n} X_{i}^{k}\right)
\end{equation*}

\begin{thm}
Пусть $\theta^{*}=h^{-1}(\overline{g(\mathbf{X})})$~--- оценка параметра $\theta$, полученная методом моментов, причём функция $h^{-1}$ непрерывна. Тогда оценка $\theta^{*}$ состоятельна.
\end{thm}

\begin{proof}
По ЗБЧ Хинчина имеем:

\begin{equation*}
    \overline{g(\mathbf{X})}=\frac{1}{n} \sum_{i=1}^{n} g\left(X_{i}\right) \xrightarrow[]{\mathrm{p}} \mathbb{E} g\left(X_{1}\right)=h(\theta)
\end{equation*}

Ввиду непрерывности функции $h^{-1}$:

\begin{equation*}
    \theta^{*}=h^{-1}(\overline{g(\mathbf{X})}) \xrightarrow[]{\mathrm{p}} h^{-1}\left(\mathbb{E} g\left(X_{1}\right)\right)=h^{-1}(h(\theta))=\theta
\end{equation*}
\end{proof}

\begin{defn}
{\it Асимптотически нормальная оценка} параметра $\theta$ с коэффициентом $\sigma^{2}(\theta)$~--- оценка $\theta^{*}$, т.ч. при $n \rightarrow \infty$ имеет место слабая сходимость к стандартному нормальному распределению: $\sqrt{n}(\theta^{*}-\theta) \Rightarrow \mathbf{N}(0, \sigma^{2}(\theta))$.
\end{defn}

\begin{lem}
Пусть функция $g(y)$ такова, что $0 \neq \mathbb{D} g\left(X_{1}\right)<\infty$. Тогда статистика $\overline{g(\mathbf{X})}$ является асимптотически нормальной оценкой для $\mathbb{E} g\left(X_{1}\right)$ с коэффициентом $\sigma^{2}(\theta)=\mathbb{D} g\left(X_{1}\right)$:

\begin{equation*}
    \sqrt{n} \cfrac{\overline{g(\mathbf{X})}-\mathbb{E} g\left(X_{1}\right)}{\sqrt{\mathbb{D} g\left(X_{1}\right)}} \Rightarrow \mathbf{N}(0,1)
\end{equation*}
\end{lem}

\begin{proof}
Следует непосредственно из ЦПТ.
\end{proof}

\begin{rmrk}
Следующая теорема утверждает асимптотическую нормальность оценок вида

\begin{equation*}
    \theta^{*}=H(\overline{g(\mathbf{X})})=H\left(\cfrac{g\left(X_{1}\right)+\ldots+g\left(X_{n}\right)}{n}\right)
\end{equation*}

которые обычно получаются при использовании метода моментов, при этом всегда $\theta=H\left(\mathbb{E} g\left(X_{1}\right)\right)$.
\end{rmrk}

\begin{thm}
Пусть функция $g(y)$ такова, что $0 \neq \mathbb{D} g\left(X_{1}\right)<\infty$, функция $H(y)$ дифференцируема в точке $a=\mathbb{E} g\left(X_{1}\right)$ и её производная в этой точке $H^{\prime}(a)=\left.H^{\prime}(y)\right|_{y=a}$ отлична от нуля. Тогда оценка $\theta^{*}=H(\overline{g(\mathbf{X})})$
является асимптотически нормальной
оценкой для параметра $\theta=H\left(\mathbb{E} g\left(X_{1}\right)\right)=H(a)$ с коэффициентом асимптотической нормальности $\sigma^{2}(\theta)=\left(H^{\prime}(a)\right)^{2} \cdot D g\left(X_{1}\right)$.
\end{thm}

\begin{proof}
Согласно ЗБЧ последовательность $\overline{g(\mathbf{X})}$ стремится к $a=\mathbb{E} g\left(X_{1}\right)$ по вероятности с ростом $n$: Функция

\begin{equation*}
    G(y)=\left\{\begin{array}{ll}
    \cfrac{H(y)-H(a)}{y-a}, & y \neq a \\
    H^{\prime}(a), & y=a
    \end{array}\right.  
\end{equation*}

по условию непрерывна в точке $a$: Поскольку сходимость по веро-
ятности сохраняется под действием непрерывной функции, получим,
что $G(\overline{g(\mathbf{X})}) \xrightarrow[]{\mathrm{p}} G(a)=H^{\prime}(a)$.

Заметим также, что по вышеприведённой лемме величина $\sqrt{n}(\overline{g(\mathbf{X})}-a)$ слабо сходится
к нормальному распределению $\mathbf{N}(0, \mathbb{D} g(X_{1}))$: Пусть $\xi$~--- случайная величина
из этого распределения. Тогда

\begin{equation*}
    \sqrt{n}(H(\overline{g(\mathbf{X})})-H(a))=\sqrt{n}(\overline{g(\mathbf{X})}-a) \cdot G(\overline{g(\mathbf{X})}) \Rightarrow \xi \cdot H^{\prime}(a)
\end{equation*}

Мы использовали следующее свойство слабой сходимости: если $\xi_{n} \Rightarrow \xi$ и $\eta_{n} \xrightarrow[]{\mathrm{p}} c=\mathrm{const}$, то $\xi_{n} \eta_{n} \Rightarrow c \xi$. Но распределение случайной величины $\xi \cdot H^{\prime}(a)$ есть $\mathbf{N}(0,(H^{\prime}(a))^{2} \cdot \mathbb{D} g(X_{1}))$, откуда следует

\begin{equation*}
    \sigma^{2}(\theta)=\left(H^{\prime}(a)\right)^{2} \cdot \mathbb{D} g\left(X_{1}\right)
\end{equation*}

\end{proof}

\section{Метод максимального правдоподобия. Свойства оценок максимального правдоподобия}

\begin{defn}
{\it Оценка максимального правдоподобия $\hat{\theta}$ параметра $\theta$}~--- точка параметрического множества $\Theta$, в которой функция правдоподобия $L(\mathbf{X},\theta)$ при заданном $X$ достигает максимума, т.е.:
\begin{equation*}
    L(\boldsymbol{x}, \hat{\theta})=\sup _{\theta \in \Theta} L(\boldsymbol{x}, \theta)
\end{equation*}
\end{defn}

\begin{rmrk}
Поскольку функция $\operatorname{ln}y$ монотонна, то точки максимума функций $L(\mathbf{X},\theta)$ и $ln L(\mathbf{X},\theta)$ совпадают.
\end{rmrk}

Если для каждого $X$ максимум функции правдоподобия достигается во внутренней точке $\Theta$, и $L(\mathbf{X},\theta)$ дифференцируема по $\theta$, то оценка максимального правдоподобия $\hat{\theta}$ удовлетворяет уравнению:

\begin{equation*}
    \cfrac{\partial \ln L(\mathbf{x}, \theta)}{\partial \theta}=0
\end{equation*}

Если $\theta$~--- векторный параметр: $\theta=\left(\theta_{1}, \ldots, \theta_{n}\right)$, то это уравнение заменяется системой уравнений:

\begin{equation*}
    \cfrac{\partial \ln L(\mathbf{x}, \theta)}{\partial \theta_{i}}=0,~ i=\overline{1, n} 
\end{equation*}


\begin{thm}
Если существует эффективная оценка $T(\mathbf{X})$ скалярного параметра $\theta$, то она совпадает с оценкой максимального правдоподобия.
\end{thm}

\begin{proof}
Если оценка $T(\mathbf{X})$ скалярного параметра $\theta$ эффективна, то в неравенстве Рао-Крамера достигается равенство:

\begin{equation*}
    U(X,\theta) = \cfrac{\partial \ln L(\mathbf{x}, \theta)}{\partial \theta}=\cfrac{T(\mathbf{X})-\theta}{a_n(\theta)}
\end{equation*}

\end{proof}

\begin{thm}
Если $T(\mathbf{X})$ достаточная статистика, а оценка максимального правдоподобия $\hat{\theta}$ существует и единственна, то она является функцией от $T(\mathbf{X})$.
\end{thm}

\begin{proof}
Из критерия факторизации следует, что если $T=T(\mathbf{X})$ достаточная статистика, то имеет место представление:

\begin{equation*}
    L(\mathbf{X}, \theta)=g(T(\mathbf{X}), \theta) h(\mathbf{X})
\end{equation*}

Таким образом, максимизации $L(\mathbf{X},\theta)$ сводится к максимизации $g(T(\mathbf{X}), \theta)$ по $\theta$, Следовательно $\hat{\theta}$ есть функция от $T(\mathbf{X})$.
\end{proof}

\begin{defn}
    {\it Асимптотически эффективная оценка}~---
\end{defn}

\begin{thm}
    Пусть выполнены следующие условия:
    \begin{enumerate}
        \item Функция правдоподобия $L(\mathbf{X}, \theta)$ удовлетворяет условиям регулярности для первых двух производных;
        \item $\exists!~ \theta^{*}$~--- оценка максимального правдоподобия для всех $\theta$, которая достигается во внутренней точке $\Theta$.
    \end{enumerate}
    Тогда оценка $\theta^{*}$:
    \begin{enumerate}
        \item асимптотически несмещёна
        \item состоятельна
        \item асимптотически эффективна
        \item асимптотически нормальна
    \end{enumerate}
\end{thm}

Добавить асимптотическую нормальность и эффективность + Чернова стр 39 теорема для состоятельности.

\section{Интервальное оценивание. Методы центральной статистики и использования точечной оценки}

\begin{defn}
{\it Доверительный интервал} для параметра $\theta$ с коэффициентом доверия $0 \leq \alpha \leq 1$~--- интервал $(T_1(\mathbf{X}), T_2(\mathbf{X}))$, т.ч. $\mathbb{P}_{\theta}(T_1(\mathbf{X}) < \theta < T_2(\mathbf{X})) \geq \alpha$.
\end{defn}

\begin{exmp}
Пусть $X_1, \ldots, X_n$~--- выборка из $\mathbf{N}(\theta, 1)$. Тогда

\begin{equation*}
    \theta^{*}
    = \overline{X}
    = \frac{1}{n} \sum_{i=1}^{n} X_{i} \sim \mathbf{N}\left(\theta, \frac{1}{n}\right)
    \Rightarrow (\overline{X}-\theta) \sqrt{n} \sim \mathbf{N}(0,1)
\end{equation*}

Для величины, имеющей стандартное нормальное распределение, строим доверительный интервал, т.е. находим такое $t_{\alpha / 2}$, что 

\begin{equation*}
    \mathrm{P}_{\theta}\left(|(\bar{X}-\theta) \sqrt{n}|<t_{\alpha / 2}\right)=\alpha
\end{equation*}

Решаем уравнение относительно $\theta$ и получаем
\begin{equation*}
    \mathrm{P}_{\theta}\left(\bar{X}-\cfrac{t_{\alpha / 2}}{\sqrt{n}}<\theta<\bar{X}+\cfrac{t_{\alpha / 2}}{\sqrt{n}}\right)=\alpha 
\end{equation*}

\end{exmp}

\begin{defn}
{\it Центральная статистика}~--- функция $G(X,\theta)$, т.ч.:
\begin{enumerate}
    \item $G(X,\theta)$ непрерывна и строго монотонна по $\theta$ при любом фиксированном $X$.
    \item $\mathbb{P}_{\theta}(G(X, \theta)<t)=F(t)$ непрерывна и не зависит от $\theta$.
\end{enumerate}
\end{defn}

\begin{rmrk}
Формально определённая выше величина не является статистикой, т.к. зависит от неизвестного параметра $\theta$.
\end{rmrk}

Построение доверительного интервала с помощью центральной статистики:
\begin{enumerate}
    \item Зафиксируем $\alpha_{1}, \alpha_{2} \in \mathbf{R}$, т.ч.
    \begin{equation*}
        \mathbb{P}_{\theta}(\alpha_{1} \leq G(X, \theta) \leq \alpha_{2})=\alpha~\forall \theta \Leftrightarrow F_{G}(\alpha_{2})-F_{G}(\alpha_{1})=\alpha
    \end{equation*}
    \item Пусть $G(X,\theta)$ возрастает. Из условий
    \begin{equation*}
        \left\{\begin{array}{l}
        G(X, \theta) \leq \alpha_{2} \\
        G(X, \theta) \geq \alpha_{1}
        \end{array}\right.
    \end{equation*}
    находятся статистики
    \begin{equation*}
        \left\{\begin{array}{l}
            T_{2}(\mathbf{X}): G(X, T_{2}(\mathbf{X}))=\alpha_{2} \\ 
            T_{1}(\mathbf{X}): G(X, T_{1}(\mathbf{X}))=\alpha_{1}
        \end{array} 
        \Leftrightarrow T_{1}(\mathbf{X}) \leq \theta \leq T_{2}(\mathbf{X})\right.
    \end{equation*}
    откуда $\mathbb{P}_{\theta}\left(T_{1}(\mathbf{X}) \leq \theta \leq T_{2}(\mathbf{X})\right) \geq \alpha~ \forall \theta$.
\end{enumerate}

\begin{defn}
{\it Центральный доверительный предел} для параметра $\theta$ с коэффициентом доверия $0 \leq \alpha \leq 1$~--- интервал $(T_1(\mathbf{X}), T_2(\mathbf{X}))$, т.ч. 
\begin{gather*}
    \mathbb{P}_{\theta}\left(T_{1}(\mathbf{X})>\theta\right)=\cfrac{1-\alpha}{2} \\
    \mathbb{P}_{\theta}\left(T_{2}(\mathbf{X})<\theta\right)=\cfrac{1-\alpha}{2}
\end{gather*}
\end{defn}

Построение доверительного интервала с помощью точечной оценки:

\begin{enumerate}
    \item Пусть $T(\mathbf{X})$~--- точечная оценка $\theta$. Обозначим $H(t, \theta)=\mathbb{P}_{\theta}(T(\mathbf{X})<t)$. $H(t,\theta)$~--- непрерывная и строго монотонная функция $\theta$ при любом фиксированном $t$. В этом случае
    \begin{equation*}
        \left\{\begin{array}{l}
            \mathbb{P}_{\theta}\left(T(\mathbf{X})>a_{1}(\theta)\right)
            = \cfrac{1-\alpha}{2} \\ 
            \mathbb{P}_{\theta}\left(T(\mathbf{X})<\alpha_{2}(\theta)\right)
            = \cfrac{1-\alpha}{2}
        \end{array}\right. 
        \Leftrightarrow 
        \left\{\begin{array}{l}
            1 - H(\alpha_{1}(\theta), \theta)=\cfrac{1-\alpha}{2} \\ 
            H(\alpha_{2}(\theta), \theta)=\cfrac{1-\alpha}{2}
        \end{array}\right.
    \end{equation*}
    
    \item Рассмотрим вспомогательную лемму.
    \begin{lem}
        Если $H(t, \theta)$ возрастает по $\theta$, то $\alpha_{1}(\theta)$ и $\alpha_{2}(\theta)$ убывают. Если же $H(t, \theta)$ убывает по $\theta$, то $\alpha_{1}(\theta)$ и $\alpha_{2}(\theta)$ возрастают.
    \end{lem}
    \begin{proof}
        Пусть $H(t, \theta)$ возрастает. Предположим, что $\theta_{1}<\theta_{2} \Rightarrow \alpha_{2}\left(\theta_{1}\right) \leq \alpha_{2}\left(\theta_{2}\right)$ и рассмотрим $a_{2}(\theta)$, учитывая, что $H(t, \theta)$, как и всякая функция распределения, неубывает по первому аргументу:
        \begin{equation*}
            \frac{1-a}{2} 
            = H\left(\alpha_{2}\left(\theta_{1}\right) \theta_{1}\right)
            < H\left(\alpha_{2}\left(\theta_{1}\right) \theta_{2}\right) 
            \leq H\left(\alpha_{2}\left(\theta_{2}\right) \theta_{2}\right)
            = \frac{1-\alpha}{2}
        \end{equation*}
        Полученное противоречие завершает доказательство.
    \end{proof}
    \item Из леммы следует, что для любого $\theta$
    \begin{equation*}
    \begin{aligned}
        \alpha_{1}(\theta) 
        < T(\mathbf{X})
        \Leftrightarrow \theta>\varphi_{1}(T(\mathbf{X}))
        \Rightarrow \mathrm{P}_{\theta}(\theta>\varphi_{1}(T(\mathbf{X})))
        = \cfrac{1-\alpha}{2} \\
        \alpha_{2}(\theta)>T(\mathbf{X}) 
        \Leftrightarrow \theta<\varphi_{2}(T(\mathbf{X})) \Rightarrow \mathrm{P}_{\theta}(\theta<\varphi_{2}(T(\mathbf{X})))
        = \cfrac{1-\alpha}{2} \\
        \Rightarrow P_{\theta}(\underbrace{\varphi_{2}(T(\mathbf{X}))}_{T_{1}(\mathbf{X})} 
        \leq \theta 
        \leq \underbrace{\varphi_{1}(T(\mathbf{X}))}_{T_{2}(\mathbf{X})})
        = \alpha
    \end{aligned}
    \end{equation*}

\end{enumerate}

\section{Проверка гипотез. Лемма Неймана—Пирсона}

\begin{defn}
{\it Гипотеза $H$}~--- любое предположение о распределении наблюдаемой случайной величины: $H=\left\{\mathcal{F}=\mathcal{F}_{1}\right\}$ или $H=\{\mathcal{F} \in \mathbb{F}\}$, где $\mathbb{F}$~--- некоторое подмножество в множестве всех распределений. Гипотеза называется {\it простой} в первом случае, {\it сложной} во втором. Если гипотез всего две, то одну из них принято называть {\it основной}, а другую~--- {\itальтернативой}.
\end{defn}

\begin{rmrk} Типичные задачи проверки гипотез:
\begin{enumerate}
    \item Гипотезы о виде распределения;
    \item Гипотезы о проверке однородности выборки: дано несколько выборок; основная гипотезасостоит в том, что эти выборки извлечены из одного распределения;
    \item Гипотеза независимости: по выборке $(X_1,Y_1), \ldots, (X_n,Y_n)$ из $n$ независимых наблюдений пары случайных величин проверяется гипотеза $H_{1}=\left\{X_{i} \text { и } Y_{i} \text { независимы }\right\}$ при альтернативе $H_{1}=\left\{H_{1} \text { неверна }\right\}$. Обе гипотезы являются сложными;
    \item Гипотеза случайности: в эксперименте наблюдаются $n$ случайныхвеличин $X_{1}, \ldots, X_{n}$ и проверяется сложная гипотеза $H_{1}=\left\{X_{1}, \ldots, X_{n}~ \text{независимы и одинаково распределены}\right\}$
\end{enumerate}
\end{rmrk}

Пусть дана выборка $X_{1}, \ldots, X_{n}$, относительно распределения которой выдвинуты две простые гипотезы $H_{0}$ и $H_1$.
\begin{defn}
{\it Критерий}~--- правило, согласно которому гипотеза $H_0$ принимается или отвергается.
\end{defn}
Выборка ($\mathbf{X} = X_1, \ldots, X_n$) объёма $n$~--- точка в пространстве $\mathbb{R}^{n}$. Выделим множество $S \subset \mathbb{R}^{n}$~--- {\it критическую область} для гипотезы $H_0$. В этом случае критерий можно сформулировать следующим образом:
\begin{itemize}
    \item $\varphi(x) = 1 \Rightarrow$ отвергаем $H_0$, принимаем $H_1$;
    \item $\varphi(x) = 0 \Rightarrow$ отвергаем $H_1$, принимаем $H_0$;
\end{itemize}

\begin{defn}
Говорят, что произошла {\it ошибка 1-го рода}, если критерий отверг верную гипотезу $H_0$. Вероятность ошибки 1-го рода (или {\it уровень значимости критерия}): 
\begin{equation*}
    \alpha(S)=P\left\{\mathbf{X} \in S ~|~ H_{0}\right\}=P_{0}\left\{\mathbf{X} \in S\right\}
\end{equation*}
Аналогично вероятность ошибки 2-го рода:
\begin{equation*}
    \beta(S)=P\left\{\mathbf{X} \notin S ~|~ H_{1}\right\}=P_{1}\left\{\mathbf{X} \notin S\right\}
\end{equation*}
\end{defn}

\begin{defn}
{\it Мощность критерия}:
\begin{equation*}
    \gamma(S)=1-\beta(S)=P_{1}\left\{\mathbf{X} \in S\right\}
\end{equation*}
\end{defn}

\begin{center}
\begin{tabular}{|c|c|c|}
\hline \multirow{2}{*} { Истинная гипотеза } & \multicolumn{2}{|c|} { Результат принятия решения } \\
\cline {2-3} & $H_{0}$ отклонена & $H_{0}$ принята \\
\hline$H_{0}$ & $\alpha$ & $1-\alpha$ \\
\hline$H_{1}$ & $1-\beta$ & $\beta$ \\
\hline
\end{tabular}
\end{center}

Если $\gamma(S)<\alpha(S)$, то попасть в $S$ при условии истинности гипотезы $H_1$ труднее, чем при условии истинности гипотезы $H_0$, т.е. $S$~--- критическая область скорее для $H_1$. Следовательно, неравенство должно иметь вид $\gamma(S)>\alpha(S)$.

\begin{defn}
    Критерий называется {\it несмещённым}, если выполняется условие
    \begin{equation*}
        \alpha(S) \leqslant \gamma(S)=1-\beta(S)
    \end{equation*}
\end{defn}

Зададим $\alpha_0$ и и будем иметь дело только с такими критериями, где $\alpha_{0} \geqslant \alpha(S)$ (т.е. вероятность ошибки первого рода не превосходитвеличины $\alpha_0$) и дополнительно будем решать задачу $\beta(S) \rightarrow \min\limits_{S}$.

Получаем две эквивалентные задачи определения критической области $S$:
\begin{equation*}
    \begin{array}{l}
    \left\{\begin{array}{l}
    \alpha_{0} \geqslant \alpha(S) \\
    \beta(S) \rightarrow \min\limits_{S}
    \end{array}\right.
    \Leftrightarrow~
    \left\{\begin{array}{l}
    \alpha_{0} \geqslant \alpha(S) \\
    \gamma(S) \rightarrow \max\limits_{S}
    \end{array}\right.
    \end{array}
\end{equation*}

Задачи в такой постановке не всегда решаемы, так как требуетсяответить точно <<да>> или <<нет>>. Такие статистические критерииназываются {\it нерандомизированными критериями}.

\begin{exmp}
Рассмотрим {\it критическую функцию} $\varphi(x)=I\{x \in S\}$. Тогда критерий примет вид:
\begin{itemize}
    \item Если $\varphi\left(\mathbf{X}\right)=1$, тогда отвергаем гипотезу $H_0$, принимаем $H_1$.
    \item Если $\varphi\left(\mathbf{X}\right)=0$, тогда отвергаем гипотезу $H_1$, принимаем $H_0$.
\end{itemize}
\end{exmp}

\begin{exmp}
Рассмотрим другую критическую функцию $\varphi(x)=P\left\{\bar{H}_{0} / \mathbf{X}=x\right\}$. В этом случае $\varphi\left(\mathbf{X}\right) \in[0,1]$~--- условная вероятность отклонения гипотезы $H_0$. При таком определении $\varphi(x)$ приходим к {\itрандомизированному критерию}, то есть, критерию, который при некоторых значениях $s$ может не давать ответа <<да>> или <<нет>> в отношении истинности гипотезы $H_0$. Тогда формулировка критерия следующая:
\begin{itemize}
    \item с вероятностью $1 - \varphi\left(\mathbf{X}\right)$ следует принимать гипотезу $H_0$;
    \item с вероятностью $\varphi\left(\mathbf{X}\right)$ следует принимать гипотезу $H_0$ю
\end{itemize}
\end{exmp}

\begin{rmrk}
При использовании введенного обозначения вероятность ошибки первого рода, вероятность ошибки второго рода и мощность критерия будем обозначать: $\alpha(\varphi)$, $\beta(\varphi)$ и $\gamma(\varphi)=1-\beta(\varphi)$ соответственно.
\end{rmrk}

Без ограничения общности будем предполагать, что существует плотность $f_{0}(x)$ для функции распределения $F_{0}(x)$, и существует плотность $f_{1}(x)$ для функции распределения $F_{1}(x)$. В дискретном случае все результаты аналогичны.

Если верна гипотеза $H_1$, то функция правдоподобия выборки $X$ имеет вид:
\begin{equation*}
    L_{1}\left(\mathbf{X}\right)=\prod_{i=1}^{n} f_{1}\left(X_{i}\right)
\end{equation*}

Для рандомизированного критерия получаем
\begin{gather*}
    P_{0}\left(\bar{H}_{0}\right)=\int\limits_{\mathbb{R}^{n}} \varphi(x) L_{0}(x) \mu^{n}(d x)=\alpha(\varphi) \\
    P_{1}\left(H_{0}\right)=\int\limits_{\mathbb{R}^{n}}(1-\varphi(x)) L_{1}(x) \mu^{n}(d x)=\beta(\varphi) \\
    \gamma(\varphi)=\int\limits_{\mathbb{R}^{n}} \varphi(x) L_{1}(x) \mu^{n}(d x), \quad \gamma(\varphi)=1-\beta(\varphi)
\end{gather*}

Тогда задача построения статистического критерия сводится к нахождению критической функции $\varphi(x)$ и будет формулироваться следующим образом:
\begin{equation*}
    \begin{array}{l}
    \left\{\begin{array}{l}
    \alpha_{0} \geqslant \alpha(\varphi) \\
    \beta(\varphi) \rightarrow \min\limits_{\varphi}
    \end{array}\right.
    \Leftrightarrow
    \left\{\begin{array}{l}
    \alpha_{0} \geqslant \alpha(\varphi) \\
    \gamma(\varphi) \rightarrow \max\limits_{\varphi}
    \end{array}\right.
    \end{array}
\end{equation*}
Таким образом, задача заключается в том, чтобы найти наиболее мощный критерий, когда вероятность ошибки первого рода не превосходит некоторого заданного порогового значения. Решение сформулированных задач даётся леммой Неймана-Пирсона.

\begin{thm}[Лемма Неймана---Пирсона]
Пусть $\alpha_{0} \in(0,1)$, тогда при фиксированной вероятности ошибки первого рода $\alpha_{0}$ наиболее мощный критерий имеет критическую функцию $\varphi^{*}$ вида
\begin{equation*}
    \varphi^{*}(x)=\left\{\begin{array}{ll}
    1, & \text { если } L_{1}(x)>c L_{0}(x) \\
    \varepsilon, & \text { если } L_{1}(x)=c L_{0}(x) \\
    0, & \text { если } L_{1}(x)<c L_{0}(x)
    \end{array}\right.
\end{equation*}
где $L_{j}(x)=\prod_{i=1}^{n} f_{j}\left(x_{i}\right)$ соответствует гипотезе $H_j, j = \overline{1,2}$, константы $c$ и $\varepsilon$ являются решениями уравнения $\alpha\left(\varphi^{*}\right)=\alpha_{0}$.
\end{thm}

\begin{proof}
\begin{enumerate}
    \item Покажем, что константы $c$ и $\varepsilon$ могут быть найдены из уравнения $\alpha\left(\varphi^{*}\right)=\alpha_{0}$. Заметим, что
    
    \begin{equation*}
        \begin{aligned} \alpha(\varphi^{*})
        = P_{0}(L_{1}(\mathbf{X}) > c L_{0}(\mathbf{X})) 
        + \varepsilon P_{0}(L_{1}(\mathbf{X}) = c L_{0}(\mathbf{X}))=\\ 
        = P_{0}\left(\frac{L_{1}(\mathbf{X})}{L_{0}(\mathbf{X})} > c\right) 
        + \varepsilon P_{0}\left(\frac{L_{1}(\mathbf{X})}{L_{0}(\mathbf{X})} = c \right) 
        \end{aligned}
    \end{equation*}

Если предположить, что $L_{0}(\mathbf{X})=0$, то

\begin{equation*}
    P_{0}\left\{L_{0}(\mathbf{X}) = 0\right\} = \int\limits_{\left\{x: L_{0}(x)=0\right\}} L_{0}(x) \mu(d x)=0
\end{equation*}

и, следовательно, вышеприведённое равенство корректно. Поэтому рассмотрим случайную величину $\eta(\mathbf{X}) = \cfrac{L_{1}(\mathbf{X})}{L_{0}(\mathbf{X})}$

Положим $F_{H_{0}, \eta}(t)=P\{\eta \leqslant t\}$, тогда
\begin{equation*}
    \alpha\left(\varphi^{*}\right)=1-F_{H_{0}, \eta}(c)+\varepsilon\left(F_{H_{0}, \eta}(c)-F_{H_{0}, \eta}(c-0)\right)
\end{equation*}

Пусть $g(c)=1-F_{H_{0}, \eta}(c)$, константу $c_{\alpha_{0}}$ можно выбрать так, чтобы было выполнено неравенство:
\begin{equation*}
    g(c_{\alpha_{0}}) \leqslant \alpha_{0} \leqslant g(c_{\alpha_{0}}-0)
\end{equation*}

Тогда
\begin{equation*}
    \varepsilon_{\alpha_{0}} = 
    \left\{\begin{array}{ll}
         0, & \text{ если }  g\left(c_{\alpha_{0}}\right)=g\left(c_{\alpha_{0}}-0\right) \\
         \cfrac{\alpha_{0}-g\left(c_{\alpha_{0}}\right)}{g\left(c_{\alpha_{0}}-0\right)-g\left(c_{\alpha_{0}}\right)} \in [0,1], & \text{ если } g\left(c_{\alpha_{0}}\right)<g\left(c_{\alpha_{0}}-0\right)
    \end{array}\right.
\end{equation*}

В обоих случаях выполнено равенство:
\begin{equation*}
    \alpha_{0}=g\left(c_{\alpha_{0}}\right)+\varepsilon_{\alpha_{0}}\left(g\left(c_{\alpha_{0}}-0\right)-g\left(c_{\alpha_{0}}\right)\right)=\alpha\left(\varphi^{*}\right)
\end{equation*}

\item Докажем, что $\varphi^{*}(x)$~--- критическая функция наиболее мощного критерия.

Выберем любую другую критическую функцию $\tilde{\varphi}(x)$ такую, что $\alpha(\tilde{\varphi}) \leqslant \alpha_{0}$, и сравним ее с критической функцией $\varphi^{*}(x)$. Заметим, что для любого $x$ справедливо неравенство:
\begin{equation*}
    \left(\varphi^{*}(x)-\tilde{\varphi}(x)\right)\left(L_{1}(x)-c_{\alpha_{0}} L_{0}(x)\right) \geqslant 0
\end{equation*}

Тогда
\begin{equation*}
    \int\limits_{\mathbb{R}^{n}}\left(\varphi^{*}(x)-\tilde{\varphi}(x)\right)\left(L_{1}(x)-c_{\alpha_{0}} L_{0}(x)\right) \mu^{n}(d x) \geqslant 0
\end{equation*}

Раскроем скобки и преобразуем:

\begin{equation*}
    \begin{array}{l}
\int\limits_{\mathbb{R}^{n}} \varphi^{*}(x) L_{1}(x) \mu^{n}(d x)-\int\limits_{\mathbb{R}^{n}} \tilde{\varphi}(x) L_{1}(x) \mu^{n}(d x) \geqslant \\
\quad \geqslant c_{\alpha_{0}}\left(\int\limits_{\mathbb{R}^{n}} \varphi^{*}(x) L_{0}(x) \mu^{n}(d x)-\int\limits_{\mathbb{R}^{n}} \tilde{\varphi}(x) L_{0}(x) \mu^{n}(d x)\right) \geqslant 0
\end{array}
\end{equation*}

Следовательно, $\gamma\left(\varphi^{*}\right)-\gamma(\tilde{\varphi}) \geqslant c_{\alpha_{0}}\left(\alpha\left(\varphi^{*}\right)-\alpha(\tilde{\varphi})\right)$, откуда получаем неравенство:

\begin{equation*}
    \gamma\left(\varphi^{*}\right) \geqslant \gamma(\tilde{\varphi})
\end{equation*}
\end{enumerate}
\end{proof}

\section{Критерии согласия Колмогорова и $\chi^{2}$}

{\bf Критерий Колмогорова}

Выборка $X_1, \ldots, X_n$ имеет функцию распределения $F(x)$ из семейства распределений . Требуется проверить гипотезу $F(x)=F_{0}(x)$. Непараметрический критерий Колмогорова основан на статистике
\begin{equation*}
    D_{n}(\mathbf{X})=\sup _{X} | F_{n}(x)-F_{0}(x)
\end{equation*}
где $F_{0}(x)$  —  непрерывная функция распределения, а $F_{n}(x)$~---  эмпирическая функция распределения, построенная повыборке $X_1, \ldots, X_n$.

Из того, что если $\xi$~--- случайная величина, $F_{\xi}(x)$~--- непрерывна, то случайная величина $\eta=F_{\xi}(\xi)$ равномерно распределенана $[0,1]$, следует что при $F_{0}(x)=t$ вероятность $\mathbb{P}\left(D_{n}(\mathbf{X})<t\right)$ независит от $\theta$ и $F_{0}(x)$.

\begin{thm}
    Для любой непрерывной $F(x)$ при $x > 0$ выполняется
    \begin{equation*}
        \lim _{n \rightarrow \infty} P\left(\sqrt{n} D_{n}(\mathbf{X})<t\right)=K(t)=\sum_{j=-\infty}^{+\infty}(-1)^{-2 j^{2}+2}
    \end{equation*}
\end{thm}

На основе этого предельного соотношения строится непараметрический критерий Колмогорова. Пусть $\gamma_{\alpha}$~--- $\alpha$-квантиль предельного распределения $K(t)$:
\begin{equation*}
    1-K\left(\gamma_{\alpha}\right)=\alpha \Leftrightarrow \mathrm{P}\left(\sqrt{n} D_{n}(\mathbf{X}) \geq \gamma_{a} | H_{0}=\alpha\right)
\end{equation*}

Тогда гипотеза о том, что выборка взята из распределения с функцией $F_{0}(x)$ принимается, если $\sqrt{n} D_{n}(\mathbf{X}) \leq \gamma_{a}$. Уровень значимости этого критерия равен приближённо $\alpha$.

{\bf Критерий $\chi^{2}$}
Пусть имеетсявыборка $X_1, \ldots, X_n$ и требуется проверить гипотезу $H_{0}: F(x)=F_{0}(x)$. Разобьём числовую прямую на $m$ промежутков $\Delta_{1}, \Delta_{2}, \ldots, \Delta_{m-1}, \Delta_{m}$. Обозначим $V_{k}$ — число наблюдений, попавших в интервал $\Delta_{k}$. Тогда если $\xi_{i}^{(k)}=\mathrm{I}\left(X_{i} \in \Delta_{k}\right),$ тo $v_{k}=\sum_{i=1}^{n} \xi_{i}^{(k)}$.

При этом имеет место сходимость
\begin{equation*}
    \frac{v_k}{n} \xrightarrow[n \to \infty]{} P\left(X_{1} \in \Delta_{k}\right)=\int\limits_{\Delta_{k}} d F_{0}(x)=p_{k}
\end{equation*}

Строится статистика

\section{Статистические выводы о параметрах нормального распределения. Распределения $\chi^{2}$ и Стьюдента. Теорема Фишера}

\begin{defn}
    Пусть $\zeta_{1}, \ldots, \zeta_{k}$ взаимно независимые случайные величины, $\zeta_{k} \sim \mathbf{N}(0,1)$. Распределение случайной величины $\tau_{k}=\zeta_{1}^{2}+\ldots+\zeta_{k}^{2}$ называется распределением $\chi^{2}$ с $k$ степенями свободы.
\end{defn}
\begin{rmrk}
    Распределение $\chi^{2}$ с $k$ степенями свободы представляет собой гамма-распределение с параметрами формы $\frac{k}{2}$ и масштаба $\frac{1}{2}$:
    \begin{equation*}
    f_{\tau}(x)=\left\{\begin{array}{ll}
        \left(\frac{1}{2}\right)^{\frac{k}{2}} \frac{x^{\frac{k}{2}-1}}{\Gamma\left(\frac{k}{2}\right)} e^{-\frac{x}{2}}, & x>0 \\
        0, & x \leq 0
    \end{array}\right.
    \end{equation*}
\end{rmrk}

\begin{defn}
    Пусть заданы случайные величины $\zeta \sim \mathbf{N}(0,1)$ и $\tau_{k} \sim \chi_{k}^{2}$. Пусть случайные величины $\zeta$ и $\tau_{k}$ взаимно независимы. Распределение случайной величины 
    \begin{equation*}
        \xi=\frac{\zeta}{\sqrt{\frac{\tau_{k}}{k}}}
    \end{equation*}
    называется {\it распределением Стьюдента} с $k$ степенями свободы и обозначается через $T_{k}$.
\end{defn}

\end{document}
