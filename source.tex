\documentclass[oneside,final,14pt]{extreport}

\usepackage{cmap}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{relsize}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\usepackage{hhline}
\usepackage{multirow}

\usepackage{etoolbox}
    \makeatletter
    \patchcmd{\@makechapterhead}{\vspace*{50\p@}}{}{}{}% Removes space above \chapter head
    \patchcmd{\@makeschapterhead}{\vspace*{50\p@}}{}{}{}% Removes space above \chapter* head
    \makeatother

\newcommand\mydef[1]{{\bf Опр.}}
\newcommand\mynote[1]{{\bf Замеч.}}
\newcommand\myst[1]{{\bf Утв.}}
\newcommand\myqed[1]{{\bf Док-во.}}
\newcommand\myex[1]{{\bf Пример.}}
\newcommand\myprob[1]{{\mathbb{P}(#1)}}

\renewcommand{\qedsymbol}{$\blacksquare$}
\renewenvironment{proof}{{\bfseries Доказательство.}}{\qed}

\newtheorem{thm}{Теорема}[section]
\newtheorem{lem}[thm]{Лемма}
\newtheorem*{rmrk}{Замечание}
\theoremstyle{definition}
\newtheorem{defn}{Определение}[section]
\newtheorem*{exmp}{Пример}

\newenvironment{compactlist}{
\begin{list}{{$\bullet$}}{
\setlength\partopsep{0pt}
\setlength\parskip{0pt}
\setlength\parsep{0pt}
\setlength\topsep{0pt}
\setlength\itemsep{0pt}
}
}{
\end{list}
}

\setpapersize{A4}
\setmarginsrb{2cm}{1.5cm}{2cm}{1.5cm}{0pt}{0mm}{0pt}{13mm}
\linespread{1.05}

\usepackage{indentfirst}
\sloppy

\usepackage{graphicx} 

\begin{document}
\begin{titlepage}
    \centering
    \vfill
    {\scshape\large
        Московский государственный университет\\
        Факультет вычислительной математики и кибернетики\\
    }
    \vskip1cm
    {\scshape\huge
        Теория вероятностей.\\
        Математическая статистика\\
    }
    \vskip0.5cm
    {\upshape\large
        Рожков И., Рыгин А.
    }    
    \vfill
    \includegraphics[width=8cm]{pic.png}
    \vfill
    {\upshape\large
        Москва\\
        ~2020
    }
\end{titlepage}

\tableofcontents
\chapter{Теория вероятностей}

\section{Центральная предельная теорема}
\begin{thm}[Центральная предельная теорема]
    Пусть $\xi_{1}, \xi_{2}, \ldots$~--- последовательность независимых одинаково распределенных (невырожденных) случайных величин с $\mathrm{E} X_{1}^{2}<\infty$ и $S_{n}=X_{1}+\ldots+X_{n}$. Тогда
    \begin{equation*}
        \mathbb{P}\left(\frac{S_{n}-\mathrm{E} S_{n}}{\sqrt{\mathrm{D} S_{n}}} \leqslant x\right)
        \xrightarrow[n \rightarrow \infty]{}
        \Phi(x) = \frac{1}{\sqrt{2 \pi}} \int\limits_{-\infty}^{x} e^{-\frac{z^{2}}{2}} dz~ \forall x \in \mathbb{R}
    \end{equation*}
\end{thm}
\begin{proof}
Пусть $\mathrm{E} X_{1}=m, \mathrm{D} X_{1}=\sigma^{2}$ и $\varphi(t)=\mathrm{E} e^{i t\left(X_{1}-m\right)}$. Введём
\begin{equation*}
    \varphi_{n}(t)=\mathrm{E} e^{i t \frac{S_{n}-\mathrm{E} S_{n}}{\sqrt{D S_{n}}}} = \
    \left[\varphi\left(\frac{t}{\sigma \sqrt{n}}\right)\right]^{n}
\end{equation*}

В силу разложения характеристической функции
\begin{equation*}
    \varphi_{X}(t)=1+i t \mathrm{E} X+\ldots+\frac{(i t)^{n}}{n !} \mathrm{E} X^{n}+R_{n}(t)
\end{equation*}
при $n=2$ получим 
\begin{equation*}
    \varphi(t)=1-\frac{\sigma^{2} t^{2}}{2}+\bar{o}\left(t^{2}\right), \quad t \rightarrow 0
\end{equation*}
Следовательно, для любого $t \in \mathbb{R}$ при $n \rightarrow \infty$
\begin{equation*}
    \varphi_{n}(t)=\left[1-\frac{\sigma^{2} t^{2}}{2 \sigma^{2} n}+\bar{o}\left(\frac{1}{n}\right)\right] \rightarrow e^{-\frac{t^{2}}{2}}
\end{equation*}

Функция $e^{-\frac{t^{2}}{2}}$ является характеристической функцией стандартного нормальногораспределения. В силу теорем о непрерывном соответсвии между функциями распределения и характеристическими функциями центральная предельная теорема доказана.
\end{proof}

\chapter{Математическая статистика}

\section{Статистическая структура. Выборка. Статистика. Порядковые статистики. Вариационный ряд. Эмпирическая функция распределения}

\begin{defn}
{\it Статистическая структура}~--- совокупность $(\Omega, \mathcal{A}, \mathcal{F})$, где $\Omega$~---множество элементарных исходов, $\mathcal{A}$~--- $\sigma$-алгебра событий, $\mathcal{F}$~--- семейство вероятностных мер, определённых на $\mathcal{A}$, параметризованное одно- или многомерным числовым параметром: $\mathcal{F} = (\mathcal{F}_{\theta}~|~\theta \in \Theta \subset R^{m})$.
\end{defn}

\begin{defn}
{\it Выборка} $\mathbf{X} = (X_{1}, \ldots, X_{n})$ объёма $n$~--- набор из $n$ независимых и одинаково распределённых случайных величин, имеющих такое же распределение, как и наблюдаемая случайная величина $\xi$.
\end{defn}

До того, как эксперимент проведён, выборка~--- набор случайных величин, после~--- набор чисел из множества возможных значений случайной величины. Числовой набор $\mathbf{X}(\omega_0) = (X_{1}(\omega_0), \ldots, X_{n}(\omega_0)) = (x_1, \ldots, x_n)$~--- {\it реализация выборки} на элементарном исходе $\omega_0$.

\begin{defn}
{\it Статистика}~--- измеримая функция от выборки $T(\mathbf{X})$.
\end{defn}

\begin{defn}
{\it Вариационный ряд}~--- выборка $X_{1}, \ldots, X_{n}$, упорядоченная по возрастанию на каждом элементарном исходе.
\end{defn}
Вариационный ряд строится следующим образом:
\begin{gather*}
    X_{(1)}(\omega)=\min (X_{1}(\omega), \ldots, X_{n}(\omega)) \\
    X_{(k)}(\omega)=\{\forall \omega \in \Omega \Rightarrow \exists m \leq i_{1}, \ldots, i_{k-1}, i_{k}, i_{k+1}, \ldots, i_{n} \leq n, i_{j} \neq i_{m}(j \neq m): \\ 
    X_{(k)}(\omega)=X_{i_{k}}(\omega) \\
    X_{i_{1}}(\omega), \ldots, X_{i_{k-1}}(\omega) \leq X_{i_{k}}(\omega); X_{i_{k+1}}(\omega), \ldots, X_{i_{n}}(\omega)>X_{i_{k}}(\omega)\}, 2 \leq k \leq n-1 \\
    X_{(n)}(\omega)=\max (X_{1}(\omega), \ldots, X_{n}(\omega))
\end{gather*}

Элемент $X_{(k)}$~--- {\it $k$-я порядковая статистика}.

\begin{defn}
{\it Эмпирическая функция распределения}, построенная по выборке $X_{1}, \ldots, X_{n}$ объёма $n$~--- случайная функция $F_{n}^{*}: \mathbb{R} \times \Omega \rightarrow[0,1]$, при каждом $y \in \mathbb{R}$ равная:
$F_{n}^{*}(y) =\frac{1}{n} \sum_{i=1}^{n} \mathrm{I}\left(X_{i}<y\right)$
\end{defn}

Эмпирическая функция распределенния строится по вариационному ряду следующим образом:

\begin{equation*}
    F_{n}^{*}(y)=\left\{\begin{array}{ll}
    0, & \text { если } y \leqslant X_{(1)} \\
    \frac{k}{n}, & \text { если } X_{(k)}<y \leqslant X_{(k+1)} \\
    1 & \text { при } y>X_{(n)}
    \end{array}\right.
\end{equation*}

\begin{exmp}
Найдём эмпирические функции распределения для крайних порядковых статистик.

\begin{gather*}
    \begin{aligned}
        F_{(1)}(x)=\mathbb{P}(X_{(1)} < x) 
    = 1 - \mathbb{P} (\mathrm{X}_{(1)} \geq x) 
    = 1 - \mathbb{P}(x_{1} \geq x, \ldots, x_{n} \geq x) = \\
    = 1 - \prod_{i=1}^{n} \mathbb{P}(x_{i} \geq x) 
    = 1 - (\mathbb{P}({x}_{1} \geq x))^{n} 
    = 1 - (1 - F(x))^{n} 
    \end{aligned} \\
    \begin{aligned}
        F_{(n)}(x) 
        = \mathbb{P}(X_{(n)} < x) 
        = \mathbb{P}(x_{1} < x, \ldots, x_{n} < x) = \\
        = \prod_{i=1}^{n} \mathbb{P}(x_{i} < x) 
        = (\mathbb{P}({x}_{1} < x))^{n} 
        = F^{n}(x)
    \end{aligned}
\end{gather*}
\end{exmp}

\begin{thm} Свойства эмпирической функции распределения:
\begin{enumerate}
    \item Пусть $X_{1}, \ldots, X_{n}$~--- выборка из распределения $\mathcal{F}$ с функцией распределения $F$ и пусть $F_{n}^{*}$ — эмпирическая функция распределения, построенная по этой выборке. Тогда $F_{n}^{*}(y) \xrightarrow[n \to \infty]{\mathrm{p}} F(y)$ для любого $y \in \mathbb{R}.$
    \item Для любого y $\in \mathbb{R}$:
    \begin{enumerate}[label={\arabic*)}]
        \item $\mathbb{E} F_{n}^{*}(y)=F(y)$, т.е. $F_{n}^{*}(y)$~--- несмещённая оценка для $F(y)$.
        \item $\mathbb{D} F_{n}^{*}(y)=\cfrac{F(y)(1-F(y))}{n}$
        \item $\sqrt{n}(F_{n}^{*}(y)-F(y)) \Rightarrow \mathbf{N}(0, (1-F(y))F(y))$, т.е. $F_{n}^{*}(y)$~--- асимптотически нормальная оценка для $F(y)$.
        \item $n F_{n}^{*}(y) \sim \mathbf{B}(n, F(y))$
    \end{enumerate}
\end{enumerate}
\end{thm}
\pagebreak
\begin{proof}\leavevmode
\begin{enumerate}
    \item $F_{n}^{*}(y)=\frac{1}{n} \sum_{i=1}^{n} \mathrm{I}(X_{i}<y)$, при этом случайные величины $\mathrm{I}(X_{1}<y),~ \mathrm{I}(X_{2}<y), \ldots$ независимы и одинаково распределены, их математическое ожидание конечно:
    \begin{equation*}
        \mathbb{E}\mathrm{I}(X_{1}<y)=1 \cdot \mathrm{P}(X_{1}<y)+0 \cdot \mathrm{P}(X_{1} \geqslant y)=\mathrm{P}(X_{1}<y)=F(y)<\infty
    \end{equation*}
    Следовательно, применим ЗБЧ в форме Хинчина:
    \begin{equation*}
        F_{n}^{*}(y)=\cfrac{\sum_{i=1}^{n} \mathrm{I}(X_{i}<y)}{n} \xrightarrow[]{\mathrm{p}} \mathbb{E}\mathrm{I}(X_{1}<y)=F(y) 
    \end{equation*}
    \item Заметим:
    \begin{gather*}
        \mathrm{I}(X_{1}<y) \sim  \mathbf{Bi}(F(y)) \Rightarrow \mathbb{E}\mathrm{I}(X_{1}<y) = F(y) \\
        \mathbb{D}\mathrm{I}(X_{1}<y) = F(y)(1-F(y))
    \end{gather*}
    \begin{enumerate}[label={\arabic*)}]
        \item Случайные величины $\mathrm{I}(X_{i}<y)$ одинаково распределены, поэтому:
        \begin{equation*}
            \mathbb{E} F_{n}^{*}(y)=\mathbb{E} \cfrac{\sum_{i=1}^{n} \mathrm{I}(X_{i}<y)}{n}=\cfrac{\sum_{i=1}^{n} \mathbb{E}\mathrm{I}(X_{i}<y)}{n}=\cfrac{n \mathbb{E}\mathrm{I}(X_{1}<y)}{n}=F(y)  
        \end{equation*}
        
        \item Случайные величины $\mathrm{I}(X_{i}<y)$ независимы и одинаково распределены, поэтому:
        \begin{multline*}
            \mathbb{D}\mathrm{I}_{n}^{*}(y)
            = \mathbb{D} \cfrac{\sum_{i=1}^{n} \mathrm{I}(X_{i}<y)}{n}
            = \cfrac{\sum_{i=1}^{n} \mathbb{D}\mathrm{I}(X_{i}<y)}{n^{2}}
            = \\
            = \cfrac{n\mathbb{D}\mathrm{I}(X_{1}<y)}{n^{2}}
            = \cfrac{F(y)(1-F(y))}{n}
        \end{multline*}
    
        \item Применим ЦПТ:
        \begin{multline*}
            \sqrt{n}\left(F_{n}^{*}(y)-F(y)\right)
            = \sqrt{n}\left(\cfrac{\sum \mathrm{I}(X_{i}<y)}{n}-F(y)\right) 
            = \\
            = \cfrac{\sum_{i=1}^{n} \mathrm{I}(X_{i}<y)-n F(y)}{\sqrt{n}} 
            = \cfrac{\sum_{i=1}^{n} \mathrm{I}(X_{i}<y)-n \mathbb{E}\mathrm{I}(X_{1}<y)}{\sqrt{n}} 
            \Rightarrow \\
            \Rightarrow \mathbf{N}(0, \mathbb{D}\mathrm{I}(X_{1}<y))
            = \mathbf{N}(0, (1-F(y))F(y))
        \end{multline*}
        \item Следует из устойчивости по суммированию биномиального распределения. Поскольку $\mathrm{I}\left(X_{i}<y\right)$ независимы и имеют распределение Бернулли $\mathbf{Bi}(F(y))$, то их сумма
        \begin{equation*}
            n F_{n}^{*}(y)=\mathrm{I}\left(X_{1}<y\right)+\ldots+\mathrm{I}\left(X_{n}<y\right)
        \end{equation*}
        имеет биномиальное распределение $\mathbf{B}(n, F(y))$.
    \end{enumerate}
\end{enumerate}  
\end{proof}

\section{Выборочные моменты. Их свойства}

Рассмотрим случайную величину $\xi^{*}$ с эмпирическим распределением, введём для последнего числовые характеристики.

\begin{defn}
{\it Выборочное математическое ожидание:} 
\begin{equation*}
    \tilde{\mathbb{E}} \xi^{*}=\sum_{i=1}^{n} \frac{1}{n} X_{i}=\frac{1}{n} \sum_{i=1}^{n} X_{i}=\overline{X}
\end{equation*}

Выборочное матожидание функции $g(\xi^{*})$:
\begin{equation*}
    \tilde{\mathbb{E}} g\left(\xi^{*}\right)=\frac{1}{n} \sum_{i=1}^{n} g\left(X_{i}\right)=\overline{g(\mathbf{X})}
\end{equation*}
\end{defn}

\begin{defn}
{\it Выборочная дисперсия:}
\begin{equation*}
    \tilde{\mathbb{D}} \xi^{*}=\sum_{i=1}^{n} \frac{1}{n}(X_{i}-\tilde{\mathbb{E}} \xi^{*})^{2}=\frac{1}{n} \sum_{i=1}^{n}(X_{i}-\overline{X})^{2}=S^{2}
\end{equation*}
\end{defn}

\begin{defn}
{\it Несмещённая выборочная дисперсия:} 
\begin{equation*}
    S_{0}^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}
\end{equation*}
\end{defn}

\begin{defn}
{\it Выборочный момент $k$-го порядка:}
\begin{equation*}
    \tilde{\mathbb{E}}(\xi^{*})^{k}=\sum_{i=1}^{n} \frac{1}{n} X_{i}^{k}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{k}=\overline{X^{k}}
\end{equation*}
\end{defn}

Все вышеперечисленные характеристики являются случайными величинами как функции от выборки $X_{1}, \ldots, X_{n}$ и оценками для истинных моментов искомого распределения.

\begin{thm}
Выборочное среднее $\overline{X}$ является несмещённой, состоятельной и асимптотически нормальной оценкой для теоретического среднего (математического ожидания):

\begin{enumerate}[label={\arabic*.}]
    \item Если $\mathbb{E}|X_{1}|<\infty$, то $\mathbb{E}\overline{X}=\mathbb{E} X_{1}=a$
    \item Если $\mathbb{E}|X_{1}|<\infty$, то $\overline{X} \xrightarrow[]{\mathrm{p}} \mathbb{E} X_{1}=a$ при $n \rightarrow \infty$.
    \item Если $\mathbb{D} X_{1}<\infty, \quad \mathbb{D} X_{1} \neq 0$, то $\sqrt{n}(\overline{X}-\mathbb{E} X_{1}) \Rightarrow \mathbf{N}(0, \mathbb{D} X_{1})$.
\end{enumerate}
\end{thm}

\begin{proof}
\begin{enumerate}[label={\arabic*.}]
    \item $\mathbb{E} \overline{X}=\frac{1}{n}(\mathbb{E} X_{1}+\ldots+\mathbb{E} X_{n})=\frac{1}{n} \cdot n \mathbb{E} X_{1}=\mathbb{E} X_{1}=a$
    \item Из ЗБЧ в форме Хинчина:
    \begin{equation*}
        \overline{X}
        = \cfrac{X_{1}+\ldots+X_{n}}{n} \xrightarrow[]{\mathrm{p}} \mathbb{E} X_{1} 
        = a
    \end{equation*}

    \item Из ЦПТ:
    \begin{equation*}
        \sqrt{n}\left(\overline{X}-\mathbb{E} X_{1}\right) 
        = \cfrac{\sum_{i=1}^{n} X_{i}-n \mathbb{E} X_{1}}{\sqrt{n}} 
        \Rightarrow \mathbf{N}(0, \mathbb{D} X_{1})
    \end{equation*}
\end{enumerate}
\end{proof}

\begin{rmrk}
    Аналогичными свойствами обладает выборочный $k$-й момент, являющийся несмещённой, состоятельной и асимптотически нормальной оценкой для теоретического $k$-го момента.
\end{rmrk}

\begin{thm}
Пусть $\mathbb{D} X_{1}<\infty$.
\begin{enumerate}
    \item Выборочные дисперсии $S^{2}$ и $S^{2}_0$ являются состоятельными оценками для истинной дисперсии:
    \begin{equation*}
        S^{2} \xrightarrow[]{\mathrm{p}} \mathbb{D} X_{1}=\sigma^{2}, \quad S_{0}^{2} \xrightarrow[]{\mathrm{p}} \mathbb{D} X_{1}=\sigma^{2}
    \end{equation*}
    \item Величина $S^{2}$~--- смещённая оценка дисперсии, а $S^{2}_0$~--— несмещённая:
    \begin{equation*}
        \mathbb{E} S^{2}=\frac{n-1}{n} \mathbb{D} X_{1}=\frac{n-1}{n} \sigma^{2} \neq \sigma^{2}, \quad \mathbb{E} S_{0}^{2}=\mathbb{D} X_{1}=\sigma^{2}
    \end{equation*}
    
    \item Если $0 \neq \mathbb{D}(X_{1}-\mathbb{E}X_{1})^{2}<\infty$, то $S^{2}$ и $S^{2}_0$ являются асимптотически нормальными оценками истинной дисперсии:
    \begin{equation*}
        \sqrt{n}\left(S^{2}-\mathbb{D} X_{1}\right) \Rightarrow \mathbf{N}(0, \mathbb{D}(X_{1}-\mathbb{E} X_{1})^{2})
    \end{equation*}
\end{enumerate}
\end{thm}

\begin{proof}
\begin{enumerate}
    \item $S^{2}=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}=\overline{X^{2}}-(\overline{X})^{2}$

    Используя состоятельность первого и второго выборочных моментов и свойства сходимости по вероятности, получаем:
    \begin{gather*}
        S^{2}=\overline{X^{2}}-(\overline{X})^{2} \xrightarrow[]{\mathrm{p}} \mathbb{E} X_{1}^{2}-\left(\mathbb{E} X_{1}\right)^{2}=\sigma^{2} \\
        \cfrac{n}{n-1} \rightarrow 1 \Rightarrow S_{0}^{2}=\frac{n}{n-1} S^{2} \xrightarrow[]{\mathrm{p}} \sigma^{2}
    \end{gather*}
    
    \item Используя несмещённость первого и второго выборочных моментов:
    \begin{multline*}
        \mathbb{E} S^{2} = \mathbb{E}\left(\overline{X^{2}}-(\overline{X})^{2}\right)
        = \mathbb{E} \overline{X^{2}}-\mathbb{E}(\overline{X})^{2}
        = \mathbb{E} X_{1}^{2}-\mathbb{E}(\overline{X})^{2} = \\
        = \mathbb{E} X_{1}^{2}-\left((\mathbb{E} \overline{X})^{2}+\mathbb{D} \overline{X}\right)
        = \mathbb{E} X_{1}^{2}-\left(\mathbb{E} X_{1}\right)^{2}-\mathbb{D}\left(\frac{1}{n} \sum_{i=1}^{n} X_{i}\right) = \\
        = \sigma^{2}-\frac{1}{n^{2}} n \mathbb{D} X_{1}
        = \sigma^{2}-\frac{\sigma^{2}}{n}
        = \frac{n-1}{n} \sigma^{2}
    \end{multline*}
    
    Откуда следует:
    \begin{equation*}
        \mathbb{E} S_{0}^{2}=\frac{n}{n-1} \mathbb{E} S^{2}=\sigma^{2}
    \end{equation*}
    
    \item Введём случайные величины $Y_{i}=X_{i}-a$; $\mathbb{E}Y_{i} = 0, \mathbb{D} Y_{1}=\mathbb{D} X_{1}=\sigma^{2}$.
    \begin{gather*}
        S^{2}=\frac{1}{n} \sum_{i=1}^{n}(X_{i}-\overline{X})^{2}=\frac{1}{n} \sum_{i=1}^{n}(X_{i}-a-(\overline{X}-a))^{2}=\overline{Y^{2}}-(\overline{Y})^{2} \\
        \begin{aligned}
            \sqrt{n}(S^{2}-\sigma^{2}) = \sqrt{n}(\overline{Y^{2}}-(\overline{Y})^{2}-\sigma^{2})
            = \sqrt{n}t(\overline{Y^{2}}-\mathbb{E} Y_{1}^{2})-\sqrt{n}(\overline{Y})^{2} = \\
            =\frac{\sum_{i=1}^{n} Y_{i}^{2}-n \mathbb{E} Y_{1}^{2}}{\sqrt{n}}-\overline{Y} \cdot \sqrt{n} \overline{Y} \Rightarrow \mathbf{N}(0, \mathbb{D}(X_{1}-a)^{2})
    \end{aligned}
    \end{gather*}
    ...поскольку $\cfrac{\sum_{i=1}^{n} Y_{i}^{2}-n \mathbb{E} Y_{1}^{2}}{\sqrt{n}} \Rightarrow \mathbf{N}(0, \mathbb{D} Y_{1}^{2})$ по ЦПТ, а $\overline{Y} \cdot \sqrt{n} \overline{Y} \Rightarrow 0$ как произведение последовательностей $\overline{Y} \xrightarrow[n \rightarrow \infty]{p} 0$ и $\sqrt{n} \overline{Y} \Rightarrow \mathbf{N}(0, \mathbb{D} X_{1})$.
\end{enumerate}
\end{proof}

\section{Точечная оценка. несмещённость, состоятельность, оптимальность. Теорема о единственности оптимальной оценки}
\begin{defn}
{\it Статистика} или {\it оценка}~--- измеримая функция от выборки $T(\mathbf{X})$.
\end{defn}

\begin{defn}
{\it Несмещённая оценка} парамаетра $\theta$~--- статистика $T(\mathbf{X})$, т.ч. $\forall \theta \in \Theta: \mathbb{E}T(\theta) = \theta$.
\end{defn}

\begin{defn}
{\it Асимптотически несмещённая оценка} параметра $\theta$~--- статистика $T(\mathbf{X})$, т.ч. $\forall \theta \in \Theta: \mathbb{E}T(\theta) \xrightarrow[n \rightarrow \infty]{} \theta$.
\end{defn}

\begin{defn}
{\it Состоятельная оценка} параметра $\theta$~--- статистика $T(\mathbf{X})$, т.ч. $\forall \theta \in \Theta: T(\theta) \xrightarrow[n \rightarrow \infty]{p} \theta$.
\end{defn}

Оценки также могут вводиться и для функций $\tau(\theta)$ параметра $\theta$; они обладают всеми аналогичными свойствами.

Несмещённость означает отсутствие ошибки «в среднем», т. е. при систематическом использовании данной оценки. Несмещённость является желательным, но не обязательным свойством оценок. Достаточно, чтобы смещение оценки (разница между её средним значением и истинным параметром) уменьшалось с ростом объёма выборки. Поэтому асимптотическая несмещённость является весьма желательным свойством оценок. Свойство состоятельности означает, что последовательность оценок приближается к неизвестному параметру при увеличении количества наблюдений. В отсутствие этого свойства оценка совершенно «несостоятельна» как оценка.

\begin{rmrk}
    Отметим некоторые свойства несмещённых и состоятельных оценок.
    \begin{enumerate}
        \item несмещённые оценки не единственны.
        
        К примеру в качестве несмещённой оценки для математического ожидания $\mathbb{E} X$ могут выступать $\mathbb{E} X_{1}$ или$\mathbb{E} \overline(\mathbf{X})$.
        
        \item несмещённые оценки могут не существовать.
        \begin{exmp}
            Дано распределение $\operatorname{Pois}(\theta)$, над которым произведено одно наблюдение. Найти несмещённую оценку для функции $\tau(\theta) = \cfrac{1}{\theta}$.
                \begin{equation*}
                    \mathbb{E}T(\theta) 
                    = \mathlarger{\mathlarger{\sum}}_{x=0}^{\infty}T(x)e^{-\theta}\cfrac{\theta^{x}}{x!} 
                    = \cfrac{1}{\theta}
                    \Rightarrow \mathlarger{\mathlarger{\sum}}_{x=0}^{\infty}T(x)\cfrac{\theta^{x+1}}{x!}
                    = \mathlarger{\mathlarger{\sum}}_{r=0}^{\infty}\cfrac{\theta^{r}}{r!}
                    \Rightarrow T(x) \equiv \cfrac{1}{\theta}
                \end{equation*}
            Т.к. полученная статистика зависит от $\theta$, искомой несмещённой оценки для $\tau(\theta)$ не существует.
        \end{exmp}
        
    \item несмещённые оценки могут существовать, но быть бессмысленными.
    \begin{exmp}
        Дано отрицательное биноминальное распределение $\operatorname{B}(1, \theta)$, над которым произведено одно наблюдение. Найти несмещённую оценку для параметра $\theta$.
        \begin{gather*}
            \mathbb{E}T(\theta) 
            = \sum_{x=0}^{\infty}T(x)\theta^{x} 
            = \cfrac{\theta}{1-\theta} 
            = \sum_{r=1}^{\infty}\theta^{r} \\
            T(x) = 
            \left\{\begin{array}{ll}
                0, & \text { если } x = 0 \\
                1, & \text { если } x \geq 1
            \end{array}\right.
        \end{gather*}
    Значения этой статистики не принадлежат параметрическому множеству $\Theta = (0, 1)$, следовательно, эта оценка бессмысленна.
    \end{exmp}
    
    \item Состоятельные оценки не единственны.
    
    К примеру, выборочная дисперсия $S^{2}$ и несмещённая выборочная дисперсия $S_0^{2}$ являются состоятельными оценками теоретической дисперсии.
    
    \item Состоятельные оценки могут быть смещёнными.
    
    Как было показано ранее, выборочная дисперсия является состоятельной, но смещённой оценкой теоретической дисперсии
    
    \end{enumerate}
\end{rmrk}

Для несмещённой оценки $T(\mathbf{X})$ функции $\tau(\theta)$: $\mathbb{E}_{\theta}(T(\mathbf{X})-\tau(\theta))^{2}=\mathbb{D}_{\theta} T(\mathbf{X})$. Т.к. для двух разных оценок $T_1(\mathbf{X})$, $T_2(\mathbf{X})$ соответствующие дисперсии могут быть несравнимыми, введём понятие оптимальной оценки.

\begin{defn}
{\it Оптимальная оценка} функции $\tau(\theta)$~--- статистика $T(\mathbf{X})$, т.ч.:
\begin{enumerate}
    \item $T(\mathbf{X})$~--- несмещённая.
    \item $T(\mathbf{X})$ имеет равномерно минимальную дисперсию, т.е. для любой другой несмещённой оценки $T^{*}(\mathbf{X})$ функции $\tau(\theta)$: $\mathbb{D}_{\theta} T(\mathbf{X}) \leq \mathbb{D}_{\theta} T_{1}(\mathbf{X})~ \forall X$.
\end{enumerate}
\end{defn}
\pagebreak
\begin{thm}
Если существует оптимальная оценка функции $\tau(\theta)$, то она единственна.
\end{thm}

\begin{proof}
Предположим обратное: пусть существуют две оптимальные оценки $T_1(\mathbf{X})$ и $T_2(\mathbf{X})$ функции $\tau(\theta)$. Тогда в силу их несмещённости: $\mathbb{E}_{\theta} T_{1}(\mathbf{X})=\mathbb{E}_{\theta} T_{2}(\mathbf{X})=T(\theta)$, а а в силу того, что они имеют равномерно минимальную дисперсию: $\mathbb{D}_{\theta} T_{1}(\mathbf{X})=\mathbb{D}_{\theta} T_{2}(\mathbf{X})~ \forall \theta$.

Введём новую статистику: 
\begin{equation}
    T_{3}(\mathbf{X})=\cfrac{T_{1}(\mathbf{X})+T_{2}(\mathbf{X})}{2}
\end{equation}

Так как $\mathbb{E}_{\theta} T_{3}(\mathbf{X})=\cfrac{\mathbb{E}_{\theta} T_{1}(\mathbf{X})+\mathbb{E}_{\theta} T_{2}(\mathbf{X})}{2}=\tau(\theta)$, то $T_{3}(\mathbf{X})$~--- несмещённая оценка функции $\tau(\theta)$.

Имеем также:
\begin{equation*}
    \mathbb{D}_{\theta} T_{3}(\mathbf{X})=\cfrac{\mathbb{D}_{\theta}\left(T_{1}(\mathbf{X})+T_{2}(\mathbf{X})\right)}{4} =
    \cfrac{\mathbb{D}_{\theta} T_{1}(\mathbf{X})+\mathbb{D}_{\theta} T_{2}(\mathbf{X})+2 \operatorname{cov}\left(T_{1}(\mathbf{X}) T_{2}(\mathbf{X})\right)}{4}
\end{equation*}

В силу свойства
\begin{equation*}
    \mathbb{E}_{\theta} \xi^{2}<\infty, \mathbb{E}_{\theta} \eta^{2}<\infty \Rightarrow|\operatorname{cov}(\xi, \eta)| = | \mathbb{E}(\xi-\mathbb{E} \xi)(\eta-\mathbb{E} \eta)| \leq \sqrt{\mathbb{D} \xi} \sqrt{\mathbb{D} \eta},
\end{equation*}
где равенство достигается тогда и только тогда, когда $\xi=a \eta+b$, получаем:
\begin{equation*}
    \mathbb{D}_{\theta} T_{3}(\mathbf{X}) \leq \cfrac{\mathbb{D}_{\theta} T_{1}(\mathbf{X})+\mathbb{D}_{\theta} T_{2}(\mathbf{X})+2 \sqrt{\mathbb{D}_{\theta} T_{1}(\mathbf{X})} \sqrt{\mathbb{D}_{\theta} T_{2}(\mathbf{X})})}{4} =\mathbb{D}_{\theta} T_{1}(\mathbf{X})
\end{equation*}

В силу того, что $T_1(\mathbf{X})$ и $T_2(\mathbf{X})$ — оптимальные, дисперсия $T_3(\mathbf{X})$ не может быть меньше дисперсии $T_1(\mathbf{X})$, следовательно, справедливо равенство, достигаемое при следующих условиях:
\begin{equation*}
\begin{aligned}
    T_{1}(\mathbf{X})=a T_{2}(\mathbf{X})+b \Rightarrow \mathbb{E} T_{1}(\mathbf{X})
    = a \mathbb{E} T_{2}(\mathbf{X})+b 
    \Leftrightarrow \\
    \Leftrightarrow T(\theta) = a T(\theta)+b~ \forall \theta \Rightarrow a = 1, b = 0
\end{aligned}
\end{equation*}

\end{proof}

\section{Функция правдоподобия. Достаточные статистики, полные статистики. Теорема факторизации}

В зависимости от типа распределения $\mathcal{F}_\theta$ обозначим через $f_{\theta}(y)$ одну из следующих функций:
\begin{equation*}
    f_{\theta}(y) =
    \left\{\begin{array}{ll}
    \text { плотность } f_{\theta}(y), & \text { если } \mathcal{F}_{\theta} \text { абсолютно непрерывно, } \\
    P_{\theta}\left(X_{1}=y\right), & \text { если } \mathcal{F}_{\theta} \text { дискретно. }
    \end{array}\right.
\end{equation*}

\begin{defn}
{\it Функция правдоподобия} выборки $\mathbf{X}$:
\begin{equation*}
    L(\mathbf{X} , \theta)=f_{\theta}\left(X_{1}\right) \cdot f_{\theta}\left(X_{2}\right) \cdot \ldots \cdot f_{\theta}\left(X_{n}\right)=\prod_{i=1}^{n} f_{\theta}\left(X_{i}\right)
\end{equation*}
\end{defn}

В дискретном случае функция правдоподобия принимает вид:
\begin{equation*}
\begin{aligned}
    L(\mathbf{x} , \theta)=\prod_{i=1}^{n} f_{\theta}(x_{i}) 
    = \mathrm{P}_{\theta}(X_{1}=x_{1}) \cdot \ldots \cdot \mathrm{P}_{\theta}(X_{n}=x_{n}) = \\
    = \mathrm{P}_{\theta}(X_{1}=x_{1}, \ldots, X_{n}=x_{n})
\end{aligned}
\end{equation*}

Таким образом, смысл функции правдоподобия~--- вероятность попасть в заданную точку при соответствующем параметре $\theta$ в дискретном случае; для абсолютно непрерывного аналогично~--- вероятность попасть в куб с центром в $x_1, \ldots, x_n$ и сторонами $dx_1, \ldots, dx_n$.

\begin{defn}
{\it Достаточная статистика} для параметра $\theta$~--- статистика $T(\mathbf{X})$, т.ч. $\forall t,~ \forall B \in \mathfrak{B}(\mathbb{R}^{n})$ условное распределение $\mathbb{P}(X_1, \ldots, X_n \in B~|~T=t)$ не зависит от параметра $\theta$.
\end{defn}

Иными словами, если значение статистики $T$ известно и фиксировано, то даже знание её распределения больше не даёт никакой информации о параметре; достаточно лишь вычислить $T$ по выборке.

\begin{thm}[Критерий факторизации]
$T(\mathbf{X})$~--- достаточная статистика $\Leftrightarrow$ её функция правдоподобия представима в виде $L(\mathbf{X}_{1}, \ldots, X_{n} , \theta) \stackrel{\text{п.н.}}{=} h(\mathbf{X}) \cdot \Psi(S, \theta)$
\end{thm}

\begin{proof}
Рассмотрим только дискретный случай. Пусть $T(\mathbf{X})$~--- достаточная статистика. Если $T(\mathbf{X})=t$, то событие $\{\mathbf{X}=\mathbf{x}\} \subseteq \{T(\mathbf{X})=t\}$. Поэтому
\begin{multline*}
    L(\mathbf{x}, \theta) = \mathrm{P}_{\theta}(\mathbf{X}=\mathbf{x})=\mathrm{P}_{\theta}(\mathbf{X}=\mathbf{x} | T(\mathbf{X})=t) =\\
    = \underbrace{\mathrm{P}_{\theta}(T(\mathbf{X})=t)}_{g(T(\mathbf{x}), \theta)} \underbrace{\mathrm{P}_{\theta}(\mathbf{X}=\mathbf{x} | T(\mathbf{X})=t)}_{h(\mathbf{x}, t)}
\end{multline*}

Пусть теперь функция правдоподобия имеет вид $L(\mathbf{x}, \theta)=g(T(\mathbf{x}, \theta) h(\mathbf{x})$. Тогда, если $x$ таково, что $T(\mathbf{x})=t$, то:
\begin{multline*}
    \mathrm{P}(\mathbf{X}=\mathbf{x} | T(\mathbf{X})=t) =\frac{\mathrm{P}(\mathbf{X}=\mathbf{x}, T(\mathbf{X})=t)}{\mathrm{P}(T(\mathbf{X})=t)}
    =\frac{\mathrm{P}(\mathbf{X}=\mathbf{x})}{\sum_{\mathbf{x}^{\prime}: T(\mathbf{x}^{\prime})=t} \mathrm{P}(\mathbf{X}=\mathbf{x}^{\prime})} = \\
    = \frac{g(t, \theta) h(\mathbf{x})}{\sum_{\mathbf{x}^{\prime}: T(\mathbf{x}^{\prime})=t} g(t, \theta) h(\mathbf{x}^{\prime})}
    = \frac{h(\mathbf{x})}{\sum\limits_{\mathbf{x}^{\prime}: T(\mathbf{x}^{\prime})=t} h(\mathbf{x}^{\prime})}
\end{multline*}
\end{proof}

\begin{defn}
{\it Полная статистика} для параметра $\theta$~--- статистика $T(\mathbf{X})$, т.ч. $\mathbb{E} g(T)=0~\forall \theta \in \Theta \Rightarrow g(T) \stackrel{\text{п.н.}}{=}0$
\end{defn}

\section{Неравенство Рао—Крамера. Эффективные оценки}

Пусть $X_1, \ldots, X_n$  —  некоторая выборка с функцией правдоподобия $L(\mathbf{X}, \theta)$ относительно некоторой меры $\mu$. Введём функцию $\varphi(\theta)=\int\limits_{\mathbf{R}^{n}} T(x) L(x, \theta) \mu(d x)<\infty$, в дальнейшем считая, что она дифференцируема необходимое число раз.

\begin{defn}
Функция правдоподобия $L(\mathbf{X}, \theta)$ {\it удовлетворяет условиям регулярности для $m$-й производной}, если существует
\begin{equation*}
    \cfrac{d^{m} \varphi(\theta)}{d \theta^{m}}=\int\limits_{\mathbb{R}^{n}} T(x) \cfrac{\partial^{m} L(x, \theta)}{\partial \theta^{m}} \mu(d x),
\end{equation*}
причём множество $\left\{ {x~|~L(x,\theta) > 0} \right\}$ не зависит от параметра $\theta$.
\end{defn}

\begin{thm}[Неравенство Рао-Крамера]
Пусть $X_1, \ldots, X_n$ — выборка, $L(\mathbf{X}, \theta)$ удовлетворяет условиям регулярности для первой производной и $\tau(\theta)$  —  дифференцируемая функция $\theta$. Тогда:
\begin{enumerate}
    \item Для любой $~T(\mathbf{X})$,~--- несмещённой оценки функции $\tau(\theta)$, справедливо неравенство:
    \begin{gather*}
        \mathbb{D}_{\theta} T(\mathbf{X}) \geq \cfrac{(\tau^{\prime}(\theta))^{2}}{\mathbb{E}_{\theta} U^{2}(X, \theta)}~\forall \theta \in \Theta, \\
        \text{где}~ U(X, \theta)=\cfrac{\partial \ln L(\mathbf{X}, \theta)}{\partial \theta}~\text{(функция вклада)}
    \end{gather*}
    
    \item Равенство достигается $\Leftrightarrow \exists~ a_n(\theta):~ T(\mathbf{X})-\tau(\theta)=a_{n}(\theta) \cdot U(X, \theta)$
\end{enumerate}
\end{thm}

\begin{proof}
$\int L(x, \theta) \mu(d x)=1 \Rightarrow \int \cfrac{\partial L(x, \theta)}{\partial \theta} \mu(d x)=0$

Из условий регулярности $L(\mathbf{X}, \theta)$ для следует:
\begin{equation*}
    \int T(x) L(x, \theta) \mu(d x)=\mathbb{E}_{\theta} T(\mathbf{X})=T(\theta) \Rightarrow \int T(x) \cfrac{\partial L(x, \theta)}{\partial \theta} \mu(d x)=\tau^{\prime}(\theta)
\end{equation*}

Заметим, что
\begin{equation*}
    \cfrac{\partial L(x, \theta)}{\partial \theta}=\cfrac{\partial \ln L(x, \theta)}{\partial \theta} \cdot L(x, \theta)
\end{equation*}

Откуда следует:
\begin{gather*}
    \int U(x, \theta) L(x, \theta) \mu(d x)=0 \Leftrightarrow \mathbb{E}_{\theta} U(X, \theta)=0 \\
\int T(x) U(x, \theta) L(x, \theta) \mu(d x)=\tau^{\prime}(\theta) \Leftrightarrow \mathbb{E}_{\theta} T(\mathbf{X}) U(X, \theta)=\tau^{\prime}(\theta)
\end{gather*}

Вычитая из первого равенства, помноженного на $\tau(\theta)$, второе, получаем:
\begin{equation*}
    \mathbb{E}_{\theta}(T(\mathbf{X})-T(\theta)) U(X, \theta)=\tau^{\prime}(\theta)
\end{equation*}

В левой части полученного равенства стоит ковариация случайных величин $T(\mathbf{X})$ и $U(X,\theta)$:
\begin{equation*}
    \operatorname{cov}_{\theta}(T(\mathbf{X}), U(X, \theta))=T^{\prime}(\theta)
\end{equation*}

Из неравенства Коши-Буняковского:
\begin{equation*}
    \left(\tau^{\prime}(\theta)\right)^{2}=\operatorname{cov}_{\theta}^{2}(T(\mathbf{X}) U(X, \theta)) \leq \mathbb{D}_{\theta} T(\mathbf{X}) \mathbb{D}_{\theta} U(X, \theta)=\mathbb{D}_{\theta} T(\mathbf{X}) \mathbb{E}_{\theta} U^{2}(X, \theta)
\end{equation*}

...что равносильно п.1 теоремы:
\begin{equation*}
    \mathbb{D}_{\theta} T(\mathbf{X}) \geq \cfrac{\left[\tau^{\prime}(\theta)\right]^{2}}{\mathbb{E}_{\theta} U^{2}(X, \theta)}
\end{equation*}

Неравенство достигается, если линейно связаны:
\begin{equation*}
    T(\mathbf{X})=\varphi(\theta) U(X, \theta)+\psi(\theta) \Rightarrow T(\theta)=\psi(\theta) \Rightarrow a_{n}(\theta)=\varphi(\theta)
\end{equation*}

\end{proof}

Рассмотрим некоторый класс оценок $K=\left\{\hat{\theta}\left(\mathbf{X}\right)\right\}$ параметра $\theta$.
\begin{defn}
    Говорят, что оценка $\theta^{*}\left(\mathbf{X}\right) \in K$ является эффективной оценкой параметра $\theta$ в классе $K$, если для любой другой оценки $\hat{\theta} \in K$ имеет место неравенство:
    \begin{equation*}
        E\left(\theta^{*}-\theta\right)^{2} \leqslant E(\hat{\theta}-\theta)^{2}~ \forall \theta \in \Theta
    \end{equation*}
\end{defn}
Обозначим класс несмещённых оценок:
\begin{equation*}
    K_{0}=\left\{\hat{\theta}\left(\mathbf{X}\right): E \hat{\theta}=\theta, \forall \theta \in \Theta\right\}
\end{equation*}
Оценка, эффективная в $K_0$ называется просто {\it эффективной}.

Для оценки $\theta^{*} \in K_{0}$ по определению дисперсии
\begin{equation*}
    \mathbb{E}\left(\theta^{*}-\theta\right)^{2}=\mathbb{E}\left(\theta^{*}-\mathbb{E} \theta^{*}\right)^{2}=\mathbb{D} \theta^{*}
\end{equation*}

Добавить Чернова стр 35

\begin{rmrk}
Если в неравенстве Рао---Крамера достигается равенство, то полученная оценка~--- эффективная.

Если существует эффективная оценка для функции $\tau(\theta)$, то ни для какой другой функции от $\theta$, кроме линейного преобразования $\tau(\theta)$, эффективной оценки существовать не будет. 
\end{rmrk}

\section{Теорема Рао—Блекуэлла—Колмогорова. Оптимальность оценок являющихся функцией полной достаточной статистики}

\begin{thm}[Теорема Рао—Блекуэлла—Колмогорова] Если оптимальная оценка функции $\tau(\theta)$ существует, то она является функцией от достаточной статистики.
\end{thm}

\begin{proof}
В доказательстве используются следующие свойства условного матожидания: 
\begin{gather*}
    \mathbb{E} f(x, z)=\mathbb{E}(\mathbb{E}(f(x, z) | z)) \\
    \mathbb{E}(g(z) | z)=g(z)
\end{gather*}

\begin{enumerate}
    \item Пусть $T(\mathbf{X})$~--- достаточная статистика, $T_1(\mathbf{X})$~--- несмещённая оценка функции $\tau(\theta)$, т.е. $\mathbb{E} T_{1}(\mathbf{X})=\tau(\theta)$. Рассмотрим функцию $H(T)=\mathbb{E}\left(T_{1} | T\right)$. Тогда из первого свойства следует:
    \begin{equation*}
        \mathbb{E} H(T)=\mathbb{E}\left(\mathbb{E}\left(T_{1} |     T\right)\right)=\mathbb{E} T_{1}=\tau(\theta) \Rightarrow H(T)     \text{~--- несмещённая оценка~} \tau(\theta)
    \end{equation*}

    \item Докажем равномерную минимальность её дисперсии:
    \begin{multline*}
        \mathbb{E}((T_{1}-H(T))(H(T)-\tau(\theta))
        = \mathbb{E}(\mathbb{E}((T_{1}-H(T))(H(T)-\tau(\theta)) | T)) 
        = \\
        = \mathbb{E}((H(T)-H(T))(H(T)-\tau(\theta)))
        = 0
    \end{multline*}

    Тогда из свойств условного матожидания
    \begin{multline*}
        \mathrm{D}\left(T_{1}\right) 
        = \mathrm{E}\left(T_{1}-\tau(\theta)\right)^{2}=\mathrm{E}\left(T_{1}-H(T)+H(T)-\tau(\theta)\right)^{2} =\\
        = \mathrm{E}\left(T_{1}-H(T)\right)^{2}+\mathrm{D}(H(T)) \geqslant \mathrm{D}(H(T))
    \end{multline*}
\end{enumerate}
Таким образом, $H(T)$~--- оптимальная оценка $\tau(\theta)$.

\end{proof}

\begin{thm}
{\it Теорема Колмогорова:} Если $T(\mathbf{X})$~--- полная достаточная статистика, то она является оптимальной оценкой своего математического ожидания.
\end{thm}

\begin{proof}
Докажем, что $T(\mathbf{X})$ является единственной несмещённой оценкой для $\mathbb{E}T(\mathbf{X})$. Тогда $T(\mathbf{X})$ будет оптимальной оценкой. Предположим, что $T_1(\mathbf{X})$~--- оптимальная оценка для $\mathbb{E}T(\mathbf{X})$. Из теоремы Рао-Блекуэлла-Колмогорова получаем, что $T_{1}=H(T)$ и $\mathbb{E} T_{1}=\mathbb{E} T$. Тогда:

\begin{equation*}
    \mathbb{E} \underbrace{(T(\mathbf{X})-H(T(\mathbf{X})))}_{\varphi(T)}=0
\end{equation*}

Из условия полноты $T(\mathbf{X})$ следует, что $\varphi(T)=0$ с вероятностью 1, т.е. $T=H(T)$ с вероятностью 1.
\end{proof}

\section{Метод моментов. Свойства оценок, полученных методом моментов}

Пусть $X_1, \ldots, X_n$~--- выборка объёма $n$ из параметрического семейства распределений $\mathcal{F}_\theta$. Выберем функцию $g(y): \mathbb{R} \rightarrow \mathbb{R}$ так, чтобы существовал момент $\mathbb{E} g\left(X_{1}\right)=h(\theta)$ и функция $h(\theta)$ была обратима на $\Theta$. Разрешим полученное уравнение относительно $\theta$, а затем вместо истинного момента возьмём выборочный:

\begin{equation*}
    \theta=h^{-1}\left(\mathbb{E} g\left(X_{1}\right)\right), \quad \theta^{*}=h^{-1}(\overline{g(\mathbf{X})})=h^{-1}\left(\frac{1}{n} \sum_{i=1}^{n} g\left(X_{i}\right)\right)
\end{equation*}

Полученная оценка $\theta^{*}$~--- {\it оценка метода моментов} для параметра $\theta$. Чаще всего берут $g(y)=y^{k}$. В этом случае, при условии обратимости функции $h$ на $\Omega$:
\begin{equation*}
    \mathbb{E} X_{1}^{k}=h(\theta), \quad \theta=h^{-1}\left(\mathbb{E} X_{1}^{k}\right), \quad \theta^{*}=h^{-1}(\overline{X^{k}})=h^{-1}\left(\frac{1}{n} \sum_{i=1}^{n} X_{i}^{k}\right)
\end{equation*}

\begin{thm}
Пусть $\theta^{*}=h^{-1}(\overline{g(\mathbf{X})})$~--- оценка параметра $\theta$, полученная методом моментов, причём функция $h^{-1}$ непрерывна. Тогда оценка $\theta^{*}$ состоятельна.
\end{thm}

\begin{proof}
По ЗБЧ Хинчина имеем:

\begin{equation*}
    \overline{g(\mathbf{X})}=\frac{1}{n} \sum_{i=1}^{n} g\left(X_{i}\right) \xrightarrow[]{\mathrm{p}} \mathbb{E} g\left(X_{1}\right)=h(\theta)
\end{equation*}

Ввиду непрерывности функции $h^{-1}$:

\begin{equation*}
    \theta^{*}=h^{-1}(\overline{g(\mathbf{X})}) \xrightarrow[]{\mathrm{p}} h^{-1}\left(\mathbb{E} g\left(X_{1}\right)\right)=h^{-1}(h(\theta))=\theta
\end{equation*}
\end{proof}

\begin{defn}
{\it Асимптотически нормальная оценка} параметра $\theta$ с коэффициентом $\sigma^{2}(\theta)$~--- оценка $\theta^{*}$, т.ч. при $n \rightarrow \infty$ имеет место слабая сходимость к стандартному нормальному распределению: $\sqrt{n}(\theta^{*}-\theta) \Rightarrow \mathbf{N}(0, \sigma^{2}(\theta))$.
\end{defn}

\begin{lem}
Пусть функция $g(y)$ такова, что $0 \neq \mathbb{D} g\left(X_{1}\right)<\infty$. Тогда статистика $\overline{g(\mathbf{X})}$ является асимптотически нормальной оценкой для $\mathbb{E} g\left(X_{1}\right)$ с коэффициентом $\sigma^{2}(\theta)=\mathbb{D} g\left(X_{1}\right)$:

\begin{equation*}
    \sqrt{n} \cfrac{\overline{g(\mathbf{X})}-\mathbb{E} g\left(X_{1}\right)}{\sqrt{\mathbb{D} g\left(X_{1}\right)}} \Rightarrow \mathbf{N}(0,1)
\end{equation*}
\end{lem}

\begin{proof}
Следует непосредственно из ЦПТ.
\end{proof}

\begin{rmrk}
Следующая теорема утверждает асимптотическую нормальность оценок вида

\begin{equation*}
    \theta^{*}=H(\overline{g(\mathbf{X})})=H\left(\cfrac{g\left(X_{1}\right)+\ldots+g\left(X_{n}\right)}{n}\right)
\end{equation*}

которые обычно получаются при использовании метода моментов, при этом всегда $\theta=H\left(\mathbb{E} g\left(X_{1}\right)\right)$.
\end{rmrk}

\begin{thm}
Пусть функция $g(y)$ такова, что $0 \neq \mathbb{D} g\left(X_{1}\right)<\infty$, функция $H(y)$ дифференцируема в точке $a=\mathbb{E} g\left(X_{1}\right)$ и её производная в этой точке $H^{\prime}(a)=\left.H^{\prime}(y)\right|_{y=a}$ отлична от нуля. Тогда оценка $\theta^{*}=H(\overline{g(\mathbf{X})})$
является асимптотически нормальной
оценкой для параметра $\theta=H\left(\mathbb{E} g\left(X_{1}\right)\right)=H(a)$ с коэффициентом асимптотической нормальности $\sigma^{2}(\theta)=\left(H^{\prime}(a)\right)^{2} \cdot D g\left(X_{1}\right)$.
\end{thm}

\begin{proof}
Согласно ЗБЧ последовательность $\overline{g(\mathbf{X})}$ стремится к $a=\mathbb{E} g\left(X_{1}\right)$ по вероятности с ростом $n$: Функция

\begin{equation*}
    G(y)=\left\{\begin{array}{ll}
    \cfrac{H(y)-H(a)}{y-a}, & y \neq a \\
    H^{\prime}(a), & y=a
    \end{array}\right.  
\end{equation*}

по условию непрерывна в точке $a$: Поскольку сходимость по веро-
ятности сохраняется под действием непрерывной функции, получим,
что $G(\overline{g(\mathbf{X})}) \xrightarrow[]{\mathrm{p}} G(a)=H^{\prime}(a)$.

Заметим также, что по вышеприведённой лемме величина $\sqrt{n}(\overline{g(\mathbf{X})}-a)$ слабо сходится
к нормальному распределению $\mathbf{N}(0, \mathbb{D} g(X_{1}))$: Пусть $\xi$~--- случайная величина
из этого распределения. Тогда

\begin{equation*}
    \sqrt{n}(H(\overline{g(\mathbf{X})})-H(a))=\sqrt{n}(\overline{g(\mathbf{X})}-a) \cdot G(\overline{g(\mathbf{X})}) \Rightarrow \xi \cdot H^{\prime}(a)
\end{equation*}

Мы использовали следующее свойство слабой сходимости: если $\xi_{n} \Rightarrow \xi$ и $\eta_{n} \xrightarrow[]{\mathrm{p}} c=\mathrm{const}$, то $\xi_{n} \eta_{n} \Rightarrow c \xi$. Но распределение случайной величины $\xi \cdot H^{\prime}(a)$ есть $\mathbf{N}(0,(H^{\prime}(a))^{2} \cdot \mathbb{D} g(X_{1}))$, откуда следует

\begin{equation*}
    \sigma^{2}(\theta)=\left(H^{\prime}(a)\right)^{2} \cdot \mathbb{D} g\left(X_{1}\right)
\end{equation*}

\end{proof}

\section{Метод максимального правдоподобия. Свойства оценок максимального правдоподобия}

\begin{defn}
{\it Оценка максимального правдоподобия $\hat{\theta}$ параметра $\theta$}~--- точка параметрического множества $\Theta$, в которой функция правдоподобия $L(\mathbf{X},\theta)$ при заданном $X$ достигает максимума, т.е.:
\begin{equation*}
    L(\boldsymbol{x}, \hat{\theta})=\sup _{\theta \in \Theta} L(\boldsymbol{x}, \theta)
\end{equation*}
\end{defn}

\begin{rmrk}
Поскольку функция $\operatorname{ln}y$ монотонна, то точки максимума функций $L(\mathbf{X},\theta)$ и $ln L(\mathbf{X},\theta)$ совпадают.
\end{rmrk}

Если для каждого $X$ максимум функции правдоподобия достигается во внутренней точке $\Theta$, и $L(\mathbf{X},\theta)$ дифференцируема по $\theta$, то оценка максимального правдоподобия $\hat{\theta}$ удовлетворяет уравнению:

\begin{equation*}
    \cfrac{\partial \ln L(\mathbf{x}, \theta)}{\partial \theta}=0
\end{equation*}

Если $\theta$~--- векторный параметр: $\theta=\left(\theta_{1}, \ldots, \theta_{n}\right)$, то это уравнение заменяется системой уравнений:

\begin{equation*}
    \cfrac{\partial \ln L(\mathbf{x}, \theta)}{\partial \theta_{i}}=0,~ i=\overline{1, n} 
\end{equation*}


\begin{thm}
Если существует эффективная оценка $T(\mathbf{X})$ скалярного параметра $\theta$, то она совпадает с оценкой максимального правдоподобия.
\end{thm}

\begin{proof}
Если оценка $T(\mathbf{X})$ скалярного параметра $\theta$ эффективна, то в неравенстве Рао-Крамера достигается равенство:

\begin{equation*}
    U(X,\theta) = \cfrac{\partial \ln L(\mathbf{x}, \theta)}{\partial \theta}=\cfrac{T(\mathbf{X})-\theta}{a_n(\theta)}
\end{equation*}

\end{proof}

\begin{thm}
Если $T(\mathbf{X})$ достаточная статистика, а оценка максимального правдоподобия $\hat{\theta}$ существует и единственна, то она является функцией от $T(\mathbf{X})$.
\end{thm}

\begin{proof}
Из критерия факторизации следует, что если $T=T(\mathbf{X})$ достаточная статистика, то имеет место представление:

\begin{equation*}
    L(\mathbf{X}, \theta)=g(T(\mathbf{X}), \theta) h(\mathbf{X})
\end{equation*}

Таким образом, максимизации $L(\mathbf{X},\theta)$ сводится к максимизации $g(T(\mathbf{X}), \theta)$ по $\theta$, Следовательно $\hat{\theta}$ есть функция от $T(\mathbf{X})$.
\end{proof}

\begin{defn}
    {\it Асимптотически эффективная оценка}~---
\end{defn}

\begin{thm}
    Пусть выполнены следующие условия:
    \begin{enumerate}
        \item Функция правдоподобия $L(\mathbf{X}, \theta)$ удовлетворяет условиям регулярности для первых двух производных;
        \item $\exists!~ \theta^{*}$~--- оценка максимального правдоподобия для всех $\theta$, которая достигается во внутренней точке $\Theta$.
    \end{enumerate}
    Тогда оценка $\theta^{*}$:
    \begin{enumerate}
        \item асимптотически несмещёна
        \item состоятельна
        \item асимптотически эффективна
        \item асимптотически нормальна
    \end{enumerate}
\end{thm}

Добавить асимптотическую нормальность и эффективность + Чернова стр 39 теорема для состоятельности.

\section{Интервальное оценивание. Методы центральной статистики и использования точечной оценки}

\begin{defn}
{\it Доверительный интервал} для параметра $\theta$ с коэффициентом доверия $0 \leq \alpha \leq 1$~--- интервал $(T_1(\mathbf{X}), T_2(\mathbf{X}))$, т.ч. $\mathbb{P}_{\theta}(T_1(\mathbf{X}) < \theta < T_2(\mathbf{X})) \geq \alpha$.
\end{defn}

\begin{exmp}
Пусть $X_1, \ldots, X_n$~--- выборка из $\mathbf{N}(\theta, 1)$. Тогда

\begin{equation*}
    \theta^{*}
    = \overline{X}
    = \frac{1}{n} \sum_{i=1}^{n} X_{i} \sim \mathbf{N}\left(\theta, \frac{1}{n}\right)
    \Rightarrow (\overline{X}-\theta) \sqrt{n} \sim \mathbf{N}(0,1)
\end{equation*}

Для величины, имеющей стандартное нормальное распределение, строим доверительный интервал, т.е. находим такое $t_{\alpha / 2}$, что 

\begin{equation*}
    \mathrm{P}_{\theta}\left(|(\bar{X}-\theta) \sqrt{n}|<t_{\alpha / 2}\right)=\alpha
\end{equation*}

Решаем уравнение относительно $\theta$ и получаем
\begin{equation*}
    \mathrm{P}_{\theta}\left(\bar{X}-\cfrac{t_{\alpha / 2}}{\sqrt{n}}<\theta<\bar{X}+\cfrac{t_{\alpha / 2}}{\sqrt{n}}\right)=\alpha 
\end{equation*}

\end{exmp}

\begin{defn}
{\it Центральная статистика}~--- функция $G(X,\theta)$, т.ч.:
\begin{enumerate}
    \item $G(X,\theta)$ непрерывна и строго монотонна по $\theta$ при любом фиксированном $X$.
    \item $\mathbb{P}_{\theta}(G(X, \theta)<t)=F(t)$ непрерывна и не зависит от $\theta$.
\end{enumerate}
\end{defn}

\begin{rmrk}
Формально определённая выше величина не является статистикой, т.к. зависит от неизвестного параметра $\theta$.
\end{rmrk}

Построение доверительного интервала с помощью центральной статистики:
\begin{enumerate}
    \item Зафиксируем $\alpha_{1}, \alpha_{2} \in \mathbf{R}$, т.ч.
    \begin{equation*}
        \mathbb{P}_{\theta}(\alpha_{1} \leq G(X, \theta) \leq \alpha_{2})=\alpha~\forall \theta \Leftrightarrow F_{G}(\alpha_{2})-F_{G}(\alpha_{1})=\alpha
    \end{equation*}
    \item Пусть $G(X,\theta)$ возрастает. Из условий
    \begin{equation*}
        \left\{\begin{array}{l}
        G(X, \theta) \leq \alpha_{2} \\
        G(X, \theta) \geq \alpha_{1}
        \end{array}\right.
    \end{equation*}
    находятся статистики
    \begin{equation*}
        \left\{\begin{array}{l}
            T_{2}(\mathbf{X}): G(X, T_{2}(\mathbf{X}))=\alpha_{2} \\ 
            T_{1}(\mathbf{X}): G(X, T_{1}(\mathbf{X}))=\alpha_{1}
        \end{array} 
        \Leftrightarrow T_{1}(\mathbf{X}) \leq \theta \leq T_{2}(\mathbf{X})\right.
    \end{equation*}
    откуда $\mathbb{P}_{\theta}\left(T_{1}(\mathbf{X}) \leq \theta \leq T_{2}(\mathbf{X})\right) \geq \alpha~ \forall \theta$.
\end{enumerate}

\begin{defn}
{\it Центральный доверительный предел} для параметра $\theta$ с коэффициентом доверия $0 \leq \alpha \leq 1$~--- интервал $(T_1(\mathbf{X}), T_2(\mathbf{X}))$, т.ч. 
\begin{gather*}
    \mathbb{P}_{\theta}\left(T_{1}(\mathbf{X})>\theta\right)=\cfrac{1-\alpha}{2} \\
    \mathbb{P}_{\theta}\left(T_{2}(\mathbf{X})<\theta\right)=\cfrac{1-\alpha}{2}
\end{gather*}
\end{defn}

Построение доверительного интервала с помощью точечной оценки:

\begin{enumerate}
    \item Пусть $T(\mathbf{X})$~--- точечная оценка $\theta$. Обозначим $H(t, \theta)=\mathbb{P}_{\theta}(T(\mathbf{X})<t)$. $H(t,\theta)$~--- непрерывная и строго монотонная функция $\theta$ при любом фиксированном $t$. В этом случае
    \begin{equation*}
        \left\{\begin{array}{l}
            \mathbb{P}_{\theta}\left(T(\mathbf{X})>a_{1}(\theta)\right)
            = \cfrac{1-\alpha}{2} \\ 
            \mathbb{P}_{\theta}\left(T(\mathbf{X})<\alpha_{2}(\theta)\right)
            = \cfrac{1-\alpha}{2}
        \end{array}\right. 
        \Leftrightarrow 
        \left\{\begin{array}{l}
            1 - H(\alpha_{1}(\theta), \theta)=\cfrac{1-\alpha}{2} \\ 
            H(\alpha_{2}(\theta), \theta)=\cfrac{1-\alpha}{2}
        \end{array}\right.
    \end{equation*}
    
    \item Рассмотрим вспомогательную лемму.
    \begin{lem}
        Если $H(t, \theta)$ возрастает по $\theta$, то $\alpha_{1}(\theta)$ и $\alpha_{2}(\theta)$ убывают. Если же $H(t, \theta)$ убывает по $\theta$, то $\alpha_{1}(\theta)$ и $\alpha_{2}(\theta)$ возрастают.
    \end{lem}
    \begin{proof}
        Пусть $H(t, \theta)$ возрастает. Предположим, что $\theta_{1}<\theta_{2} \Rightarrow \alpha_{2}\left(\theta_{1}\right) \leq \alpha_{2}\left(\theta_{2}\right)$ и рассмотрим $a_{2}(\theta)$, учитывая, что $H(t, \theta)$, как и всякая функция распределения, неубывает по первому аргументу:
        \begin{equation*}
            \frac{1-a}{2} 
            = H\left(\alpha_{2}\left(\theta_{1}\right) \theta_{1}\right)
            < H\left(\alpha_{2}\left(\theta_{1}\right) \theta_{2}\right) 
            \leq H\left(\alpha_{2}\left(\theta_{2}\right) \theta_{2}\right)
            = \frac{1-\alpha}{2}
        \end{equation*}
        Полученное противоречие завершает доказательство.
    \end{proof}
    \item Из леммы следует, что для любого $\theta$
    \begin{equation*}
    \begin{aligned}
        \alpha_{1}(\theta) 
        < T(\mathbf{X})
        \Leftrightarrow \theta>\varphi_{1}(T(\mathbf{X}))
        \Rightarrow \mathrm{P}_{\theta}(\theta>\varphi_{1}(T(\mathbf{X})))
        = \cfrac{1-\alpha}{2} \\
        \alpha_{2}(\theta)>T(\mathbf{X}) 
        \Leftrightarrow \theta<\varphi_{2}(T(\mathbf{X})) \Rightarrow \mathrm{P}_{\theta}(\theta<\varphi_{2}(T(\mathbf{X})))
        = \cfrac{1-\alpha}{2} \\
        \Rightarrow P_{\theta}(\underbrace{\varphi_{2}(T(\mathbf{X}))}_{T_{1}(\mathbf{X})} 
        \leq \theta 
        \leq \underbrace{\varphi_{1}(T(\mathbf{X}))}_{T_{2}(\mathbf{X})})
        = \alpha
    \end{aligned}
    \end{equation*}

\end{enumerate}

\section{Проверка гипотез. Лемма Неймана—Пирсона}

\begin{defn}
{\it Гипотеза $H$}~--- любое предположение о распределении наблюдаемой случайной величины: $H=\left\{\mathcal{F}=\mathcal{F}_{1}\right\}$ или $H=\{\mathcal{F} \in \mathbb{F}\}$, где $\mathbb{F}$~--- некоторое подмножество в множестве всех распределений. Гипотеза называется {\it простой} в первом случае, {\it сложной} во втором. Если гипотез всего две, то одну из них принято называть {\it основной}, а другую~--- {\itальтернативой}.
\end{defn}

\begin{rmrk} Типичные задачи проверки гипотез:
\begin{enumerate}
    \item Гипотезы о виде распределения;
    \item Гипотезы о проверке однородности выборки: дано несколько выборок; основная гипотезасостоит в том, что эти выборки извлечены из одного распределения;
    \item Гипотеза независимости: по выборке $(X_1,Y_1), \ldots, (X_n,Y_n)$ из $n$ независимых наблюдений пары случайных величин проверяется гипотеза $H_{1}=\left\{X_{i} \text { и } Y_{i} \text { независимы }\right\}$ при альтернативе $H_{1}=\left\{H_{1} \text { неверна }\right\}$. Обе гипотезы являются сложными;
    \item Гипотеза случайности: в эксперименте наблюдаются $n$ случайныхвеличин $X_{1}, \ldots, X_{n}$ и проверяется сложная гипотеза $H_{1}=\left\{X_{1}, \ldots, X_{n}~ \text{независимы и одинаково распределены}\right\}$
\end{enumerate}
\end{rmrk}

Пусть дана выборка $X_{1}, \ldots, X_{n}$, относительно распределения которой выдвинуты две простые гипотезы $H_{0}$ и $H_1$.
\begin{defn}
{\it Критерий}~--- правило, согласно которому гипотеза $H_0$ принимается или отвергается.
\end{defn}
Выборка ($\mathbf{X} = X_1, \ldots, X_n$) объёма $n$~--- точка в пространстве $\mathbb{R}^{n}$. Выделим множество $S \subset \mathbb{R}^{n}$~--- {\it критическую область} для гипотезы $H_0$. В этом случае критерий можно сформулировать следующим образом:
\begin{itemize}
    \item $\varphi(x) = 1 \Rightarrow$ отвергаем $H_0$, принимаем $H_1$;
    \item $\varphi(x) = 0 \Rightarrow$ отвергаем $H_1$, принимаем $H_0$;
\end{itemize}

\begin{defn}
Говорят, что произошла {\it ошибка 1-го рода}, если критерий отверг верную гипотезу $H_0$. Вероятность ошибки 1-го рода (или {\it уровень значимости критерия}): 
\begin{equation*}
    \alpha(S)=P\left\{\mathbf{X} \in S ~|~ H_{0}\right\}=P_{0}\left\{\mathbf{X} \in S\right\}
\end{equation*}
Аналогично вероятность ошибки 2-го рода:
\begin{equation*}
    \beta(S)=P\left\{\mathbf{X} \notin S ~|~ H_{1}\right\}=P_{1}\left\{\mathbf{X} \notin S\right\}
\end{equation*}
\end{defn}

\begin{defn}
{\it Мощность критерия}:
\begin{equation*}
    \gamma(S)=1-\beta(S)=P_{1}\left\{\mathbf{X} \in S\right\}
\end{equation*}
\end{defn}

\begin{center}
\begin{tabular}{|c|c|c|}
\hline \multirow{2}{*} { Истинная гипотеза } & \multicolumn{2}{|c|} { Результат принятия решения } \\
\cline {2-3} & $H_{0}$ отклонена & $H_{0}$ принята \\
\hline$H_{0}$ & $\alpha$ & $1-\alpha$ \\
\hline$H_{1}$ & $1-\beta$ & $\beta$ \\
\hline
\end{tabular}
\end{center}

Если $\gamma(S)<\alpha(S)$, то попасть в $S$ при условии истинности гипотезы $H_1$ труднее, чем при условии истинности гипотезы $H_0$, т.е. $S$~--- критическая область скорее для $H_1$. Следовательно, неравенство должно иметь вид $\gamma(S)>\alpha(S)$.

\begin{defn}
    Критерий называется {\it несмещённым}, если выполняется условие
    \begin{equation*}
        \alpha(S) \leqslant \gamma(S)=1-\beta(S)
    \end{equation*}
\end{defn}

Зададим $\alpha_0$ и и будем иметь дело только с такими критериями, где $\alpha_{0} \geqslant \alpha(S)$ (т.е. вероятность ошибки первого рода не превосходитвеличины $\alpha_0$) и дополнительно будем решать задачу $\beta(S) \rightarrow \min\limits_{S}$.

Получаем две эквивалентные задачи определения критической области $S$:
\begin{equation*}
    \begin{array}{l}
    \left\{\begin{array}{l}
    \alpha_{0} \geqslant \alpha(S) \\
    \beta(S) \rightarrow \min\limits_{S}
    \end{array}\right.
    \Leftrightarrow~
    \left\{\begin{array}{l}
    \alpha_{0} \geqslant \alpha(S) \\
    \gamma(S) \rightarrow \max\limits_{S}
    \end{array}\right.
    \end{array}
\end{equation*}

Задачи в такой постановке не всегда решаемы, так как требуетсяответить точно <<да>> или <<нет>>. Такие статистические критерииназываются {\it нерандомизированными критериями}.

\begin{exmp}
Рассмотрим {\it критическую функцию} $\varphi(x)=I\{x \in S\}$. Тогда критерий примет вид:
\begin{itemize}
    \item Если $\varphi\left(\mathbf{X}\right)=1$, тогда отвергаем гипотезу $H_0$, принимаем $H_1$.
    \item Если $\varphi\left(\mathbf{X}\right)=0$, тогда отвергаем гипотезу $H_1$, принимаем $H_0$.
\end{itemize}
\end{exmp}

\begin{exmp}
Рассмотрим другую критическую функцию $\varphi(x)=P\left\{\bar{H}_{0} / \mathbf{X}=x\right\}$. В этом случае $\varphi\left(\mathbf{X}\right) \in[0,1]$~--- условная вероятность отклонения гипотезы $H_0$. При таком определении $\varphi(x)$ приходим к {\itрандомизированному критерию}, то есть, критерию, который при некоторых значениях $s$ может не давать ответа <<да>> или <<нет>> в отношении истинности гипотезы $H_0$. Тогда формулировка критерия следующая:
\begin{itemize}
    \item с вероятностью $1 - \varphi\left(\mathbf{X}\right)$ следует принимать гипотезу $H_0$;
    \item с вероятностью $\varphi\left(\mathbf{X}\right)$ следует принимать гипотезу $H_0$ю
\end{itemize}
\end{exmp}

\begin{rmrk}
При использовании введенного обозначения вероятность ошибки первого рода, вероятность ошибки второго рода и мощность критерия будем обозначать: $\alpha(\varphi)$, $\beta(\varphi)$ и $\gamma(\varphi)=1-\beta(\varphi)$ соответственно.
\end{rmrk}

Без ограничения общности будем предполагать, что существует плотность $f_{0}(x)$ для функции распределения $F_{0}(x)$, и существует плотность $f_{1}(x)$ для функции распределения $F_{1}(x)$. В дискретном случае все результаты аналогичны.

Если верна гипотеза $H_1$, то функция правдоподобия выборки $X$ имеет вид:
\begin{equation*}
    L_{1}\left(\mathbf{X}\right)=\prod_{i=1}^{n} f_{1}\left(X_{i}\right)
\end{equation*}

Для рандомизированного критерия получаем
\begin{gather*}
    P_{0}\left(\bar{H}_{0}\right)=\int\limits_{\mathbb{R}^{n}} \varphi(x) L_{0}(x) \mu^{n}(d x)=\alpha(\varphi) \\
    P_{1}\left(H_{0}\right)=\int\limits_{\mathbb{R}^{n}}(1-\varphi(x)) L_{1}(x) \mu^{n}(d x)=\beta(\varphi) \\
    \gamma(\varphi)=\int\limits_{\mathbb{R}^{n}} \varphi(x) L_{1}(x) \mu^{n}(d x), \quad \gamma(\varphi)=1-\beta(\varphi)
\end{gather*}

Тогда задача построения статистического критерия сводится к нахождению критической функции $\varphi(x)$ и будет формулироваться следующим образом:
\begin{equation*}
    \begin{array}{l}
    \left\{\begin{array}{l}
    \alpha_{0} \geqslant \alpha(\varphi) \\
    \beta(\varphi) \rightarrow \min\limits_{\varphi}
    \end{array}\right.
    \Leftrightarrow
    \left\{\begin{array}{l}
    \alpha_{0} \geqslant \alpha(\varphi) \\
    \gamma(\varphi) \rightarrow \max\limits_{\varphi}
    \end{array}\right.
    \end{array}
\end{equation*}
Таким образом, задача заключается в том, чтобы найти наиболее мощный критерий, когда вероятность ошибки первого рода не превосходит некоторого заданного порогового значения. Решение сформулированных задач даётся леммой Неймана-Пирсона.

\begin{thm}[Лемма Неймана---Пирсона]
Пусть $\alpha_{0} \in(0,1)$, тогда при фиксированной вероятности ошибки первого рода $\alpha_{0}$ наиболее мощный критерий имеет критическую функцию $\varphi^{*}$ вида
\begin{equation*}
    \varphi^{*}(x)=\left\{\begin{array}{ll}
    1, & \text { если } L_{1}(x)>c L_{0}(x) \\
    \varepsilon, & \text { если } L_{1}(x)=c L_{0}(x) \\
    0, & \text { если } L_{1}(x)<c L_{0}(x)
    \end{array}\right.
\end{equation*}
где $L_{j}(x)=\prod_{i=1}^{n} f_{j}\left(x_{i}\right)$ соответствует гипотезе $H_j, j = \overline{1,2}$, константы $c$ и $\varepsilon$ являются решениями уравнения $\alpha\left(\varphi^{*}\right)=\alpha_{0}$.
\end{thm}

\begin{proof}
\begin{enumerate}
    \item Покажем, что константы $c$ и $\varepsilon$ могут быть найдены из уравнения $\alpha\left(\varphi^{*}\right)=\alpha_{0}$. Заметим, что
    
    \begin{equation*}
        \begin{aligned} \alpha(\varphi^{*})
        = P_{0}(L_{1}(\mathbf{X}) > c L_{0}(\mathbf{X})) 
        + \varepsilon P_{0}(L_{1}(\mathbf{X}) = c L_{0}(\mathbf{X}))=\\ 
        = P_{0}\left(\frac{L_{1}(\mathbf{X})}{L_{0}(\mathbf{X})} > c\right) 
        + \varepsilon P_{0}\left(\frac{L_{1}(\mathbf{X})}{L_{0}(\mathbf{X})} = c \right) 
        \end{aligned}
    \end{equation*}

Если предположить, что $L_{0}(\mathbf{X})=0$, то

\begin{equation*}
    P_{0}\left\{L_{0}(\mathbf{X}) = 0\right\} = \int\limits_{\left\{x: L_{0}(x)=0\right\}} L_{0}(x) \mu(d x)=0
\end{equation*}

и, следовательно, вышеприведённое равенство корректно. Поэтому рассмотрим случайную величину $\eta(\mathbf{X}) = \cfrac{L_{1}(\mathbf{X})}{L_{0}(\mathbf{X})}$

Положим $F_{H_{0}, \eta}(t)=P\{\eta \leqslant t\}$, тогда
\begin{equation*}
    \alpha\left(\varphi^{*}\right)=1-F_{H_{0}, \eta}(c)+\varepsilon\left(F_{H_{0}, \eta}(c)-F_{H_{0}, \eta}(c-0)\right)
\end{equation*}

Пусть $g(c)=1-F_{H_{0}, \eta}(c)$, константу $c_{\alpha_{0}}$ можно выбрать так, чтобы было выполнено неравенство:
\begin{equation*}
    g(c_{\alpha_{0}}) \leqslant \alpha_{0} \leqslant g(c_{\alpha_{0}}-0)
\end{equation*}

Тогда
\begin{equation*}
    \varepsilon_{\alpha_{0}} = 
    \left\{\begin{array}{ll}
         0, & \text{ если }  g\left(c_{\alpha_{0}}\right)=g\left(c_{\alpha_{0}}-0\right) \\
         \cfrac{\alpha_{0}-g\left(c_{\alpha_{0}}\right)}{g\left(c_{\alpha_{0}}-0\right)-g\left(c_{\alpha_{0}}\right)} \in [0,1], & \text{ если } g\left(c_{\alpha_{0}}\right)<g\left(c_{\alpha_{0}}-0\right)
    \end{array}\right.
\end{equation*}

В обоих случаях выполнено равенство:
\begin{equation*}
    \alpha_{0}=g\left(c_{\alpha_{0}}\right)+\varepsilon_{\alpha_{0}}\left(g\left(c_{\alpha_{0}}-0\right)-g\left(c_{\alpha_{0}}\right)\right)=\alpha\left(\varphi^{*}\right)
\end{equation*}

\item Докажем, что $\varphi^{*}(x)$~--- критическая функция наиболее мощного критерия.

Выберем любую другую критическую функцию $\tilde{\varphi}(x)$ такую, что $\alpha(\tilde{\varphi}) \leqslant \alpha_{0}$, и сравним ее с критической функцией $\varphi^{*}(x)$. Заметим, что для любого $x$ справедливо неравенство:
\begin{equation*}
    \left(\varphi^{*}(x)-\tilde{\varphi}(x)\right)\left(L_{1}(x)-c_{\alpha_{0}} L_{0}(x)\right) \geqslant 0
\end{equation*}

Тогда
\begin{equation*}
    \int\limits_{\mathbb{R}^{n}}\left(\varphi^{*}(x)-\tilde{\varphi}(x)\right)\left(L_{1}(x)-c_{\alpha_{0}} L_{0}(x)\right) \mu^{n}(d x) \geqslant 0
\end{equation*}

Раскроем скобки и преобразуем:

\begin{equation*}
    \begin{array}{l}
\int\limits_{\mathbb{R}^{n}} \varphi^{*}(x) L_{1}(x) \mu^{n}(d x)-\int\limits_{\mathbb{R}^{n}} \tilde{\varphi}(x) L_{1}(x) \mu^{n}(d x) \geqslant \\
\quad \geqslant c_{\alpha_{0}}\left(\int\limits_{\mathbb{R}^{n}} \varphi^{*}(x) L_{0}(x) \mu^{n}(d x)-\int\limits_{\mathbb{R}^{n}} \tilde{\varphi}(x) L_{0}(x) \mu^{n}(d x)\right) \geqslant 0
\end{array}
\end{equation*}

Следовательно, $\gamma\left(\varphi^{*}\right)-\gamma(\tilde{\varphi}) \geqslant c_{\alpha_{0}}\left(\alpha\left(\varphi^{*}\right)-\alpha(\tilde{\varphi})\right)$, откуда получаем неравенство:

\begin{equation*}
    \gamma\left(\varphi^{*}\right) \geqslant \gamma(\tilde{\varphi})
\end{equation*}
\end{enumerate}
\end{proof}

\section{Критерии согласия Колмогорова и $\chi^{2}$}

{\bf Критерий Колмогорова}

Выборка $X_1, \ldots, X_n$ имеет функцию распределения $F(x)$ из семейства распределений . Требуется проверить гипотезу $F(x)=F_{0}(x)$. Непараметрический критерий Колмогорова основан на статистике
\begin{equation*}
    D_{n}(\mathbf{X})=\sup _{X} | F_{n}(x)-F_{0}(x)
\end{equation*}
где $F_{0}(x)$  —  непрерывная функция распределения, а $F_{n}(x)$~---  эмпирическая функция распределения, построенная повыборке $X_1, \ldots, X_n$.

Из того, что если $\xi$~--- случайная величина, $F_{\xi}(x)$~--- непрерывна, то случайная величина $\eta=F_{\xi}(\xi)$ равномерно распределенана $[0,1]$, следует что при $F_{0}(x)=t$ вероятность $\mathbb{P}\left(D_{n}(\mathbf{X})<t\right)$ независит от $\theta$ и $F_{0}(x)$.

\begin{thm}
    Для любой непрерывной $F(x)$ при $x > 0$ выполняется
    \begin{equation*}
        \lim _{n \rightarrow \infty} P\left(\sqrt{n} D_{n}(\mathbf{X})<t\right)=K(t)=\sum_{j=-\infty}^{+\infty}(-1)^{-2 j^{2}+2}
    \end{equation*}
\end{thm}

На основе этого предельного соотношения строится непараметрический критерий Колмогорова. Пусть $\gamma_{\alpha}$~--- $\alpha$-квантиль предельного распределения $K(t)$:
\begin{equation*}
    1-K\left(\gamma_{\alpha}\right)=\alpha \Leftrightarrow \mathrm{P}\left(\sqrt{n} D_{n}(\mathbf{X}) \geq \gamma_{a} | H_{0}=\alpha\right)
\end{equation*}

Тогда гипотеза о том, что выборка взята из распределения с функцией $F_{0}(x)$ принимается, если $\sqrt{n} D_{n}(\mathbf{X}) \leq \gamma_{a}$. Уровень значимости этого критерия равен приближённо $\alpha$.

{\bf Критерий $\chi^{2}$}
Пусть имеетсявыборка $X_1, \ldots, X_n$ и требуется проверить гипотезу $H_{0}: F(x)=F_{0}(x)$. Разобьём числовую прямую на $m$ промежутков $\Delta_{1}, \Delta_{2}, \ldots, \Delta_{m-1}, \Delta_{m}$. Обозначим $V_{k}$ — число наблюдений, попавших в интервал $\Delta_{k}$. Тогда если $\xi_{i}^{(k)}=\mathrm{I}\left(X_{i} \in \Delta_{k}\right),$ тo $v_{k}=\sum_{i=1}^{n} \xi_{i}^{(k)}$.

При этом имеет место сходимость
\begin{equation*}
    \frac{v_k}{n} \xrightarrow[n \to \infty]{} P\left(X_{1} \in \Delta_{k}\right)=\int\limits_{\Delta_{k}} d F_{0}(x)=p_{k}
\end{equation*}

Строится статистика

\section{Статистические выводы о параметрах нормального распределения. Распределения $\chi^{2}$ и Стьюдента. Теорема Фишера}

\begin{defn}
    Пусть $\zeta_{1}, \ldots, \zeta_{k}$ взаимно независимые случайные величины, $\zeta_{k} \sim \mathbf{N}(0,1)$. Распределение случайной величины $\tau_{k}=\zeta_{1}^{2}+\ldots+\zeta_{k}^{2}$ называется распределением $\chi^{2}$ с $k$ степенями свободы.
\end{defn}
\begin{rmrk}
    Распределение $\chi^{2}$ с $k$ степенями свободы представляет собой гамма-распределение с параметрами формы $\frac{k}{2}$ и масштаба $\frac{1}{2}$:
    \begin{equation*}
    f_{\tau}(x)=\left\{\begin{array}{ll}
        \left(\frac{1}{2}\right)^{\frac{k}{2}} \frac{x^{\frac{k}{2}-1}}{\Gamma\left(\frac{k}{2}\right)} e^{-\frac{x}{2}}, & x>0 \\
        0, & x \leq 0
    \end{array}\right.
    \end{equation*}
\end{rmrk}

\begin{defn}
    Пусть заданы случайные величины $\zeta \sim \mathbf{N}(0,1)$ и $\tau_{k} \sim \chi_{k}^{2}$. Пусть случайные величины $\zeta$ и $\tau_{k}$ взаимно независимы. Распределение случайной величины 
    \begin{equation*}
        \xi=\frac{\zeta}{\sqrt{\frac{\tau_{k}}{k}}}
    \end{equation*}
    называется {\it распределением Стьюдента} с $k$ степенями свободы и обозначается через $T_{k}$.
\end{defn}

\end{document}
